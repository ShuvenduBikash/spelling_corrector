{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9533,
     "status": "ok",
     "timestamp": 1535255969007,
     "user": {
      "displayName": "Shuvendu BIkash",
      "photoUrl": "//lh4.googleusercontent.com/-cb2XgdoysqI/AAAAAAAAAAI/AAAAAAAAAFg/3GJlDMCUuNI/s50-c-k-no/photo.jpg",
      "userId": "114258866435529703404"
     },
     "user_tz": -360
    },
    "id": "Oi7fhhdI5Ima",
    "outputId": "0a3c153a-9c7c-4f0f-9725-b42928573c07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in c:\\users\\bikas\\anaconda3\\lib\\site-packages (1.0.22)\n",
      "Requirement already satisfied: observations in c:\\users\\bikas\\anaconda3\\lib\\site-packages (0.1.4)\n",
      "Requirement already satisfied: numpy>=1.7 in c:\\users\\bikas\\anaconda3\\lib\\site-packages (from observations) (1.14.5)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\bikas\\anaconda3\\lib\\site-packages (from observations) (1.11.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode\n",
    "!pip install observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8258,
     "status": "ok",
     "timestamp": 1535255977367,
     "user": {
      "displayName": "Shuvendu BIkash",
      "photoUrl": "//lh4.googleusercontent.com/-cb2XgdoysqI/AAAAAAAAAAI/AAAAAAAAAFg/3GJlDMCUuNI/s50-c-k-no/photo.jpg",
      "userId": "114258866435529703404"
     },
     "user_tz": -360
    },
    "id": "CRs1lhjh4ttT",
    "outputId": "6180eb10-0649-4415-9c9b-cecfd8aa8d7d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bikas\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "# Import TensorFlow >= 1.10 and enable eager execution\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import unidecode\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EnG1_mD4EOGB"
   },
   "source": [
    "## Load PTB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13829,
     "status": "ok",
     "timestamp": 1535256006703,
     "user": {
      "displayName": "Shuvendu BIkash",
      "photoUrl": "//lh4.googleusercontent.com/-cb2XgdoysqI/AAAAAAAAAAI/AAAAAAAAAFg/3GJlDMCUuNI/s50-c-k-no/photo.jpg",
      "userId": "114258866435529703404"
     },
     "user_tz": -360
    },
    "id": "fpvsxgZOEKkS",
    "outputId": "e4703fe3-8da4-4ca7-e17f-cd44b5074771"
   },
   "outputs": [],
   "source": [
    "import observations\n",
    "text, testfile, valfile = getattr(observations, 'ptb')('data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1131,
     "status": "ok",
     "timestamp": 1535256007949,
     "user": {
      "displayName": "Shuvendu BIkash",
      "photoUrl": "//lh4.googleusercontent.com/-cb2XgdoysqI/AAAAAAAAAAI/AAAAAAAAAFg/3GJlDMCUuNI/s50-c-k-no/photo.jpg",
      "userId": "114258866435529703404"
     },
     "user_tz": -360
    },
    "id": "IOGvsf87EWfJ",
    "outputId": "f47efcad-206c-47e5-f9b1-3bb9f7030d63"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5269890"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1513,
     "status": "ok",
     "timestamp": 1535256009723,
     "user": {
      "displayName": "Shuvendu BIkash",
      "photoUrl": "//lh4.googleusercontent.com/-cb2XgdoysqI/AAAAAAAAAAI/AAAAAAAAAFg/3GJlDMCUuNI/s50-c-k-no/photo.jpg",
      "userId": "114258866435529703404"
     },
     "user_tz": -360
    },
    "id": "RM1Av3BjylpW",
    "outputId": "ecaa3474-f0cd-4e79-9a3c-55c477691311"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4715474\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(w):\n",
    "    w = w.lower()\n",
    "    w = re.sub(\"<eos>\", \" \", w)\n",
    "    w = re.sub(\"<unk>\", \" \", w)\n",
    "    \n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-z?.!,]+\", \" \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    \n",
    "    return w\n",
    "\n",
    "text = preprocess_sentence(text)\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1069,
     "status": "ok",
     "timestamp": 1535256011004,
     "user": {
      "displayName": "Shuvendu BIkash",
      "photoUrl": "//lh4.googleusercontent.com/-cb2XgdoysqI/AAAAAAAAAAI/AAAAAAAAAFg/3GJlDMCUuNI/s50-c-k-no/photo.jpg",
      "userId": "114258866435529703404"
     },
     "user_tz": -360
    },
    "id": "khE448cQ6Xfw",
    "outputId": "f06c5c9d-e9b2-4b24-b2f2-cbd7c2be39c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique characters  29\n"
     ]
    }
   ],
   "source": [
    "# unique contains all the unique characters in the file\n",
    "unique = ['<start>']+ sorted(set(text))\n",
    "\n",
    "# creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(unique)}\n",
    "idx2char = {i:u for i, u in enumerate(unique)}\n",
    "\n",
    "print(\"number of unique characters \", len(unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c4lvvfUS6dSz"
   },
   "outputs": [],
   "source": [
    "# setting the maximum length sentence we want for a single input in characters\n",
    "max_length = 40\n",
    "\n",
    "# length of the vocabulary in chars\n",
    "vocab_size = len(unique)\n",
    "\n",
    "# the embedding dimension \n",
    "embedding_dim = 16\n",
    "\n",
    "# number of RNN (here GRU) units\n",
    "units = 1024\n",
    "\n",
    "# batch size \n",
    "BATCH_SIZE = 48\n",
    "\n",
    "# buffer size to shuffle our dataset\n",
    "BUFFER_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3405,
     "status": "ok",
     "timestamp": 1535256016095,
     "user": {
      "displayName": "Shuvendu BIkash",
      "photoUrl": "//lh4.googleusercontent.com/-cb2XgdoysqI/AAAAAAAAAAI/AAAAAAAAAFg/3GJlDMCUuNI/s50-c-k-no/photo.jpg",
      "userId": "114258866435529703404"
     },
     "user_tz": -360
    },
    "id": "tIuKhrHsl0LI",
    "outputId": "df1ea0e1-af1f-49ab-cf79-fc6d109ed327"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(117886, 40)\n",
      "(117886, 40)\n"
     ]
    }
   ],
   "source": [
    "input_text = []\n",
    "target_text = []\n",
    "\n",
    "for f in range(0, len(text)-max_length, max_length):\n",
    "    inps = text[f:f+max_length]\n",
    "    targ = text[f:f+max_length]\n",
    "\n",
    "    input_text.append([char2idx[i] for i in inps])\n",
    "    target_text.append([char2idx[t] for t in targ])\n",
    "    \n",
    "print (np.array(input_text).shape)\n",
    "print (np.array(target_text).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13345,
     "status": "ok",
     "timestamp": 1535256029536,
     "user": {
      "displayName": "Shuvendu BIkash",
      "photoUrl": "//lh4.googleusercontent.com/-cb2XgdoysqI/AAAAAAAAAAI/AAAAAAAAAFg/3GJlDMCUuNI/s50-c-k-no/photo.jpg",
      "userId": "114258866435529703404"
     },
     "user_tz": -360
    },
    "id": "FrM3H4JMl-D5",
    "outputId": "034adb16-b1fc-467c-8cc5-224d62ec237f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-3bbc2611a7c8>:2: batch_and_drop_remainder (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.batch(..., drop_remainder=True)`.\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((input_text, target_text)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.apply(tf.contrib.data.batch_and_drop_remainder(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RdRPkpZmmtSe"
   },
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J4JrCs-QmCQl"
   },
   "outputs": [],
   "source": [
    "def gru(units):\n",
    "  # If you have a GPU, we recommend using CuDNNGRU(provides a 3x speedup than GRU)\n",
    "  # the code automatically does that.\n",
    "  if tf.test.is_gpu_available():\n",
    "    return tf.keras.layers.CuDNNGRU(units, \n",
    "                                    return_sequences=True, \n",
    "                                    return_state=True,\n",
    "                                    recurrent_initializer='glorot_uniform')\n",
    "  else:\n",
    "    return tf.keras.layers.GRU(units, \n",
    "                               return_sequences=True, \n",
    "                               return_state=True,\n",
    "                               recurrent_initializer='glorot_uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8T4dsBt_nWTh"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.enc_units)\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)        \n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X9fldFu5nYLu"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.dec_units)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        # used for attention\n",
    "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        \n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        \n",
    "        # score shape == (batch_size, max_length, hidden_size)\n",
    "        score = tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis))\n",
    "        \n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "        \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        # output shape == (batch_size * max_length, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        # output shape == (batch_size * max_length, vocab)\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, state, attention_weights\n",
    "        \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.dec_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ViKc0u3WqVY4"
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(len(unique), embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(len(unique), embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q8Sbf22Yq4sx"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(.00025)\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = 1 - np.equal(real, 0)\n",
    "  loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K9VDoDV3rqCB"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.checkpointable.util.CheckpointLoadStatus at 0x134f8841320>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "UtkqNvVVrtH9",
    "outputId": "0470a286-b70c-493d-fe2f-c5daeee9d49e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rom only a handful of big adveroisebslvn ==>  \n",
      "r laaf i ebnand ul of big ad eroin  s tn  Epoch 9 Batch 0 @5 erroes Loss 1.2288\n",
      "  Epoch 9 Batch 10 @5 erroes Loss 1.3441\n",
      "  Epoch 9 Batch 20 @5 erroes Loss 1.2078\n",
      "  Epoch 9 Batch 30 @5 erroes Loss 1.3311\n",
      "  Epoch 9 Batch 40 @5 erroes Loss 1.1424\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 30\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (inp, targ)) in enumerate(dataset):\n",
    "        loss = 0\n",
    "        \n",
    "        # modify input data to represent error\n",
    "        nb_error = min(max(epoch - 0, 0), 5) # stoping change in first 5 epoch\n",
    "        random_index = np.random.randint(0, max_length, (BATCH_SIZE, nb_error))\n",
    "        random_value = np.random.randint(1, len(unique), (BATCH_SIZE, nb_error))\n",
    "        \n",
    "        for b in range(BATCH_SIZE):\n",
    "            for d in range(nb_error):\n",
    "                inp = inp.numpy()\n",
    "                inp[b, random_index[d, d]] = random_value[b, d]\n",
    "                inp = tf.contrib.eager.Variable(inp)\n",
    "                # tf.scatter_update(inp, [b, random_index[d, d]], random_value[b, d])\n",
    "        \n",
    "        if batch % 50 ==0:\n",
    "            # print one for test\n",
    "            input_string = [idx2char[idx.numpy()] for idx in inp[0]]\n",
    "            for i in input_string:\n",
    "                print(i, end='')\n",
    "            print(' ==>  ')\n",
    "        \n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_output, enc_hidden = encoder(inp, hidden)\n",
    "            \n",
    "            dec_hidden = enc_hidden\n",
    "            \n",
    "            dec_input = tf.expand_dims([char2idx['<start>']] * BATCH_SIZE, 1)       \n",
    "            \n",
    "            # Teacher forcing - feeding the target as the next input\n",
    "            for t in range(0, targ.shape[1]):\n",
    "                # passing enc_output to the decoder\n",
    "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "                \n",
    "                if batch % 50==0:\n",
    "                    output = predictions[0]\n",
    "                    print(idx2char[output.numpy().argmax(-1)], end='')\n",
    "                    \n",
    "                \n",
    "                loss += loss_function(targ[:, t], predictions)\n",
    "                \n",
    "                # using teacher forcing\n",
    "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "        \n",
    "        batch_loss = (loss / int(targ.shape[1]))\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        variables = encoder.variables + decoder.variables\n",
    "        \n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        if batch % 10 == 0:\n",
    "            print('  Epoch {} Batch {} @{} erroes Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         nb_error,\n",
    "                                                         batch_loss.numpy()))\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    \n",
    "    print('Epoch {} Loss {:.4f}\\n'.format(epoch + 1,\n",
    "                                        total_loss / batch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "brZsYa3P_IKd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Spelling_correction.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
