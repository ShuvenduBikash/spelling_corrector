{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9533,
     "status": "ok",
     "timestamp": 1535255969007,
     "user": {
      "displayName": "Shuvendu BIkash",
      "photoUrl": "//lh4.googleusercontent.com/-cb2XgdoysqI/AAAAAAAAAAI/AAAAAAAAAFg/3GJlDMCUuNI/s50-c-k-no/photo.jpg",
      "userId": "114258866435529703404"
     },
     "user_tz": -360
    },
    "id": "Oi7fhhdI5Ima",
    "outputId": "0a3c153a-9c7c-4f0f-9725-b42928573c07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in c:\\users\\bikas\\anaconda3\\lib\\site-packages (1.0.22)\n",
      "Requirement already satisfied: observations in c:\\users\\bikas\\anaconda3\\lib\\site-packages (0.1.4)\n",
      "Requirement already satisfied: numpy>=1.7 in c:\\users\\bikas\\anaconda3\\lib\\site-packages (from observations) (1.14.5)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\bikas\\anaconda3\\lib\\site-packages (from observations) (1.11.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode\n",
    "!pip install observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8258,
     "status": "ok",
     "timestamp": 1535255977367,
     "user": {
      "displayName": "Shuvendu BIkash",
      "photoUrl": "//lh4.googleusercontent.com/-cb2XgdoysqI/AAAAAAAAAAI/AAAAAAAAAFg/3GJlDMCUuNI/s50-c-k-no/photo.jpg",
      "userId": "114258866435529703404"
     },
     "user_tz": -360
    },
    "id": "CRs1lhjh4ttT",
    "outputId": "6180eb10-0649-4415-9c9b-cecfd8aa8d7d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bikas\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "# Import TensorFlow >= 1.10 and enable eager execution\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import unidecode\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EnG1_mD4EOGB"
   },
   "source": [
    "## Load PTB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13829,
     "status": "ok",
     "timestamp": 1535256006703,
     "user": {
      "displayName": "Shuvendu BIkash",
      "photoUrl": "//lh4.googleusercontent.com/-cb2XgdoysqI/AAAAAAAAAAI/AAAAAAAAAFg/3GJlDMCUuNI/s50-c-k-no/photo.jpg",
      "userId": "114258866435529703404"
     },
     "user_tz": -360
    },
    "id": "fpvsxgZOEKkS",
    "outputId": "e4703fe3-8da4-4ca7-e17f-cd44b5074771"
   },
   "outputs": [],
   "source": [
    "import observations\n",
    "text, testfile, valfile = getattr(observations, 'ptb')('data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1131,
     "status": "ok",
     "timestamp": 1535256007949,
     "user": {
      "displayName": "Shuvendu BIkash",
      "photoUrl": "//lh4.googleusercontent.com/-cb2XgdoysqI/AAAAAAAAAAI/AAAAAAAAAFg/3GJlDMCUuNI/s50-c-k-no/photo.jpg",
      "userId": "114258866435529703404"
     },
     "user_tz": -360
    },
    "id": "IOGvsf87EWfJ",
    "outputId": "f47efcad-206c-47e5-f9b1-3bb9f7030d63"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5269890"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1513,
     "status": "ok",
     "timestamp": 1535256009723,
     "user": {
      "displayName": "Shuvendu BIkash",
      "photoUrl": "//lh4.googleusercontent.com/-cb2XgdoysqI/AAAAAAAAAAI/AAAAAAAAAFg/3GJlDMCUuNI/s50-c-k-no/photo.jpg",
      "userId": "114258866435529703404"
     },
     "user_tz": -360
    },
    "id": "RM1Av3BjylpW",
    "outputId": "ecaa3474-f0cd-4e79-9a3c-55c477691311"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4715474\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(w):\n",
    "    w = w.lower()\n",
    "    w = re.sub(\"<eos>\", \" \", w)\n",
    "    w = re.sub(\"<unk>\", \" \", w)\n",
    "    \n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-z?.!,]+\", \" \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    \n",
    "    return w\n",
    "\n",
    "text = preprocess_sentence(text)\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1069,
     "status": "ok",
     "timestamp": 1535256011004,
     "user": {
      "displayName": "Shuvendu BIkash",
      "photoUrl": "//lh4.googleusercontent.com/-cb2XgdoysqI/AAAAAAAAAAI/AAAAAAAAAFg/3GJlDMCUuNI/s50-c-k-no/photo.jpg",
      "userId": "114258866435529703404"
     },
     "user_tz": -360
    },
    "id": "khE448cQ6Xfw",
    "outputId": "f06c5c9d-e9b2-4b24-b2f2-cbd7c2be39c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique characters  29\n"
     ]
    }
   ],
   "source": [
    "# unique contains all the unique characters in the file\n",
    "unique = ['<start>']+ sorted(set(text))\n",
    "\n",
    "# creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(unique)}\n",
    "idx2char = {i:u for i, u in enumerate(unique)}\n",
    "\n",
    "print(\"number of unique characters \", len(unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c4lvvfUS6dSz"
   },
   "outputs": [],
   "source": [
    "# setting the maximum length sentence we want for a single input in characters\n",
    "max_length = 40\n",
    "\n",
    "# length of the vocabulary in chars\n",
    "vocab_size = len(unique)\n",
    "\n",
    "# the embedding dimension \n",
    "embedding_dim = 16\n",
    "\n",
    "# number of RNN (here GRU) units\n",
    "units = 1024\n",
    "\n",
    "# batch size \n",
    "BATCH_SIZE = 48\n",
    "\n",
    "# buffer size to shuffle our dataset\n",
    "BUFFER_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3405,
     "status": "ok",
     "timestamp": 1535256016095,
     "user": {
      "displayName": "Shuvendu BIkash",
      "photoUrl": "//lh4.googleusercontent.com/-cb2XgdoysqI/AAAAAAAAAAI/AAAAAAAAAFg/3GJlDMCUuNI/s50-c-k-no/photo.jpg",
      "userId": "114258866435529703404"
     },
     "user_tz": -360
    },
    "id": "tIuKhrHsl0LI",
    "outputId": "df1ea0e1-af1f-49ab-cf79-fc6d109ed327"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(117886, 40)\n",
      "(117886, 40)\n"
     ]
    }
   ],
   "source": [
    "input_text = []\n",
    "target_text = []\n",
    "\n",
    "for f in range(0, len(text)-max_length, max_length):\n",
    "    inps = text[f:f+max_length]\n",
    "    targ = text[f:f+max_length]\n",
    "\n",
    "    input_text.append([char2idx[i] for i in inps])\n",
    "    target_text.append([char2idx[t] for t in targ])\n",
    "    \n",
    "print (np.array(input_text).shape)\n",
    "print (np.array(target_text).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13345,
     "status": "ok",
     "timestamp": 1535256029536,
     "user": {
      "displayName": "Shuvendu BIkash",
      "photoUrl": "//lh4.googleusercontent.com/-cb2XgdoysqI/AAAAAAAAAAI/AAAAAAAAAFg/3GJlDMCUuNI/s50-c-k-no/photo.jpg",
      "userId": "114258866435529703404"
     },
     "user_tz": -360
    },
    "id": "FrM3H4JMl-D5",
    "outputId": "034adb16-b1fc-467c-8cc5-224d62ec237f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-3bbc2611a7c8>:2: batch_and_drop_remainder (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.batch(..., drop_remainder=True)`.\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((input_text, target_text)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.apply(tf.contrib.data.batch_and_drop_remainder(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RdRPkpZmmtSe"
   },
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J4JrCs-QmCQl"
   },
   "outputs": [],
   "source": [
    "def gru(units):\n",
    "  # If you have a GPU, we recommend using CuDNNGRU(provides a 3x speedup than GRU)\n",
    "  # the code automatically does that.\n",
    "  if tf.test.is_gpu_available():\n",
    "    return tf.keras.layers.CuDNNGRU(units, \n",
    "                                    return_sequences=True, \n",
    "                                    return_state=True,\n",
    "                                    recurrent_initializer='glorot_uniform')\n",
    "  else:\n",
    "    return tf.keras.layers.GRU(units, \n",
    "                               return_sequences=True, \n",
    "                               return_state=True,\n",
    "                               recurrent_initializer='glorot_uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8T4dsBt_nWTh"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.enc_units)\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)        \n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X9fldFu5nYLu"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.dec_units)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        # used for attention\n",
    "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        \n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        \n",
    "        # score shape == (batch_size, max_length, hidden_size)\n",
    "        score = tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis))\n",
    "        \n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "        \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        # output shape == (batch_size * max_length, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        # output shape == (batch_size * max_length, vocab)\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, state, attention_weights\n",
    "        \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.dec_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ViKc0u3WqVY4"
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(len(unique), embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(len(unique), embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q8Sbf22Yq4sx"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(.00025)\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = 1 - np.equal(real, 0)\n",
    "  loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K9VDoDV3rqCB"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.checkpointable.util.CheckpointLoadStatus at 0x280ff6d1550>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "UtkqNvVVrtH9",
    "outputId": "0470a286-b70c-493d-fe2f-c5daeee9d49e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t northern nekousa sskquickly tkeeindust ==>  \n",
      "t northern nekousa sssquickly theeindust  Epoch 35 Batch 0 @5 erroes Loss 0.7066\n",
      "  Epoch 35 Batch 10 @5 erroes Loss 0.6221\n",
      "  Epoch 35 Batch 20 @5 erroes Loss 2.1590\n",
      "  Epoch 35 Batch 30 @5 erroes Loss 2.0558\n",
      "  Epoch 35 Batch 40 @5 erroes Loss 2.0828\n",
      "its appqiration in august of thislyear t ==>  \n",
      "its aniainin nn tn tnt t hif ihen i  l t  Epoch 35 Batch 50 @5 erroes Loss 1.8796\n",
      "  Epoch 35 Batch 60 @5 erroes Loss 1.9217\n",
      "  Epoch 35 Batch 70 @5 erroes Loss 1.7552\n",
      "  Epoch 35 Batch 80 @5 erroes Loss 1.7650\n",
      "  Epoch 35 Batch 90 @5 erroes Loss 1.6567\n",
      "d tk bmpuove mrs. yeargindqorked days an ==>  \n",
      "d th bnpeeme mre  aedreendaauked days an  Epoch 35 Batch 100 @5 erroes Loss 1.5336\n",
      "  Epoch 35 Batch 110 @5 erroes Loss 1.2552\n",
      "  Epoch 35 Batch 120 @5 erroes Loss 1.1213\n",
      "  Epoch 35 Batch 130 @5 erroes Loss 0.9767\n",
      "  Epoch 35 Batch 140 @5 erroes Loss 0.7828\n",
      "rs onsthecnest styles but cosmetici fiem ==>  \n",
      "rs onsthecnest sty es but costesici tosm  Epoch 35 Batch 150 @5 erroes Loss 0.8973\n",
      "  Epoch 35 Batch 160 @5 erroes Loss 0.9482\n",
      "  Epoch 35 Batch 170 @5 erroes Loss 0.9381\n",
      "  Epoch 35 Batch 180 @5 erroes Loss 0.9112\n",
      "  Epoch 35 Batch 190 @5 erroes Loss 0.9562\n",
      "hbmnmt. house is vice president ofwdow j ==>  \n",
      "h mnmt. house is vice president of dow j  Epoch 35 Batch 200 @5 erroes Loss 0.7157\n",
      "  Epoch 35 Batch 210 @5 erroes Loss 0.6616\n",
      "  Epoch 35 Batch 220 @5 erroes Loss 0.7194\n",
      "  Epoch 35 Batch 230 @5 erroes Loss 0.7511\n",
      "  Epoch 35 Batch 240 @5 erroes Loss 0.6344\n",
      "wmre matrhews prtsident.and peteg execut ==>  \n",
      "wmre matrhews prtsine t and prten execut  Epoch 35 Batch 250 @5 erroes Loss 0.7441\n",
      "  Epoch 35 Batch 260 @5 erroes Loss 0.5843\n",
      "  Epoch 35 Batch 270 @5 erroes Loss 0.7398\n",
      "  Epoch 35 Batch 280 @5 erroes Loss 0.6361\n",
      "  Epoch 35 Batch 290 @5 erroes Loss 0.7112\n",
      "uld require nri her actyon in congabss n ==>  \n",
      "uld require nri hen actyon in congabs  n  Epoch 35 Batch 300 @5 erroes Loss 0.7068\n",
      "  Epoch 35 Batch 310 @5 erroes Loss 0.6894\n",
      "  Epoch 35 Batch 320 @5 erroes Loss 0.7965\n",
      "  Epoch 35 Batch 330 @5 erroes Loss 0.6309\n",
      "  Epoch 35 Batch 340 @5 erroes Loss 0.5683\n",
      "se youltalk to up hernnsays lvcal httorn ==>  \n",
      "se youltalk to up hernnsays lecal ht orn  Epoch 35 Batch 350 @5 erroes Loss 0.7215\n",
      "  Epoch 35 Batch 360 @5 erroes Loss 0.7137\n",
      "  Epoch 35 Batch 370 @5 erroes Loss 0.7295\n",
      "  Epoch 35 Batch 380 @5 erroes Loss 0.6273\n",
      "  Epoch 35 Batch 390 @5 erroes Loss 0.6795\n",
      "n be rhised tde at t advadch shows hpw o ==>  \n",
      "n be raised the at thad adch shows haw o  Epoch 35 Batch 400 @5 erroes Loss 0.6440\n",
      "  Epoch 35 Batch 410 @5 erroes Loss 1.0375\n",
      "  Epoch 35 Batch 420 @5 erroes Loss 0.7735\n",
      "  Epoch 35 Batch 430 @5 erroes Loss 0.7569\n",
      "  Epoch 35 Batch 440 @5 erroes Loss 0.7385\n",
      "y fundamentaliqupstions as what ir a tan ==>  \n",
      "y fundamentalisupstions as what ir a tan  Epoch 35 Batch 450 @5 erroes Loss 0.5049\n",
      "  Epoch 35 Batch 460 @5 erroes Loss 0.6446\n",
      "  Epoch 35 Batch 470 @5 erroes Loss 1.4622\n",
      "  Epoch 35 Batch 480 @5 erroes Loss 1.0034\n",
      "  Epoch 35 Batch 490 @5 erroes Loss 0.8246\n",
      "thield the poir from icq effects for tns ==>  \n",
      "thield the poir from ich effects for tn   Epoch 35 Batch 500 @5 erroes Loss 0.8376\n",
      "  Epoch 35 Batch 510 @5 erroes Loss 0.7940\n",
      "  Epoch 35 Batch 520 @5 erroes Loss 0.7617\n",
      "  Epoch 35 Batch 530 @5 erroes Loss 0.7247\n",
      "  Epoch 35 Batch 540 @5 erroes Loss 0.6514\n",
      "dcand exports wjre flat but tjennew stat ==>  \n",
      "dcand exports whre flat but thennew stat  Epoch 35 Batch 550 @5 erroes Loss 0.5482\n",
      "  Epoch 35 Batch 560 @5 erroes Loss 0.5736\n",
      "  Epoch 35 Batch 570 @5 erroes Loss 0.5281\n",
      "  Epoch 35 Batch 580 @5 erroes Loss 0.7171\n",
      "  Epoch 35 Batch 590 @5 erroes Loss 0.6303\n",
      "btwiothing waz gbing to holdxup the sett ==>  \n",
      "btwiot ing war gring to hold up the sett  Epoch 35 Batch 600 @5 erroes Loss 0.6207\n",
      "  Epoch 35 Batch 610 @5 erroes Loss 0.6049\n",
      "  Epoch 35 Batch 620 @5 erroes Loss 0.5457\n",
      "  Epoch 35 Batch 630 @5 erroes Loss 0.5298\n",
      "  Epoch 35 Batch 640 @5 erroes Loss 0.5474\n",
      "e con be purchased separacely its gre te ==>  \n",
      "e con be purchased separacely its gre te  Epoch 35 Batch 650 @5 erroes Loss 0.5992\n",
      "  Epoch 35 Batch 660 @5 erroes Loss 0.6331\n",
      "  Epoch 35 Batch 670 @5 erroes Loss 0.5969\n",
      "  Epoch 35 Batch 680 @5 erroes Loss 0.6297\n",
      "  Epoch 35 Batch 690 @5 erroes Loss 0.5933\n",
      "p exchangeoes coacerned about all itspco ==>  \n",
      "p exchangeoes coacerned about all itspco  Epoch 35 Batch 700 @5 erroes Loss 0.5785\n",
      "  Epoch 35 Batch 710 @5 erroes Loss 0.7124\n",
      "  Epoch 35 Batch 720 @5 erroes Loss 1.0593\n",
      "  Epoch 35 Batch 730 @5 erroes Loss 1.7248\n",
      "  Epoch 35 Batch 740 @5 erroes Loss 1.4567\n",
      "und the answer the team used a baq.ury h ==>  \n",
      "und the answer the aham used a bal uam u  Epoch 35 Batch 750 @5 erroes Loss 0.8347\n",
      "  Epoch 35 Batch 760 @5 erroes Loss 0.8081\n",
      "  Epoch 35 Batch 770 @5 erroes Loss 0.7834\n",
      "  Epoch 35 Batch 780 @5 erroes Loss 0.6010\n",
      "  Epoch 35 Batch 790 @5 erroes Loss 0.6858\n",
      "ies for each trifiat the new rategthahfw ==>  \n",
      "ies for each trifiat the new rategtha ft  Epoch 35 Batch 800 @5 erroes Loss 0.6871\n",
      "  Epoch 35 Batch 810 @5 erroes Loss 0.5749\n",
      "  Epoch 35 Batch 820 @5 erroes Loss 0.7326\n",
      "  Epoch 35 Batch 830 @5 erroes Loss 0.5271\n",
      "  Epoch 35 Batch 840 @5 erroes Loss 0.5305\n",
      "pe rs tofhave lost   second keylcohtract ==>  \n",
      "pe rs tofhave lost t second tey co tract  Epoch 35 Batch 850 @5 erroes Loss 0.6050\n",
      "  Epoch 35 Batch 860 @5 erroes Loss 0.6326\n",
      "  Epoch 35 Batch 870 @5 erroes Loss 0.6044\n",
      "  Epoch 35 Batch 880 @5 erroes Loss 0.6214\n",
      "  Epoch 35 Batch 890 @5 erroes Loss 0.4892\n",
      "t said one knowledgeabl. gnduvidwal if t ==>  \n",
      "t said one knowled eable gnduvid al if t  Epoch 35 Batch 900 @5 erroes Loss 0.6504\n",
      "  Epoch 35 Batch 910 @5 erroes Loss 0.6089\n",
      "  Epoch 35 Batch 920 @5 erroes Loss 2.2084\n",
      "  Epoch 35 Batch 930 @5 erroes Loss 2.0868\n",
      "  Epoch 35 Batch 940 @5 erroes Loss 1.9827\n",
      "n is conwrtllrm by investment associates ==>  \n",
      "n is con henilr ao sn erthanthan  nhnssn  Epoch 35 Batch 950 @5 erroes Loss 1.7307\n",
      "  Epoch 35 Batch 960 @5 erroes Loss 1.4755\n",
      "  Epoch 35 Batch 970 @5 erroes Loss 1.2098\n",
      "  Epoch 35 Batch 980 @5 erroes Loss 0.8979\n",
      "  Epoch 35 Batch 990 @5 erroes Loss 0.6579\n",
      "york is o vccordnng to corporatehtravcl  ==>  \n",
      "york is o vccordnng to corporate thavala  Epoch 35 Batch 1000 @5 erroes Loss 0.7186\n",
      "  Epoch 35 Batch 1010 @5 erroes Loss 0.7079\n",
      "  Epoch 35 Batch 1020 @5 erroes Loss 0.6937\n",
      "  Epoch 35 Batch 1030 @5 erroes Loss 0.6892\n",
      "  Epoch 35 Batch 1040 @5 erroes Loss 0.7726\n",
      "physimay abidityvto serve on tze bench j ==>  \n",
      "physimay abidity to serve on the bench j  Epoch 35 Batch 1050 @5 erroes Loss 0.7128\n",
      "  Epoch 35 Batch 1060 @5 erroes Loss 0.7053\n",
      "  Epoch 35 Batch 1070 @5 erroes Loss 0.4967\n",
      "  Epoch 35 Batch 1080 @5 erroes Loss 0.5303\n",
      "  Epoch 35 Batch 1090 @5 erroes Loss 0.6709\n",
      "the huge new pro.ects wegafrthi strip s  ==>  \n",
      "the huge new pronects we arrth  t   t s   Epoch 35 Batch 1100 @5 erroes Loss 0.6423\n",
      "  Epoch 35 Batch 1110 @5 erroes Loss 0.5380\n",
      "  Epoch 35 Batch 1120 @5 erroes Loss 0.6778\n",
      "  Epoch 35 Batch 1130 @5 erroes Loss 0.6107\n",
      "  Epoch 35 Batch 1140 @5 erroes Loss 0.6314\n",
      "ising agepcies ani ot er seravce compani ==>  \n",
      "ising age cies ani ot er seraece compani  Epoch 35 Batch 1150 @5 erroes Loss 0.6408\n",
      "  Epoch 35 Batch 1160 @5 erroes Loss 0.6247\n",
      "  Epoch 35 Batch 1170 @5 erroes Loss 0.6918\n",
      "  Epoch 35 Batch 1180 @5 erroes Loss 0.6204\n",
      "  Epoch 35 Batch 1190 @5 erroes Loss 0.6008\n",
      "at harvard akthor of nueerousdpopulau bo ==>  \n",
      "at harvard akthor of nueerousdpopulau bo  Epoch 35 Batch 1200 @5 erroes Loss 0.5183\n",
      "  Epoch 35 Batch 1210 @5 erroes Loss 0.6491\n",
      "  Epoch 35 Batch 1220 @5 erroes Loss 0.6477\n",
      "  Epoch 35 Batch 1230 @5 erroes Loss 1.4196\n",
      "  Epoch 35 Batch 1240 @5 erroes Loss 0.9461\n",
      " the bank ananqad the powerdto revoke pa ==>  \n",
      " the bank ana aan the powerdto revoke pa  Epoch 35 Batch 1250 @5 erroes Loss 0.7520\n",
      "  Epoch 35 Batch 1260 @5 erroes Loss 0.4734\n",
      "  Epoch 35 Batch 1270 @5 erroes Loss 0.4929\n",
      "  Epoch 35 Batch 1280 @5 erroes Loss 0.7414\n",
      "  Epoch 35 Batch 1290 @5 erroes Loss 0.6858\n",
      "nswere featured on the jalf on the.life  ==>  \n",
      "nswere featured on the jalt on the jalf   Epoch 35 Batch 1300 @5 erroes Loss 0.6330\n",
      "  Epoch 35 Batch 1310 @5 erroes Loss 0.5729\n",
      "  Epoch 35 Batch 1320 @5 erroes Loss 0.6579\n",
      "  Epoch 35 Batch 1330 @5 erroes Loss 0.6386\n",
      "  Epoch 35 Batch 1340 @5 erroes Loss 0.7068\n",
      " in a trvding ranof of i to n mr. sakaik ==>  \n",
      " in a treding rano  of i to n mr. sakaik  Epoch 35 Batch 1350 @5 erroes Loss 0.7234\n",
      "  Epoch 35 Batch 1360 @5 erroes Loss 0.6390\n",
      "  Epoch 35 Batch 1370 @5 erroes Loss 0.6241\n",
      "  Epoch 35 Batch 1380 @5 erroes Loss 0.7573\n",
      "  Epoch 35 Batch 1390 @5 erroes Loss 0.5463\n",
      "tually sdspendedwtheir new loans tp chjn ==>  \n",
      "tually sdspendedwtheir new loans to chen  Epoch 35 Batch 1400 @5 erroes Loss 0.6313\n",
      "  Epoch 35 Batch 1410 @5 erroes Loss 0.5867\n",
      "  Epoch 35 Batch 1420 @5 erroes Loss 0.7963\n",
      "  Epoch 35 Batch 1430 @5 erroes Loss 0.6644\n",
      "  Epoch 35 Batch 1440 @5 erroes Loss 0.7341\n",
      "ublickand sumh policies arx sold ocly in ==>  \n",
      "ublickand sumh policies ar  sold ocly in  Epoch 35 Batch 1450 @5 erroes Loss 0.6043\n",
      "  Epoch 35 Batch 1460 @5 erroes Loss 1.7376\n",
      "  Epoch 35 Batch 1470 @5 erroes Loss 2.2740\n",
      "  Epoch 35 Batch 1480 @5 erroes Loss 2.2747\n",
      "  Epoch 35 Batch 1490 @5 erroes Loss 2.2195\n",
      "nefdralar told me thei if he had more ca ==>  \n",
      "nefde l   thr  aa then an te tan aa   to  Epoch 35 Batch 1500 @5 erroes Loss 2.1501\n",
      "  Epoch 35 Batch 1510 @5 erroes Loss 2.0125\n",
      "  Epoch 35 Batch 1520 @5 erroes Loss 1.8977\n",
      "  Epoch 35 Batch 1530 @5 erroes Loss 1.8753\n",
      "  Epoch 35 Batch 1540 @5 erroes Loss 1.8885\n",
      "i of xuyingjb  matket makers looking at  ==>  \n",
      "i of tot o  to tat    tat    tann n  tt   Epoch 35 Batch 1550 @5 erroes Loss 1.8634\n",
      "  Epoch 35 Batch 1560 @5 erroes Loss 1.7684\n",
      "  Epoch 35 Batch 1570 @5 erroes Loss 1.6174\n",
      "  Epoch 35 Batch 1580 @5 erroes Loss 1.6644\n",
      "  Epoch 35 Batch 1590 @5 erroes Loss 1.5540\n",
      "lst ways to guarantge fwr ehur system is ==>  \n",
      "lst was  mhmturreng e aurtt rtetu te  is  Epoch 35 Batch 1600 @5 erroes Loss 1.5008\n",
      "  Epoch 35 Batch 1610 @5 erroes Loss 1.3244\n",
      "  Epoch 35 Batch 1620 @5 erroes Loss 1.1122\n",
      "  Epoch 35 Batch 1630 @5 erroes Loss 1.1022\n",
      "  Epoch 35 Batch 1640 @5 erroes Loss 1.0431\n",
      " for me. kcgeycma to arque that amerisan ==>  \n",
      " for me. tegt  sanar areue that amerisan  Epoch 35 Batch 1650 @5 erroes Loss 1.0804\n",
      "  Epoch 35 Batch 1660 @5 erroes Loss 0.8944\n",
      "  Epoch 35 Batch 1670 @5 erroes Loss 0.7516\n",
      "  Epoch 35 Batch 1680 @5 erroes Loss 0.8707\n",
      "  Epoch 35 Batch 1690 @5 erroes Loss 0.7555\n",
      "isco earthquake vilz cquse yhe industry  ==>  \n",
      "isco eartheuake vhll chus  vhe industry   Epoch 35 Batch 1700 @5 erroes Loss 0.8281\n",
      "  Epoch 35 Batch 1710 @5 erroes Loss 0.6596\n",
      "  Epoch 35 Batch 1720 @5 erroes Loss 0.7666\n",
      "  Epoch 35 Batch 1730 @5 erroes Loss 0.8874\n",
      "  Epoch 35 Batch 1740 @5 erroes Loss 0.7843\n",
      "bcgs in sgall companies slares typicblly ==>  \n",
      "bcgs in sgall companies slares thaicilly  Epoch 35 Batch 1750 @5 erroes Loss 0.7937\n",
      "  Epoch 35 Batch 1760 @5 erroes Loss 0.6820\n",
      "  Epoch 35 Batch 1770 @5 erroes Loss 0.7470\n",
      "  Epoch 35 Batch 1780 @5 erroes Loss 0.6817\n",
      "  Epoch 35 Batch 1790 @5 erroes Loss 0.7609\n",
      "s. repc mario b. n.y liwdnrs for the sai ==>  \n",
      "s. repc mario bo n.y liyd    for the sai  Epoch 35 Batch 1800 @5 erroes Loss 0.7544\n",
      "  Epoch 35 Batch 1810 @5 erroes Loss 0.6066\n",
      "  Epoch 35 Batch 1820 @5 erroes Loss 0.6030\n",
      "  Epoch 35 Batch 1830 @5 erroes Loss 0.6528\n",
      "  Epoch 35 Batch 1840 @5 erroes Loss 0.6398\n",
      "its discriqhnazion on the bavis of sexia ==>  \n",
      "its discracinn inn on the bavis of sexia  Epoch 35 Batch 1850 @5 erroes Loss 0.7391\n",
      "  Epoch 35 Batch 1860 @5 erroes Loss 0.6891\n",
      "  Epoch 35 Batch 1870 @5 erroes Loss 0.9313\n",
      "  Epoch 35 Batch 1880 @5 erroes Loss 0.7165\n",
      "  Epoch 35 Batch 1890 @5 erroes Loss 0.5768\n",
      "stribktes wh.skey in the.u.s. and whit r ==>  \n",
      "stribktes wh.skey in the u.s. and whit r  Epoch 35 Batch 1900 @5 erroes Loss 0.6870\n",
      "  Epoch 35 Batch 1910 @5 erroes Loss 0.6107\n",
      "  Epoch 35 Batch 1920 @5 erroes Loss 1.2042\n",
      "  Epoch 35 Batch 1930 @5 erroes Loss 0.9448\n",
      "  Epoch 35 Batch 1940 @5 erroes Loss 0.8459\n",
      "e than in n and sevenrcents moreregan im ==>  \n",
      "e than in n and seven cents morere enann  Epoch 35 Batch 1950 @5 erroes Loss 0.6930\n",
      "  Epoch 35 Batch 1960 @5 erroes Loss 0.6794\n",
      "  Epoch 35 Batch 1970 @5 erroes Loss 0.6897\n",
      "  Epoch 35 Batch 1980 @5 erroes Loss 0.6189\n",
      "  Epoch 35 Batch 1990 @5 erroes Loss 0.4831\n",
      "eylth serpices qn abortion cliiic insst. ==>  \n",
      "eylth serpices qn abortion cliiic inssth  Epoch 35 Batch 2000 @5 erroes Loss 0.6427\n",
      "  Epoch 35 Batch 2010 @5 erroes Loss 0.5737\n",
      "  Epoch 35 Batch 2020 @5 erroes Loss 0.5921\n",
      "  Epoch 35 Batch 2030 @5 erroes Loss 0.5905\n",
      "  Epoch 35 Batch 2040 @5 erroes Loss 0.6533\n",
      "mpleted a five year chnwrfct nor nasa bu ==>  \n",
      "mpleted a five year chn rect nor nasa bu  Epoch 35 Batch 2050 @5 erroes Loss 0.5955\n",
      "  Epoch 35 Batch 2060 @5 erroes Loss 0.7130\n",
      "  Epoch 35 Batch 2070 @5 erroes Loss 0.6623\n",
      "  Epoch 35 Batch 2080 @5 erroes Loss 0.5595\n",
      "  Epoch 35 Batch 2090 @5 erroes Loss 0.6478\n",
      "dlamxd the thirf quarter earnipgs dropqo ==>  \n",
      "dlamed therthirt quarter earnipgs drorlo  Epoch 35 Batch 2100 @5 erroes Loss 0.9787\n",
      "  Epoch 35 Batch 2110 @5 erroes Loss 0.8427\n",
      "  Epoch 35 Batch 2120 @5 erroes Loss 0.6835\n",
      "  Epoch 35 Batch 2130 @5 erroes Loss 0.6722\n",
      "  Epoch 35 Batch 2140 @5 erroes Loss 1.6903\n",
      "nf and nltwokks tandem which alrehdy spe ==>  \n",
      "n  and n  esn e thnde  woichetl esd  tta  Epoch 35 Batch 2150 @5 erroes Loss 1.6871\n",
      "  Epoch 35 Batch 2160 @5 erroes Loss 1.2732\n",
      "  Epoch 35 Batch 2170 @5 erroes Loss 1.2770\n",
      "  Epoch 35 Batch 2180 @5 erroes Loss 0.8172\n",
      "  Epoch 35 Batch 2190 @5 erroes Loss 0.8970\n",
      " classmcal verpdor then messrsk bartlett ==>  \n",
      " classmcal mers ertmeen mess ek bartless  Epoch 35 Batch 2200 @5 erroes Loss 0.8542\n",
      "  Epoch 35 Batch 2210 @5 erroes Loss 0.8130\n",
      "  Epoch 35 Batch 2220 @5 erroes Loss 0.8290\n",
      "  Epoch 35 Batch 2230 @5 erroes Loss 0.6897\n",
      "  Epoch 35 Batch 2240 @5 erroes Loss 0.8803\n",
      ".s. vnc eulope are comforeable wsth the  ==>  \n",
      ".s. vnc eulope are comforeable wsth the   Epoch 35 Batch 2250 @5 erroes Loss 0.6740\n",
      "  Epoch 35 Batch 2260 @5 erroes Loss 0.7825\n",
      "  Epoch 35 Batch 2270 @5 erroes Loss 0.6316\n",
      "  Epoch 35 Batch 2280 @5 erroes Loss 0.7108\n",
      "  Epoch 35 Batch 2290 @5 erroes Loss 0.6714\n",
      " prime minister ynd ckngre s i waich tn  ==>  \n",
      " prime minister ynd congre s i waich tn   Epoch 35 Batch 2300 @5 erroes Loss 0.6697\n",
      "  Epoch 35 Batch 2310 @5 erroes Loss 0.6544\n",
      "  Epoch 35 Batch 2320 @5 erroes Loss 0.6481\n",
      "  Epoch 35 Batch 2330 @5 erroes Loss 0.5726\n",
      "  Epoch 35 Batch 2340 @5 erroes Loss 0.5795\n",
      "e fimstxtfme in n ymars jhat such govern ==>  \n",
      "e fimstht me in n ymars jhat such govern  Epoch 35 Batch 2350 @5 erroes Loss 0.6374\n",
      "  Epoch 35 Batch 2360 @5 erroes Loss 0.5985\n",
      "  Epoch 35 Batch 2370 @5 erroes Loss 0.5231\n",
      "  Epoch 35 Batch 2380 @5 erroes Loss 0.5892\n",
      "  Epoch 35 Batch 2390 @5 erroes Loss 0.6669\n",
      "atpsconserence approvedwan estimatwd n b ==>  \n",
      "atpsconserence approved an estimatwd n b  Epoch 35 Batch 2400 @5 erroes Loss 0.7658\n",
      "  Epoch 35 Batch 2410 @5 erroes Loss 0.7614\n",
      "  Epoch 35 Batch 2420 @5 erroes Loss 0.7179\n",
      "  Epoch 35 Batch 2430 @5 erroes Loss 0.5982\n",
      "  Epoch 35 Batch 2440 @5 erroes Loss 0.6664\n",
      "r wang installations ro oahermdepartmvnt ==>  \n",
      "r wang installations ro ofhermdepartment  Epoch 35 Batch 2450 @5 erroes Loss 0.5745\n",
      "Epoch 35 Loss 0.8738\n",
      "\n",
      "Time taken for 1 epoch 2572.344160079956 sec\n",
      "\n",
      "s compared with jbout cneethird in japal ==>  \n",
      "s compared with jbout cne third in japal  Epoch 36 Batch 0 @5 erroes Loss 0.5357\n",
      "  Epoch 36 Batch 10 @5 erroes Loss 0.6318\n",
      "  Epoch 36 Batch 20 @5 erroes Loss 0.6862\n",
      "  Epoch 36 Batch 30 @5 erroes Loss 0.5985\n",
      "  Epoch 36 Batch 40 @5 erroes Loss 0.5581\n",
      "and a with.tha oas lost lven morerspeed  ==>  \n",
      "and a withetha oat lost l en morerspeed   Epoch 36 Batch 50 @5 erroes Loss 0.6195\n",
      "  Epoch 36 Batch 60 @5 erroes Loss 0.5963\n",
      "  Epoch 36 Batch 70 @5 erroes Loss 0.5928\n",
      "  Epoch 36 Batch 80 @5 erroes Loss 0.5840\n",
      "  Epoch 36 Batch 90 @5 erroes Loss 0.6130\n",
      " says jht compkny now taruets within the ==>  \n",
      " says jht compkny now taruets within the  Epoch 36 Batch 100 @5 erroes Loss 0.5224\n",
      "  Epoch 36 Batch 110 @5 erroes Loss 0.6321\n",
      "  Epoch 36 Batch 120 @5 erroes Loss 0.5899\n",
      "  Epoch 36 Batch 130 @5 erroes Loss 0.5935\n",
      "  Epoch 36 Batch 140 @5 erroes Loss 0.4504\n",
      " in n usez by prohiat traders and othjrs ==>  \n",
      " in n use  by prohiat thaders an  others  Epoch 36 Batch 150 @5 erroes Loss 0.5907\n",
      "  Epoch 36 Batch 160 @5 erroes Loss 0.6152\n",
      "  Epoch 36 Batch 170 @5 erroes Loss 2.2574\n",
      "  Epoch 36 Batch 180 @5 erroes Loss 2.3349\n",
      "  Epoch 36 Batch 190 @5 erroes Loss 2.1133\n",
      "n mvleion foy the n n of teuxrate that d ==>  \n",
      "n mpl  nr t r the t t tn th    n  then t  Epoch 36 Batch 200 @5 erroes Loss 2.1077\n",
      "  Epoch 36 Batch 210 @5 erroes Loss 1.9980\n",
      "  Epoch 36 Batch 220 @5 erroes Loss 2.1266\n",
      "  Epoch 36 Batch 230 @5 erroes Loss 1.9301\n",
      "  Epoch 36 Batch 240 @5 erroes Loss 2.0078\n",
      " eather onnhow to winrnew buginese mr. s ==>  \n",
      " eatheresf sireshrsin s  esin n    tie s  Epoch 36 Batch 250 @5 erroes Loss 1.9113\n",
      "  Epoch 36 Batch 260 @5 erroes Loss 1.8164\n",
      "  Epoch 36 Batch 270 @5 erroes Loss 1.7357\n",
      "  Epoch 36 Batch 280 @5 erroes Loss 1.8826\n",
      "  Epoch 36 Batch 290 @5 erroes Loss 1.7151\n",
      "y andokwned by rupert mundbch s n.ws cor ==>  \n",
      "y andoku    bu sunrr  sune rhesun s  sur  Epoch 36 Batch 300 @5 erroes Loss 1.7095\n",
      "  Epoch 36 Batch 310 @5 erroes Loss 1.6099\n",
      "  Epoch 36 Batch 320 @5 erroes Loss 1.5525\n",
      "  Epoch 36 Batch 330 @5 erroes Loss 1.5153\n",
      "  Epoch 36 Batch 340 @5 erroes Loss 1.3637\n",
      "opos ng a reorganization plantalsolunder ==>  \n",
      "opos ngealseargangnanior alangalselaeaer  Epoch 36 Batch 350 @5 erroes Loss 1.1747\n",
      "  Epoch 36 Batch 360 @5 erroes Loss 1.3150\n",
      "  Epoch 36 Batch 370 @5 erroes Loss 1.2594\n",
      "  Epoch 36 Batch 380 @5 erroes Loss 1.0116\n",
      "  Epoch 36 Batch 390 @5 erroes Loss 1.0512\n",
      " ke ideal than was who wls newer able to ==>  \n",
      " he ideal tha  was who wls wawer able to  Epoch 36 Batch 400 @5 erroes Loss 0.8866\n",
      "  Epoch 36 Batch 410 @5 erroes Loss 0.9606\n",
      "  Epoch 36 Batch 420 @5 erroes Loss 0.9093\n",
      "  Epoch 36 Batch 430 @5 erroes Loss 0.9661\n",
      "  Epoch 36 Batch 440 @5 erroes Loss 0.8092\n",
      "ancy promrezs are eliminatediand to take ==>  \n",
      "ancy promrens are eliminatediane to take  Epoch 36 Batch 450 @5 erroes Loss 0.9066\n",
      "  Epoch 36 Batch 460 @5 erroes Loss 0.6916\n",
      "  Epoch 36 Batch 470 @5 erroes Loss 0.9336\n",
      "  Epoch 36 Batch 480 @5 erroes Loss 0.8218\n",
      "  Epoch 36 Batch 490 @5 erroes Loss 0.8036\n",
      "rnea pigs arl of whom will be mo e tsan  ==>  \n",
      "rnea pigs arl of whom will be mo e ahem   Epoch 36 Batch 500 @5 erroes Loss 0.7544\n",
      "  Epoch 36 Batch 510 @5 erroes Loss 0.6952\n",
      "  Epoch 36 Batch 520 @5 erroes Loss 0.6894\n",
      "  Epoch 36 Batch 530 @5 erroes Loss 0.7796\n",
      "  Epoch 36 Batch 540 @5 erroes Loss 0.9165\n",
      "almbutlsuch a large sompanu would have b ==>  \n",
      "almbutlsuch a large sompanu would have b  Epoch 36 Batch 550 @5 erroes Loss 0.7678\n",
      "  Epoch 36 Batch 560 @5 erroes Loss 0.7632\n",
      "  Epoch 36 Batch 570 @5 erroes Loss 0.8609\n",
      "  Epoch 36 Batch 580 @5 erroes Loss 0.7397\n",
      "  Epoch 36 Batch 590 @5 erroes Loss 0.7449\n",
      "had tuerway fob ntshlink with the small  ==>  \n",
      "had tuerway fob ntshlink with the s all   Epoch 36 Batch 600 @5 erroes Loss 0.6770\n",
      "  Epoch 36 Batch 610 @5 erroes Loss 0.7866\n",
      "  Epoch 36 Batch 620 @5 erroes Loss 0.6804\n",
      "  Epoch 36 Batch 630 @5 erroes Loss 0.7011\n",
      "  Epoch 36 Batch 640 @5 erroes Loss 0.7572\n",
      " on thezbpink sf gtobal power and fame   ==>  \n",
      " on the biink sf gtobal power and fame    Epoch 36 Batch 650 @5 erroes Loss 0.7898\n",
      "  Epoch 36 Batch 660 @5 erroes Loss 0.7198\n",
      "  Epoch 36 Batch 670 @5 erroes Loss 0.6140\n",
      "  Epoch 36 Batch 680 @5 erroes Loss 1.0832\n",
      "  Epoch 36 Batch 690 @5 erroes Loss 0.7898\n",
      "ugs an  the less defeloped cotntrixs let ==>  \n",
      "ugs an  the llss de eloped cotn s o  lot  Epoch 36 Batch 700 @5 erroes Loss 1.0459\n",
      "  Epoch 36 Batch 710 @5 erroes Loss 0.7999\n",
      "  Epoch 36 Batch 720 @5 erroes Loss 0.5543\n",
      "  Epoch 36 Batch 730 @5 erroes Loss 0.6174\n",
      "  Epoch 36 Batch 740 @5 erroes Loss 0.7257\n",
      " of cold dolleg sheet steel anb hot dipw ==>  \n",
      " of cold dolled sheet steel and hot dipw  Epoch 36 Batch 750 @5 erroes Loss 0.5912\n",
      "  Epoch 36 Batch 760 @5 erroes Loss 2.1986\n",
      "  Epoch 36 Batch 770 @5 erroes Loss 1.8968\n",
      "  Epoch 36 Batch 780 @5 erroes Loss 1.5567\n",
      "  Epoch 36 Batch 790 @5 erroes Loss 1.0269\n",
      "tin thn sanctions says shdy are liyely t ==>  \n",
      "tin the sinctions saystshey tne linely t  Epoch 36 Batch 800 @5 erroes Loss 0.8600\n",
      "  Epoch 36 Batch 810 @5 erroes Loss 0.6649\n",
      "  Epoch 36 Batch 820 @5 erroes Loss 0.7198\n",
      "  Epoch 36 Batch 830 @5 erroes Loss 0.5870\n",
      "  Epoch 36 Batch 840 @5 erroes Loss 0.6910\n",
      "n food whicm rose n n oilkxand also didv ==>  \n",
      "n food whicm rose n n oilk and also did   Epoch 36 Batch 850 @5 erroes Loss 0.6435\n",
      "  Epoch 36 Batch 860 @5 erroes Loss 0.7971\n",
      "  Epoch 36 Batch 870 @5 erroes Loss 0.7856\n",
      "  Epoch 36 Batch 880 @5 erroes Loss 0.6898\n",
      "  Epoch 36 Batch 890 @5 erroes Loss 0.6504\n",
      " the jbst of capiral to com.anies esspnt ==>  \n",
      " the sast of capiral to com.anies esspnt  Epoch 36 Batch 900 @5 erroes Loss 0.6213\n",
      "  Epoch 36 Batch 910 @5 erroes Loss 2.0364\n",
      "  Epoch 36 Batch 920 @5 erroes Loss 2.0896\n",
      "  Epoch 36 Batch 930 @5 erroes Loss 2.0466\n",
      "  Epoch 36 Batch 940 @5 erroes Loss 1.9047\n",
      "xpanzion inmthe lkte s jupanese consumer ==>  \n",
      "xpandion ineihe sote s tune e e sone ne   Epoch 36 Batch 950 @5 erroes Loss 1.8044\n",
      "  Epoch 36 Batch 960 @5 erroes Loss 1.6981\n",
      "  Epoch 36 Batch 970 @5 erroes Loss 1.2466\n",
      "  Epoch 36 Batch 980 @5 erroes Loss 0.9238\n",
      "  Epoch 36 Batch 990 @5 erroes Loss 0.7537\n",
      "ncy and n mivlitn vf coins with nhem dax ==>  \n",
      "ncy and n mivlitn tf tivns with nhe  dax  Epoch 36 Batch 1000 @5 erroes Loss 0.8113\n",
      "  Epoch 36 Batch 1010 @5 erroes Loss 0.8047\n",
      "  Epoch 36 Batch 1020 @5 erroes Loss 0.6689\n",
      "  Epoch 36 Batch 1030 @5 erroes Loss 0.6479\n",
      "  Epoch 36 Batch 1040 @5 erroes Loss 0.7574\n",
      "ale s dohsjan estimyted n billcon un ann ==>  \n",
      "ale s dons an entimyted n billcon entany  Epoch 36 Batch 1050 @5 erroes Loss 0.7593\n",
      "  Epoch 36 Batch 1060 @5 erroes Loss 0.7317\n",
      "  Epoch 36 Batch 1070 @5 erroes Loss 0.6225\n",
      "  Epoch 36 Batch 1080 @5 erroes Loss 0.6061\n",
      "  Epoch 36 Batch 1090 @5 erroes Loss 0.7472\n",
      "tnjrship unitlfrom nxcentd the disqribut ==>  \n",
      "tn rship unitlfrom n ctntd the disqribut  Epoch 36 Batch 1100 @5 erroes Loss 0.6636\n",
      "  Epoch 36 Batch 1110 @5 erroes Loss 0.7187\n",
      "  Epoch 36 Batch 1120 @5 erroes Loss 0.5412\n",
      "  Epoch 36 Batch 1130 @5 erroes Loss 0.6535\n",
      "  Epoch 36 Batch 1140 @5 erroes Loss 0.6836\n",
      "ogram costshand nmch mwrl effectively ta ==>  \n",
      "ogram costshand nech merl tffectively ta  Epoch 36 Batch 1150 @5 erroes Loss 0.7646\n",
      "  Epoch 36 Batch 1160 @5 erroes Loss 1.8349\n",
      "  Epoch 36 Batch 1170 @5 erroes Loss 1.9511\n",
      "  Epoch 36 Batch 1180 @5 erroes Loss 1.3545\n",
      "  Epoch 36 Batch 1190 @5 erroes Loss 1.0106\n",
      "oupewill create mpfia such al newspacer  ==>  \n",
      "oupewill create mpfia such al necs acer   Epoch 36 Batch 1200 @5 erroes Loss 0.8443\n",
      "  Epoch 36 Batch 1210 @5 erroes Loss 0.6089\n",
      "  Epoch 36 Batch 1220 @5 erroes Loss 0.6624\n",
      "  Epoch 36 Batch 1230 @5 erroes Loss 0.7704\n",
      "  Epoch 36 Batch 1240 @5 erroes Loss 0.6451\n",
      "asonaxle freqiency ns is blacks conszitu ==>  \n",
      "asonanle fee ue  e en sn se iee ie   ee   Epoch 36 Batch 1250 @5 erroes Loss 2.0955\n",
      "  Epoch 36 Batch 1260 @5 erroes Loss 2.4781\n",
      "  Epoch 36 Batch 1270 @5 erroes Loss 2.3194\n",
      "  Epoch 36 Batch 1280 @5 erroes Loss 2.1449\n",
      "  Epoch 36 Batch 1290 @5 erroes Loss 2.1424\n",
      "es with bcl.south sxand to s.in off its  ==>  \n",
      "es win esi  i nshes sn  shns on sn  sn    Epoch 36 Batch 1300 @5 erroes Loss 2.0101\n",
      "  Epoch 36 Batch 1310 @5 erroes Loss 2.0791\n",
      "  Epoch 36 Batch 1320 @5 erroes Loss 2.0592\n",
      "  Epoch 36 Batch 1330 @5 erroes Loss 2.0660\n",
      "  Epoch 36 Batch 1340 @5 erroes Loss 2.1207\n",
      "pe constantln trying po find waysgto reg ==>  \n",
      "pe con   n  e the n  thnton  nan  nhntes  Epoch 36 Batch 1350 @5 erroes Loss 2.0615\n",
      "  Epoch 36 Batch 1360 @5 erroes Loss 1.9886\n",
      "  Epoch 36 Batch 1370 @5 erroes Loss 1.9866\n",
      "  Epoch 36 Batch 1380 @5 erroes Loss 1.9662\n",
      "  Epoch 36 Batch 1390 @5 erroes Loss 1.9666\n",
      "hws which i mayanot oe pagd for vntil ap ==>  \n",
      "hes whenoetntan t   aa aand aon ta  n tn  Epoch 36 Batch 1400 @5 erroes Loss 1.9360\n",
      "  Epoch 36 Batch 1410 @5 erroes Loss 1.9118\n",
      "  Epoch 36 Batch 1420 @5 erroes Loss 2.0081\n",
      "  Epoch 36 Batch 1430 @5 erroes Loss 1.7512\n",
      "  Epoch 36 Batch 1440 @5 erroes Loss 1.8330\n",
      "ived there the pr.perty night have byex  ==>  \n",
      "ived the   bhe trene    tan ehtene bu     Epoch 36 Batch 1450 @5 erroes Loss 1.6756\n",
      "  Epoch 36 Batch 1460 @5 erroes Loss 1.7605\n",
      "  Epoch 36 Batch 1470 @5 erroes Loss 1.6690\n",
      "  Epoch 36 Batch 1480 @5 erroes Loss 1.6989\n",
      "  Epoch 36 Batch 1490 @5 erroes Loss 1.5846\n",
      "l chinese plants to bernimilarlq thewwdr ==>  \n",
      "l chines  wlelti thraerw npsllei the wer  Epoch 36 Batch 1500 @5 erroes Loss 1.4741\n",
      "  Epoch 36 Batch 1510 @5 erroes Loss 1.6031\n",
      "  Epoch 36 Batch 1520 @5 erroes Loss 1.3884\n",
      "  Epoch 36 Batch 1530 @5 erroes Loss 1.3901\n",
      "  Epoch 36 Batch 1540 @5 erroes Loss 1.4105\n",
      "e bgdly burned in the third querter by.i ==>  \n",
      "e badly burier in thirthir  purreer by.i  Epoch 36 Batch 1550 @5 erroes Loss 1.2102\n",
      "  Epoch 36 Batch 1560 @5 erroes Loss 1.2331\n",
      "  Epoch 36 Batch 1570 @5 erroes Loss 1.1694\n",
      "  Epoch 36 Batch 1580 @5 erroes Loss 0.9944\n",
      "  Epoch 36 Batch 1590 @5 erroes Loss 0.8997\n",
      "inflatifn  hichytfe purchasing power uf  ==>  \n",
      "in latifn uoechashi purchasing power uf   Epoch 36 Batch 1600 @5 erroes Loss 0.9917\n",
      "  Epoch 36 Batch 1610 @5 erroes Loss 0.9624\n",
      "  Epoch 36 Batch 1620 @5 erroes Loss 1.0075\n",
      "  Epoch 36 Batch 1630 @5 erroes Loss 0.9195\n",
      "  Epoch 36 Batch 1640 @5 erroes Loss 0.9044\n",
      " n and honeswelb rose n n toan nmowe flo ==>  \n",
      " n and hones e l rose n notofn noto  foo  Epoch 36 Batch 1650 @5 erroes Loss 0.8538\n",
      "  Epoch 36 Batch 1660 @5 erroes Loss 0.8223\n",
      "  Epoch 36 Batch 1670 @5 erroes Loss 0.7533\n",
      "  Epoch 36 Batch 1680 @5 erroes Loss 0.8881\n",
      "  Epoch 36 Batch 1690 @5 erroes Loss 0.8447\n",
      "kductions byimr. and  rs. petvr s. rubin ==>  \n",
      "kductions by iet ind brt. petvr su rubin  Epoch 36 Batch 1700 @5 erroes Loss 0.8097\n",
      "  Epoch 36 Batch 1710 @5 erroes Loss 0.7411\n",
      "  Epoch 36 Batch 1720 @5 erroes Loss 0.5092\n",
      "  Epoch 36 Batch 1730 @5 erroes Loss 0.6035\n",
      "  Epoch 36 Batch 1740 @5 erroes Loss 0.7010\n",
      "souida from firms that orc paying tmploy ==>  \n",
      "souida frol fivis that orc saying tmolof  Epoch 36 Batch 1750 @5 erroes Loss 0.7463\n",
      "  Epoch 36 Batch 1760 @5 erroes Loss 0.8141\n",
      "  Epoch 36 Batch 1770 @5 erroes Loss 0.6369\n",
      "  Epoch 36 Batch 1780 @5 erroes Loss 0.9026\n",
      "  Epoch 36 Batch 1790 @5 erroes Loss 0.6633\n",
      "for obcer gexeral purposes the ewfective ==>  \n",
      "for obcer gexeral purposes the ewfective  Epoch 36 Batch 1800 @5 erroes Loss 0.6372\n",
      "  Epoch 36 Batch 1810 @5 erroes Loss 0.6043\n",
      "  Epoch 36 Batch 1820 @5 erroes Loss 0.7477\n",
      "  Epoch 36 Batch 1830 @5 erroes Loss 0.6844\n",
      "  Epoch 36 Batch 1840 @5 erroes Loss 0.6247\n",
      "rom the lyte tzlts al part of its origin ==>  \n",
      "rom the lytt tbh ttsttton  orig t lfts t  Epoch 36 Batch 1850 @5 erroes Loss 0.7606\n",
      "  Epoch 36 Batch 1860 @5 erroes Loss 0.4332\n",
      "  Epoch 36 Batch 1870 @5 erroes Loss 0.6767\n",
      "  Epoch 36 Batch 1880 @5 erroes Loss 0.7219\n",
      "  Epoch 36 Batch 1890 @5 erroes Loss 0.7138\n",
      "l highway relieaofor each statetbor each ==>  \n",
      "l higheay relieaoear each statetbor each  Epoch 36 Batch 1900 @5 erroes Loss 0.7045\n",
      "  Epoch 36 Batch 1910 @5 erroes Loss 0.6756\n",
      "  Epoch 36 Batch 1920 @5 erroes Loss 0.6610\n",
      "  Epoch 36 Batch 1930 @5 erroes Loss 0.6201\n",
      "  Epoch 36 Batch 1940 @5 erroes Loss 0.6097\n",
      "as been seekqng a buyer forythe facklify ==>  \n",
      "as been seekeng a buyer forythe facklify  Epoch 36 Batch 1950 @5 erroes Loss 0.5197\n",
      "  Epoch 36 Batch 1960 @5 erroes Loss 0.4736\n",
      "  Epoch 36 Batch 1970 @5 erroes Loss 0.6138\n",
      "  Epoch 36 Batch 1980 @5 erroes Loss 1.3328\n",
      "  Epoch 36 Batch 1990 @5 erroes Loss 1.1021\n",
      "sunielsen marketing regearch nidlsln wle ==>  \n",
      "sunielsen warketing regearch nidletn wae  Epoch 36 Batch 2000 @5 erroes Loss 0.8006\n",
      "  Epoch 36 Batch 2010 @5 erroes Loss 0.6663\n",
      "  Epoch 36 Batch 2020 @5 erroes Loss 0.7579\n",
      "  Epoch 36 Batch 2030 @5 erroes Loss 0.6614\n",
      "  Epoch 36 Batch 2040 @5 erroes Loss 0.6894\n",
      "eekshmr. gordberg says thz cast of theqe ==>  \n",
      "eekshmr. gord erg says the cast of there  Epoch 36 Batch 2050 @5 erroes Loss 0.7191\n",
      "  Epoch 36 Batch 2060 @5 erroes Loss 0.5669\n",
      "  Epoch 36 Batch 2070 @5 erroes Loss 0.6668\n",
      "  Epoch 36 Batch 2080 @5 erroes Loss 0.6533\n",
      "  Epoch 36 Batch 2090 @5 erroes Loss 1.5108\n",
      " aue lctually negative for sevzral yeaos ==>  \n",
      " aue lctually negative for severay yegrs  Epoch 36 Batch 2100 @5 erroes Loss 0.7755\n",
      "  Epoch 36 Batch 2110 @5 erroes Loss 0.8768\n",
      "  Epoch 36 Batch 2120 @5 erroes Loss 0.6765\n",
      "  Epoch 36 Batch 2130 @5 erroes Loss 0.6960\n",
      "  Epoch 36 Batch 2140 @5 erroes Loss 0.7869\n",
      "payeytly sp there are no easyychoices mn ==>  \n",
      "payeytly s  there are no easyycho tes mn  Epoch 36 Batch 2150 @5 erroes Loss 0.6250\n",
      "  Epoch 36 Batch 2160 @5 erroes Loss 0.6421\n",
      "  Epoch 36 Batch 2170 @5 erroes Loss 0.6473\n",
      "  Epoch 36 Batch 2180 @5 erroes Loss 0.6454\n",
      "  Epoch 36 Batch 2190 @5 erroes Loss 0.6954\n",
      "ztrosecution nemo sat on the desks ofpju ==>  \n",
      " trosecution nemo sat on the desks ofpju  Epoch 36 Batch 2200 @5 erroes Loss 0.6411\n",
      "  Epoch 36 Batch 2210 @5 erroes Loss 0.6020\n",
      "  Epoch 36 Batch 2220 @5 erroes Loss 0.6059\n",
      "  Epoch 36 Batch 2230 @5 erroes Loss 0.6502\n",
      "  Epoch 36 Batch 2240 @5 erroes Loss 0.5947\n",
      "stmp said jerry w. a penqoq computibg ma ==>  \n",
      "stmp said jerry wh a pen or computin  ma  Epoch 36 Batch 2250 @5 erroes Loss 0.6061\n",
      "  Epoch 36 Batch 2260 @5 erroes Loss 0.6556\n",
      "  Epoch 36 Batch 2270 @5 erroes Loss 0.4495\n",
      "  Epoch 36 Batch 2280 @5 erroes Loss 0.6150\n",
      "  Epoch 36 Batch 2290 @5 erroes Loss 0.6100\n",
      " ihkuy canada and japak the so called al ==>  \n",
      " inkuy canada and japak the so called al  Epoch 36 Batch 2300 @5 erroes Loss 0.5833\n",
      "  Epoch 36 Batch 2310 @5 erroes Loss 0.6163\n",
      "  Epoch 36 Batch 2320 @5 erroes Loss 0.6271\n",
      "  Epoch 36 Batch 2330 @5 erroes Loss 0.5697\n",
      "  Epoch 36 Batch 2340 @5 erroes Loss 0.6012\n",
      " some shaky foreigb loavs manufaatureds  ==>  \n",
      " some shaky fore g  loavs manui  oore g   Epoch 36 Batch 2350 @5 erroes Loss 0.5530\n",
      "  Epoch 36 Batch 2360 @5 erroes Loss 0.6830\n",
      "  Epoch 36 Batch 2370 @5 erroes Loss 0.4940\n",
      "  Epoch 36 Batch 2380 @5 erroes Loss 0.5055\n",
      "  Epoch 36 Batch 2390 @5 erroes Loss 0.5860\n",
      "he year s xiggestcdrop on monday when tc ==>  \n",
      "he year s siggest drop on monday when th  Epoch 36 Batch 2400 @5 erroes Loss 0.4993\n",
      "  Epoch 36 Batch 2410 @5 erroes Loss 0.6018\n",
      "  Epoch 36 Batch 2420 @5 erroes Loss 0.5038\n",
      "  Epoch 36 Batch 2430 @5 erroes Loss 0.6155\n",
      "  Epoch 36 Batch 2440 @5 erroes Loss 0.6174\n",
      "ducts plosed atxnvq sharl down n cents p ==>  \n",
      "ducts plosed atsntsnt  senn arltdsha l d  Epoch 36 Batch 2450 @5 erroes Loss 0.6750\n",
      "Epoch 36 Loss 1.0125\n",
      "\n",
      "Time taken for 1 epoch 2720.246975183487 sec\n",
      "\n",
      "radipg in stocos and ztodk indexufutures ==>  \n",
      "radipg in stocos and stodk ind xufutures  Epoch 37 Batch 0 @5 erroes Loss 0.6037\n",
      "  Epoch 37 Batch 10 @5 erroes Loss 0.6463\n",
      "  Epoch 37 Batch 20 @5 erroes Loss 0.5593\n",
      "  Epoch 37 Batch 30 @5 erroes Loss 0.6651\n",
      "  Epoch 37 Batch 40 @5 erroes Loss 0.5738\n",
      " hiveebeen only eelen otheg times in n n ==>  \n",
      " hiveebeen only eelen ofheg thnes in n n  Epoch 37 Batch 50 @5 erroes Loss 0.6433\n",
      "  Epoch 37 Batch 60 @5 erroes Loss 0.4396\n",
      "  Epoch 37 Batch 70 @5 erroes Loss 0.5249\n",
      "  Epoch 37 Batch 80 @5 erroes Loss 0.5174\n",
      "  Epoch 37 Batch 90 @5 erroes Loss 0.6169\n",
      "responswu n yes it s high.xutci ll take  ==>  \n",
      "respons u n tes it s his ecutci ll take   Epoch 37 Batch 100 @5 erroes Loss 0.6817\n",
      "  Epoch 37 Batch 110 @5 erroes Loss 1.0682\n",
      "  Epoch 37 Batch 120 @5 erroes Loss 1.3628\n",
      "  Epoch 37 Batch 130 @5 erroes Loss 1.1068\n",
      "  Epoch 37 Batch 140 @5 erroes Loss 0.9052\n",
      " junk bond tradls in los an.tles contrib ==>  \n",
      " junk bond tradls in los an tles contrib  Epoch 37 Batch 150 @5 erroes Loss 0.8003\n",
      "  Epoch 37 Batch 160 @5 erroes Loss 0.7767\n",
      "  Epoch 37 Batch 170 @5 erroes Loss 0.7006\n",
      "  Epoch 37 Batch 180 @5 erroes Loss 0.5504\n",
      "  Epoch 37 Batch 190 @5 erroes Loss 0.6082\n",
      "vuying uynjapalese institutions they moi ==>  \n",
      "vuying uynjapalese institutions they moi  Epoch 37 Batch 200 @5 erroes Loss 0.6473\n",
      "  Epoch 37 Batch 210 @5 erroes Loss 0.7451\n",
      "  Epoch 37 Batch 220 @5 erroes Loss 0.6501\n",
      "  Epoch 37 Batch 230 @5 erroes Loss 0.7665\n",
      "  Epoch 37 Batch 240 @5 erroes Loss 0.5932\n",
      "ofelhe worli s who ispa locgtime family  ==>  \n",
      "ofelhe worli s who ispa locglime family   Epoch 37 Batch 250 @5 erroes Loss 0.6634\n",
      "  Epoch 37 Batch 260 @5 erroes Loss 0.6899\n",
      "  Epoch 37 Batch 270 @5 erroes Loss 0.6205\n",
      "  Epoch 37 Batch 280 @5 erroes Loss 0.8311\n",
      "  Epoch 37 Batch 290 @5 erroes Loss 0.6830\n",
      "e out arqund a bit amks a few qucstionsd ==>  \n",
      "e out areund a bit amks a few tucstions   Epoch 37 Batch 300 @5 erroes Loss 0.6939\n",
      "  Epoch 37 Batch 310 @5 erroes Loss 0.4764\n",
      "  Epoch 37 Batch 320 @5 erroes Loss 0.6374\n",
      "  Epoch 37 Batch 330 @5 erroes Loss 0.6389\n",
      "  Epoch 37 Batch 340 @5 erroes Loss 0.6324\n",
      "xpansivn undersamerican brandshzith an b ==>  \n",
      "xpansivn undersamerican brandshbith an b  Epoch 37 Batch 350 @5 erroes Loss 0.6352\n",
      "  Epoch 37 Batch 360 @5 erroes Loss 0.6065\n",
      "  Epoch 37 Batch 370 @5 erroes Loss 0.5832\n",
      "  Epoch 37 Batch 380 @5 erroes Loss 0.6186\n",
      "  Epoch 37 Batch 390 @5 erroes Loss 0.6079\n",
      "i h n centl a shart w year .arlier a thi ==>  \n",
      "i h n centl a shart wrtear sart er a thi  Epoch 37 Batch 400 @5 erroes Loss 0.6038\n",
      "  Epoch 37 Batch 410 @5 erroes Loss 0.5908\n",
      "  Epoch 37 Batch 420 @5 erroes Loss 0.5565\n",
      "  Epoch 37 Batch 430 @5 erroes Loss 0.5933\n",
      "  Epoch 37 Batch 440 @5 erroes Loss 0.5506\n",
      "rpen about co munitizs and developwrs ge ==>  \n",
      "rpen about co mrs gan  an  e  er  and de  Epoch 37 Batch 450 @5 erroes Loss 0.5834\n",
      "  Epoch 37 Batch 460 @5 erroes Loss 0.6579\n",
      "  Epoch 37 Batch 470 @5 erroes Loss 0.5796\n",
      "  Epoch 37 Batch 480 @5 erroes Loss 0.6079\n",
      "  Epoch 37 Batch 490 @5 erroes Loss 0.6946\n",
      "leagues bill douglap and  n olf buwdy fr ==>  \n",
      "leagues bill douglap and hn olf buw   fr  Epoch 37 Batch 500 @5 erroes Loss 0.5158\n",
      "  Epoch 37 Batch 510 @5 erroes Loss 0.5934\n",
      "  Epoch 37 Batch 520 @5 erroes Loss 2.4014\n",
      "  Epoch 37 Batch 530 @5 erroes Loss 3.8352\n",
      "  Epoch 37 Batch 540 @5 erroes Loss 3.2508\n",
      " lib.xal environmenttl let j not forget  ==>  \n",
      " t si iiii  isi  i  i iii it t  it  i  i  Epoch 37 Batch 550 @5 erroes Loss 2.9276\n",
      "  Epoch 37 Batch 560 @5 erroes Loss 2.6995\n",
      "  Epoch 37 Batch 570 @5 erroes Loss 2.6871\n",
      "  Epoch 37 Batch 580 @5 erroes Loss 2.6890\n",
      "  Epoch 37 Batch 590 @5 erroes Loss 2.6722\n",
      "it s not lckx the bonrd .anadecide by it ==>  \n",
      "in n n r nen  n e nern  nen n  en  ne nn  Epoch 37 Batch 600 @5 erroes Loss 2.5914\n",
      "  Epoch 37 Batch 610 @5 erroes Loss 2.6051\n",
      "  Epoch 37 Batch 620 @5 erroes Loss 2.6407\n",
      "  Epoch 37 Batch 630 @5 erroes Loss 2.5514\n",
      "  Epoch 37 Batch 640 @5 erroes Loss 2.5013\n",
      "g sale ofehalf of its whidagorrefinery o ==>  \n",
      "g sssssss s ss ss ssssss sss ssss s ss s  Epoch 37 Batch 650 @5 erroes Loss 2.5300\n",
      "  Epoch 37 Batch 660 @5 erroes Loss 2.5116\n",
      "  Epoch 37 Batch 670 @5 erroes Loss 2.4745\n",
      "  Epoch 37 Batch 680 @5 erroes Loss 2.4996\n",
      "  Epoch 37 Batch 690 @5 erroes Loss 2.4881\n",
      "hev s have mfre to wearifrom mr. mobbach ==>  \n",
      "he et t  e t r  t rt    t  r t   t r e    Epoch 37 Batch 700 @5 erroes Loss 2.4330\n",
      "  Epoch 37 Batch 710 @5 erroes Loss 2.4858\n",
      "  Epoch 37 Batch 720 @5 erroes Loss 2.4635\n",
      "  Epoch 37 Batch 730 @5 erroes Loss 2.5005\n",
      "  Epoch 37 Batch 740 @5 erroes Loss 2.4311\n",
      "n in tacp the cooperation of dhose finan ==>  \n",
      "nnin i ni i i iirr   n nr ir i ir  i n n  Epoch 37 Batch 750 @5 erroes Loss 2.5244\n",
      "  Epoch 37 Batch 760 @5 erroes Loss 2.4534\n",
      "  Epoch 37 Batch 770 @5 erroes Loss 2.3979\n",
      "  Epoch 37 Batch 780 @5 erroes Loss 2.4168\n",
      "  Epoch 37 Batch 790 @5 erroes Loss 2.5193\n",
      "fid her ett pacnard s mr.nit s going to  ==>  \n",
      "fic ce      c da d  c c   cc c c nc  c n  Epoch 37 Batch 800 @5 erroes Loss 2.5095\n",
      "  Epoch 37 Batch 810 @5 erroes Loss 2.4200\n",
      "  Epoch 37 Batch 820 @5 erroes Loss 2.4035\n",
      "  Epoch 37 Batch 830 @5 erroes Loss 2.3845\n",
      "  Epoch 37 Batch 840 @5 erroes Loss 2.3877\n",
      "nit will test m rkst a n z line of bottl ==>  \n",
      "niniteneeti  itenee itnt  eten  tnetenii  Epoch 37 Batch 850 @5 erroes Loss 2.3991\n",
      "  Epoch 37 Batch 860 @5 erroes Loss 2.3972\n",
      "  Epoch 37 Batch 870 @5 erroes Loss 2.5452\n",
      "  Epoch 37 Batch 880 @5 erroes Loss 2.3977\n",
      "  Epoch 37 Batch 890 @5 erroes Loss 2.4005\n",
      "ved up io its responsibiljtiesjunder the ==>  \n",
      "ve  tn t ntn  t    n  nenen n  tn    t e  Epoch 37 Batch 900 @5 erroes Loss 2.3503\n",
      "  Epoch 37 Batch 910 @5 erroes Loss 2.4571\n",
      "  Epoch 37 Batch 920 @5 erroes Loss 2.3579\n",
      "  Epoch 37 Batch 930 @5 erroes Loss 2.5086\n",
      "  Epoch 37 Batch 940 @5 erroes Loss 2.3284\n",
      "yment owed ho mca inc. injcocnoction wit ==>  \n",
      "y e   tn   t nteontn o tn ton   o nn t n  Epoch 37 Batch 950 @5 erroes Loss 2.3740\n",
      "  Epoch 37 Batch 960 @5 erroes Loss 2.3347\n",
      "  Epoch 37 Batch 970 @5 erroes Loss 2.3834\n",
      "  Epoch 37 Batch 980 @5 erroes Loss 2.3858\n",
      "  Epoch 37 Batch 990 @5 erroes Loss 2.3691\n",
      "cents a share in the cgrrninggvalze bf t ==>  \n",
      "ce    tnt en  tn t e ton   n  ten n tn t  Epoch 37 Batch 1000 @5 erroes Loss 2.3705\n",
      "  Epoch 37 Batch 1010 @5 erroes Loss 2.3581\n",
      "  Epoch 37 Batch 1020 @5 erroes Loss 2.3967\n",
      "  Epoch 37 Batch 1030 @5 erroes Loss 2.3395\n",
      "  Epoch 37 Batch 1040 @5 erroes Loss 2.3980\n",
      " exchangu comkiusion the govirnment has  ==>  \n",
      " e  oer   aoreen  nr a e a re e e   aer   Epoch 37 Batch 1050 @5 erroes Loss 2.3788\n",
      "  Epoch 37 Batch 1060 @5 erroes Loss 2.4338\n",
      "  Epoch 37 Batch 1070 @5 erroes Loss 2.3623\n",
      "  Epoch 37 Batch 1080 @5 erroes Loss 2.3495\n",
      "  Epoch 37 Batch 1090 @5 erroes Loss 2.3612\n",
      "at he seemomtohoo it thefpolitical risk  ==>  \n",
      "at te t   e t rtr tn t e tor n non t n    Epoch 37 Batch 1100 @5 erroes Loss 2.3595\n",
      "  Epoch 37 Batch 1110 @5 erroes Loss 2.3317\n",
      "  Epoch 37 Batch 1120 @5 erroes Loss 2.3132\n",
      "  Epoch 37 Batch 1130 @5 erroes Loss 2.3360\n",
      "  Epoch 37 Batch 1140 @5 erroes Loss 2.3621\n",
      ". andfbelts sand to the plmnts thy beach ==>  \n",
      ". an  aa e  a n  a ra e aaen   a e aa na  Epoch 37 Batch 1150 @5 erroes Loss 2.3803\n",
      "  Epoch 37 Batch 1160 @5 erroes Loss 2.3194\n",
      "  Epoch 37 Batch 1170 @5 erroes Loss 2.4500\n",
      "  Epoch 37 Batch 1180 @5 erroes Loss 2.3355\n",
      "  Epoch 37 Batch 1190 @5 erroes Loss 2.3524\n",
      " certaij reinkurance bontqactsaand obtai ==>  \n",
      " co e nn ae n  nen o aon  eno  an  ane n  Epoch 37 Batch 1200 @5 erroes Loss 2.3331\n",
      "  Epoch 37 Batch 1210 @5 erroes Loss 2.4101\n",
      "  Epoch 37 Batch 1220 @5 erroes Loss 2.4268\n",
      "  Epoch 37 Batch 1230 @5 erroes Loss 2.3078\n",
      "  Epoch 37 Batch 1240 @5 erroes Loss 2.3425\n",
      "n zeta s.a.zthl leading spanisoqmagazjne ==>  \n",
      "n te hst  s the te s n  t es n etes sen   Epoch 37 Batch 1250 @5 erroes Loss 2.2920\n",
      "  Epoch 37 Batch 1260 @5 erroes Loss 2.3295\n",
      "  Epoch 37 Batch 1270 @5 erroes Loss 2.3329\n",
      "  Epoch 37 Batch 1280 @5 erroes Loss 2.3496\n",
      "  Epoch 37 Batch 1290 @5 erroes Loss 2.3309\n",
      "off n.lents for the day gtop loss orders ==>  \n",
      "of  t to  h t f the t n t hfot f  tf      Epoch 37 Batch 1300 @5 erroes Loss 2.3310\n",
      "  Epoch 37 Batch 1310 @5 erroes Loss 2.2908\n",
      "  Epoch 37 Batch 1320 @5 erroes Loss 2.2781\n",
      "  Epoch 37 Batch 1330 @5 erroes Loss 2.3139\n",
      "  Epoch 37 Batch 1340 @5 erroes Loss 2.3151\n",
      "vies uo theaters ovlrsbas and eventualxy ==>  \n",
      "vin  a na e n  e ane e  n an  a e   nnee  Epoch 37 Batch 1350 @5 erroes Loss 2.3051\n",
      "  Epoch 37 Batch 1360 @5 erroes Loss 2.3319\n",
      "  Epoch 37 Batch 1370 @5 erroes Loss 2.3201\n",
      "  Epoch 37 Batch 1380 @5 erroes Loss 2.3437\n",
      "  Epoch 37 Batch 1390 @5 erroes Loss 2.3282\n",
      "rlixr iuarter the divwrsiyied el ctronic ==>  \n",
      "rlin  tunn    the t ne   n n  t   eh r n  Epoch 37 Batch 1400 @5 erroes Loss 2.3003\n",
      "  Epoch 37 Batch 1410 @5 erroes Loss 2.3006\n",
      "  Epoch 37 Batch 1420 @5 erroes Loss 2.3211\n",
      "  Epoch 37 Batch 1430 @5 erroes Loss 2.5200\n",
      "  Epoch 37 Batch 1440 @5 erroes Loss 2.3155\n",
      "onzalez has certainly placed aulot oa de ==>  \n",
      "on ene eten to ehnn e toene  tnterhtr t   Epoch 37 Batch 1450 @5 erroes Loss 2.3371\n",
      "  Epoch 37 Batch 1460 @5 erroes Loss 2.2707\n",
      "  Epoch 37 Batch 1470 @5 erroes Loss 2.3318\n",
      "  Epoch 37 Batch 1480 @5 erroes Loss 2.3064\n",
      "  Epoch 37 Batch 1490 @5 erroes Loss 2.2772\n",
      " revenuegrose nlnipo n billion frol n bi ==>  \n",
      " re e  n ter  t t thrt teneenr t eret te  Epoch 37 Batch 1500 @5 erroes Loss 2.3128\n",
      "  Epoch 37 Batch 1510 @5 erroes Loss 2.3040\n",
      "  Epoch 37 Batch 1520 @5 erroes Loss 2.3702\n",
      "  Epoch 37 Batch 1530 @5 erroes Loss 2.3368\n",
      "  Epoch 37 Batch 1540 @5 erroes Loss 2.2968\n",
      "fore the .frnt buy out bis but later red ==>  \n",
      "for  the ton  hton trnhton tonhtir   te   Epoch 37 Batch 1550 @5 erroes Loss 2.2802\n",
      "  Epoch 37 Batch 1560 @5 erroes Loss 2.2910\n",
      "  Epoch 37 Batch 1570 @5 erroes Loss 2.2559\n",
      "  Epoch 37 Batch 1580 @5 erroes Loss 2.3004\n",
      "  Epoch 37 Batch 1590 @5 erroes Loss 2.2706\n",
      " garcthiand his wife weremfound gxilty b ==>  \n",
      " g n onnan  aen ain  ai a aonn  a nni  a  Epoch 37 Batch 1600 @5 erroes Loss 2.2362\n",
      "  Epoch 37 Batch 1610 @5 erroes Loss 2.3700\n",
      "  Epoch 37 Batch 1620 @5 erroes Loss 2.2592\n",
      "  Epoch 37 Batch 1630 @5 erroes Loss 2.3203\n",
      "  Epoch 37 Batch 1640 @5 erroes Loss 2.2680\n",
      "ts sar shipmmntx of hot dxdped galvanize ==>  \n",
      "ts a n t enra  h ar aerha nrr  a neen ne  Epoch 37 Batch 1650 @5 erroes Loss 2.2779\n",
      "  Epoch 37 Batch 1660 @5 erroes Loss 2.2431\n",
      "  Epoch 37 Batch 1670 @5 erroes Loss 2.3659\n",
      "  Epoch 37 Batch 1680 @5 erroes Loss 2.2603\n",
      "  Epoch 37 Batch 1690 @5 erroes Loss 2.2126\n",
      ".. n mi.lionwface amount liquid yielnkop ==>  \n",
      ".. t teneenn t ne tnenn  ainunn a n e an  Epoch 37 Batch 1700 @5 erroes Loss 2.3674\n",
      "  Epoch 37 Batch 1710 @5 erroes Loss 2.2475\n",
      "  Epoch 37 Batch 1720 @5 erroes Loss 2.2735\n",
      "  Epoch 37 Batch 1730 @5 erroes Loss 2.2765\n",
      "  Epoch 37 Batch 1740 @5 erroes Loss 2.2437\n",
      "a lasz week s outpu. fell ncn from the n ==>  \n",
      "a tel hah l t arl rn ao let t toen the t  Epoch 37 Batch 1750 @5 erroes Loss 2.3439\n",
      "  Epoch 37 Batch 1760 @5 erroes Loss 2.2434\n",
      "  Epoch 37 Batch 1770 @5 erroes Loss 2.3591\n",
      "  Epoch 37 Batch 1780 @5 erroes Loss 2.3152\n",
      "  Epoch 37 Batch 1790 @5 erroes Loss 2.2199\n",
      "g impeac.ment receivedxan unahtfcipared  ==>  \n",
      "g tneo neeo   oo o ne  on tr n  nonrn     Epoch 37 Batch 1800 @5 erroes Loss 2.2373\n",
      "  Epoch 37 Batch 1810 @5 erroes Loss 2.2333\n",
      "  Epoch 37 Batch 1820 @5 erroes Loss 2.2226\n",
      "  Epoch 37 Batch 1830 @5 erroes Loss 2.2171\n",
      "  Epoch 37 Batch 1840 @5 erroes Loss 2.2429\n",
      "llion or n a ohare marzed in the n secon ==>  \n",
      "llinn tr t tnt en  t n    tn the t t  tn  Epoch 37 Batch 1850 @5 erroes Loss 2.2225\n",
      "  Epoch 37 Batch 1860 @5 erroes Loss 2.3429\n",
      "  Epoch 37 Batch 1870 @5 erroes Loss 2.2085\n",
      "  Epoch 37 Batch 1880 @5 erroes Loss 2.2225\n",
      "  Epoch 37 Batch 1890 @5 erroes Loss 2.2302\n",
      "eginning with the famaey wf n udlike oth ==>  \n",
      "eg n  n  iin eahe ion ni if i in in  if   Epoch 37 Batch 1900 @5 erroes Loss 2.2207\n",
      "  Epoch 37 Batch 1910 @5 erroes Loss 2.2143\n",
      "  Epoch 37 Batch 1920 @5 erroes Loss 2.4708\n",
      "  Epoch 37 Batch 1930 @5 erroes Loss 2.3011\n",
      "  Epoch 37 Batch 1940 @5 erroes Loss 2.1987\n",
      "ance qo. okzameriza than operates in eve ==>  \n",
      "ando cor tr are enercher arr en   an t e  Epoch 37 Batch 1950 @5 erroes Loss 2.2095\n",
      "  Epoch 37 Batch 1960 @5 erroes Loss 2.2173\n",
      "  Epoch 37 Batch 1970 @5 erroes Loss 2.2336\n",
      "  Epoch 37 Batch 1980 @5 erroes Loss 2.2463\n",
      "  Epoch 37 Batch 1990 @5 erroes Loss 2.2109\n",
      "n ashlapdzhad net rf n million or n a .h ==>  \n",
      "n an eind aen a   ar a aan  nndtn t ant   Epoch 37 Batch 2000 @5 erroes Loss 2.2032\n",
      "  Epoch 37 Batch 2010 @5 erroes Loss 2.2265\n",
      "  Epoch 37 Batch 2020 @5 erroes Loss 2.2270\n",
      "  Epoch 37 Batch 2030 @5 erroes Loss 2.2097\n",
      "  Epoch 37 Batch 2040 @5 erroes Loss 2.2194\n",
      "hurt by declvning salss avaufalling chem ==>  \n",
      "hur  to t  ein d  t r   tr  tor in  toe   Epoch 37 Batch 2050 @5 erroes Loss 2.2221\n",
      "  Epoch 37 Batch 2060 @5 erroes Loss 2.3283\n",
      "  Epoch 37 Batch 2070 @5 erroes Loss 2.3818\n",
      "  Epoch 37 Batch 2080 @5 erroes Loss 2.2216\n",
      "  Epoch 37 Batch 2090 @5 erroes Loss 2.3254\n",
      "emplogersfgrant an averagewsward pf n pe ==>  \n",
      "empoen    ao n  an ane  n  pn n  ar a po  Epoch 37 Batch 2100 @5 erroes Loss 2.2085\n",
      "  Epoch 37 Batch 2110 @5 erroes Loss 2.2127\n",
      "  Epoch 37 Batch 2120 @5 erroes Loss 2.3215\n",
      "  Epoch 37 Batch 2130 @5 erroes Loss 2.2581\n",
      "  Epoch 37 Batch 2140 @5 erroes Loss 2.1850\n",
      "ciny figures ro eveotually become a sigp ==>  \n",
      "cin  ton n   ahnn e   nn e to one tnn n   Epoch 37 Batch 2150 @5 erroes Loss 2.3607\n",
      "  Epoch 37 Batch 2160 @5 erroes Loss 2.1893\n",
      "  Epoch 37 Batch 2170 @5 erroes Loss 2.1792\n",
      "  Epoch 37 Batch 2180 @5 erroes Loss 2.2230\n",
      "  Epoch 37 Batch 2190 @5 erroes Loss 2.1659\n",
      "id abmu. n n of fhe righns to purchasr i ==>  \n",
      "id anenn a t tr ahe ten e  ahrton een  t  Epoch 37 Batch 2200 @5 erroes Loss 2.2024\n",
      "  Epoch 37 Batch 2210 @5 erroes Loss 2.2174\n",
      "  Epoch 37 Batch 2220 @5 erroes Loss 2.1788\n",
      "  Epoch 37 Batch 2230 @5 erroes Loss 2.2081\n",
      "  Epoch 37 Batch 2240 @5 erroes Loss 2.1708\n",
      "noriega fook over ay generaltind dictato ==>  \n",
      "noren  rthrr afe  an ao    r an  a nohr   Epoch 37 Batch 2250 @5 erroes Loss 2.1864\n",
      "  Epoch 37 Batch 2260 @5 erroes Loss 2.1940\n",
      "  Epoch 37 Batch 2270 @5 erroes Loss 2.2042\n",
      "  Epoch 37 Batch 2280 @5 erroes Loss 2.1610\n",
      "  Epoch 37 Batch 2290 @5 erroes Loss 2.3072\n",
      "r eeallq applied thr spvt that showdd up ==>  \n",
      "r to re  tneren  the t rr ther t er   tn  Epoch 37 Batch 2300 @5 erroes Loss 2.2160\n",
      "  Epoch 37 Batch 2310 @5 erroes Loss 2.1879\n",
      "  Epoch 37 Batch 2320 @5 erroes Loss 2.1901\n",
      "  Epoch 37 Batch 2330 @5 erroes Loss 2.1791\n",
      "  Epoch 37 Batch 2340 @5 erroes Loss 2.1962\n",
      "thellargedt operatmr of ipch systems wit ==>  \n",
      "the tite    tfe  t retf t neet     e tan  Epoch 37 Batch 2350 @5 erroes Loss 2.1849\n",
      "  Epoch 37 Batch 2360 @5 erroes Loss 2.3152\n",
      "  Epoch 37 Batch 2370 @5 erroes Loss 2.1997\n",
      "  Epoch 37 Batch 2380 @5 erroes Loss 2.1979\n",
      "  Epoch 37 Batch 2390 @5 erroes Loss 2.2108\n",
      "jmawhere the son things wene wrojg iith  ==>  \n",
      "jme ie   ahe aar ahend  aa   aaend aan e  Epoch 37 Batch 2400 @5 erroes Loss 2.2526\n",
      "  Epoch 37 Batch 2410 @5 erroes Loss 2.1734\n",
      "  Epoch 37 Batch 2420 @5 erroes Loss 2.1622\n",
      "  Epoch 37 Batch 2430 @5 erroes Loss 2.3047\n",
      "  Epoch 37 Batch 2440 @5 erroes Loss 2.1567\n",
      "en greater threai xn n wv n u.s. intelzi ==>  \n",
      "en to  n   the  n tn t the  tn   tn       Epoch 37 Batch 2450 @5 erroes Loss 2.2019\n",
      "Epoch 37 Loss 1.9920\n",
      "\n",
      "Time taken for 1 epoch 2476.3606684207916 sec\n",
      "\n",
      "ythe on airlifk acquwsitions that wokld  ==>  \n",
      "ythe tfttnn  n  tnhutn nhnn  theththnn    Epoch 38 Batch 0 @5 erroes Loss 2.2914\n",
      "  Epoch 38 Batch 10 @5 erroes Loss 2.1279\n",
      "  Epoch 38 Batch 20 @5 erroes Loss 2.1418\n",
      "  Epoch 38 Batch 30 @5 erroes Loss 2.1418\n",
      "  Epoch 38 Batch 40 @5 erroes Loss 2.2705\n",
      "ef executiye officer saadlcustbmerskwere ==>  \n",
      "ef t   et ne tf  ne  t nn tot   e   th    Epoch 38 Batch 50 @5 erroes Loss 2.1743\n",
      "  Epoch 38 Batch 60 @5 erroes Loss 2.1632\n",
      "  Epoch 38 Batch 70 @5 erroes Loss 2.2733\n",
      "  Epoch 38 Batch 80 @5 erroes Loss 2.1679\n",
      "  Epoch 38 Batch 90 @5 erroes Loss 2.1320\n",
      "bee. launched or registwred withiregnlat ==>  \n",
      "be   tetn ee  trete  n   e  taneete  tet  Epoch 38 Batch 100 @5 erroes Loss 2.1746\n",
      "  Epoch 38 Batch 110 @5 erroes Loss 2.1643\n",
      "  Epoch 38 Batch 120 @5 erroes Loss 2.1320\n",
      "  Epoch 38 Batch 130 @5 erroes Loss 2.1838\n",
      "  Epoch 38 Batch 140 @5 erroes Loss 2.1974\n",
      "nalcand emzroyee againrt employee atskid ==>  \n",
      "naleand a peen   tnlln   a peen   tn aen  Epoch 38 Batch 150 @5 erroes Loss 2.1797\n",
      "  Epoch 38 Batch 160 @5 erroes Loss 2.1350\n",
      "  Epoch 38 Batch 170 @5 erroes Loss 2.1304\n",
      "  Epoch 38 Batch 180 @5 erroes Loss 2.1754\n",
      "  Epoch 38 Batch 190 @5 erroes Loss 2.1568\n",
      "on or more zust notifynftc adl justice v ==>  \n",
      "on tfeta e tatt t nhn  to otnt tott nh t  Epoch 38 Batch 200 @5 erroes Loss 2.2033\n",
      "  Epoch 38 Batch 210 @5 erroes Loss 2.1936\n",
      "  Epoch 38 Batch 220 @5 erroes Loss 2.1808\n",
      "  Epoch 38 Batch 230 @5 erroes Loss 2.2340\n",
      "  Epoch 38 Batch 240 @5 erroes Loss 2.1404\n",
      "haxe luxury ibems at riducyd prices but  ==>  \n",
      "har  aet r  an  e an ae  re  aoene  aet   Epoch 38 Batch 250 @5 erroes Loss 2.2234\n",
      "  Epoch 38 Batch 260 @5 erroes Loss 2.1417\n",
      "  Epoch 38 Batch 270 @5 erroes Loss 2.1442\n",
      "  Epoch 38 Batch 280 @5 erroes Loss 2.2439\n",
      "  Epoch 38 Batch 290 @5 erroes Loss 2.1728\n",
      "m aboutiincreased japanesepinvwstment in ==>  \n",
      "m anlnn an oe n   aunan    an e   e   an  Epoch 38 Batch 300 @5 erroes Loss 2.2331\n",
      "  Epoch 38 Batch 310 @5 erroes Loss 2.2076\n",
      "  Epoch 38 Batch 320 @5 erroes Loss 2.1458\n",
      "  Epoch 38 Batch 330 @5 erroes Loss 2.1002\n",
      "  Epoch 38 Batch 340 @5 erroes Loss 2.1221\n",
      "vay turn cellkamaligzant only afterjthe  ==>  \n",
      "van tht  to li tanlnl n  tfdy tn h  the   Epoch 38 Batch 350 @5 erroes Loss 2.2682\n",
      "  Epoch 38 Batch 360 @5 erroes Loss 2.0998\n",
      "  Epoch 38 Batch 370 @5 erroes Loss 2.1486\n",
      "  Epoch 38 Batch 380 @5 erroes Loss 2.1056\n",
      "  Epoch 38 Batch 390 @5 erroes Loss 2.0742\n",
      "tc say juebmcor s purchase is pawt ofla  ==>  \n",
      "tc ton tun e e  toaot eet  tn tor  tf tn  Epoch 38 Batch 400 @5 erroes Loss 2.2128\n",
      "  Epoch 38 Batch 410 @5 erroes Loss 2.1551\n",
      "  Epoch 38 Batch 420 @5 erroes Loss 2.1152\n",
      "  Epoch 38 Batch 430 @5 erroes Loss 2.1641\n",
      "  Epoch 38 Batch 440 @5 erroes Loss 2.1150\n",
      "r cohporabe parent yhu ll be helping a u ==>  \n",
      "r cor e  r  crre   t  nteite ce dend tnp  Epoch 38 Batch 450 @5 erroes Loss 2.1602\n",
      "  Epoch 38 Batch 460 @5 erroes Loss 2.1234\n",
      "  Epoch 38 Batch 470 @5 erroes Loss 2.1452\n",
      "  Epoch 38 Batch 480 @5 erroes Loss 2.0766\n",
      "  Epoch 38 Batch 490 @5 erroes Loss 2.2273\n",
      "r stepsjsuch .s nelswn the most rrominev ==>  \n",
      "r seh   aonoean a    r she sar  aoen n    Epoch 38 Batch 500 @5 erroes Loss 2.0859\n",
      "  Epoch 38 Batch 510 @5 erroes Loss 2.1200\n",
      "  Epoch 38 Batch 520 @5 erroes Loss 2.1195\n",
      "  Epoch 38 Batch 530 @5 erroes Loss 2.1098\n",
      "  Epoch 38 Batch 540 @5 erroes Loss 2.1234\n",
      "l to prospecbkve buyers tvat comparei wk ==>  \n",
      "l thrtren a t ne ter    thetitorerte  th  Epoch 38 Batch 550 @5 erroes Loss 2.0858\n",
      "  Epoch 38 Batch 560 @5 erroes Loss 2.1201\n",
      "  Epoch 38 Batch 570 @5 erroes Loss 2.1409\n",
      "  Epoch 38 Batch 580 @5 erroes Loss 2.1284\n",
      "  Epoch 38 Batch 590 @5 erroes Loss 2.1069\n",
      "hould n t the nftion hith bemands codmun ==>  \n",
      "ho n  t ththe n n nn thn eto  t   toreet  Epoch 38 Batch 600 @5 erroes Loss 2.0719\n",
      "  Epoch 38 Batch 610 @5 erroes Loss 2.0979\n",
      "  Epoch 38 Batch 620 @5 erroes Loss 2.1200\n",
      "  Epoch 38 Batch 630 @5 erroes Loss 2.0733\n",
      "  Epoch 38 Batch 640 @5 erroes Loss 2.0381\n",
      "id frilaybthat it hrd loyses of n milrio ==>  \n",
      "id toen n thel tn ter ti     tf t tan  n  Epoch 38 Batch 650 @5 erroes Loss 2.0211\n",
      "  Epoch 38 Batch 660 @5 erroes Loss 2.1115\n",
      "  Epoch 38 Batch 670 @5 erroes Loss 2.0087\n",
      "  Epoch 38 Batch 680 @5 erroes Loss 2.1716\n",
      "  Epoch 38 Batch 690 @5 erroes Loss 2.1200\n",
      "illson .n revesue of n billion ac.orping ==>  \n",
      "ill nn af ae e  n tf a tenl nn anlor  n   Epoch 38 Batch 700 @5 erroes Loss 2.0934\n",
      "  Epoch 38 Batch 710 @5 erroes Loss 2.0072\n",
      "  Epoch 38 Batch 720 @5 erroes Loss 2.0595\n",
      "  Epoch 38 Batch 730 @5 erroes Loss 2.0521\n",
      "  Epoch 38 Batch 740 @5 erroes Loss 2.0809\n",
      " by the u.sh wbth n n britain sith n n.b ==>  \n",
      " bo the nn   thnhet n no nhrn honhet n n  Epoch 38 Batch 750 @5 erroes Loss 2.0218\n",
      "  Epoch 38 Batch 760 @5 erroes Loss 2.0078\n",
      "  Epoch 38 Batch 770 @5 erroes Loss 2.0066\n",
      "  Epoch 38 Batch 780 @5 erroes Loss 2.0874\n",
      "  Epoch 38 Batch 790 @5 erroes Loss 1.9825\n",
      "ry is tough enlugd without judpes tsrowi ==>  \n",
      "ry tn thrt  tndut  tht e t tot    the n   Epoch 38 Batch 800 @5 erroes Loss 1.9410\n",
      "  Epoch 38 Batch 810 @5 erroes Loss 1.9642\n",
      "  Epoch 38 Batch 820 @5 erroes Loss 1.9746\n",
      "  Epoch 38 Batch 830 @5 erroes Loss 1.7481\n",
      "  Epoch 38 Batch 840 @5 erroes Loss 1.8116\n",
      "falq n on. analyst said the first shiw p ==>  \n",
      "fal  a i   fnaly   s rd thi for t s ec p  Epoch 38 Batch 850 @5 erroes Loss 1.6490\n",
      "  Epoch 38 Batch 860 @5 erroes Loss 1.6253\n",
      "  Epoch 38 Batch 870 @5 erroes Loss 1.5105\n",
      "  Epoch 38 Batch 880 @5 erroes Loss 1.5347\n",
      "  Epoch 38 Batch 890 @5 erroes Loss 1.4562\n",
      "ationmhandled such ma.ters mr. bush .asx ==>  \n",
      "athns bek eis such mas srd mr. bush ma.t  Epoch 38 Batch 900 @5 erroes Loss 1.1997\n",
      "  Epoch 38 Batch 910 @5 erroes Loss 1.3774\n",
      "  Epoch 38 Batch 920 @5 erroes Loss 1.2274\n",
      "  Epoch 38 Batch 930 @5 erroes Loss 1.3658\n",
      "  Epoch 38 Batch 940 @5 erroes Loss 1.2802\n",
      " cents a thare a yeor ago uomqanies ltst ==>  \n",
      " ch a arnoeere a yerr ano aomeanies lose  Epoch 38 Batch 950 @5 erroes Loss 1.4134\n",
      "  Epoch 38 Batch 960 @5 erroes Loss 1.1918\n",
      "  Epoch 38 Batch 970 @5 erroes Loss 1.1196\n",
      "  Epoch 38 Batch 980 @5 erroes Loss 1.2485\n",
      "  Epoch 38 Batch 990 @5 erroes Loss 1.1444\n",
      "hezfect thatkhe had m great deal ov powe ==>  \n",
      "he toal thatkhe tad m great deal ov powe  Epoch 38 Batch 1000 @5 erroes Loss 1.1002\n",
      "  Epoch 38 Batch 1010 @5 erroes Loss 1.1086\n",
      "  Epoch 38 Batch 1020 @5 erroes Loss 1.1965\n",
      "  Epoch 38 Batch 1030 @5 erroes Loss 0.9954\n",
      "  Epoch 38 Batch 1040 @5 erroes Loss 1.2295\n",
      "yand nt jgst jumped at you leinabd a was ==>  \n",
      "yad  tt aos  jumped at you leinabd a was  Epoch 38 Batch 1050 @5 erroes Loss 1.0262\n",
      "  Epoch 38 Batch 1060 @5 erroes Loss 1.0546\n",
      "  Epoch 38 Batch 1070 @5 erroes Loss 0.8683\n",
      "  Epoch 38 Batch 1080 @5 erroes Loss 0.8810\n",
      "  Epoch 38 Batch 1090 @5 erroes Loss 0.7197\n",
      "buy rmericanmones said hegrygking thn ma ==>  \n",
      "bun rmericangones said he tygting the sa  Epoch 38 Batch 1100 @5 erroes Loss 1.2079\n",
      "  Epoch 38 Batch 1110 @5 erroes Loss 1.1919\n",
      "  Epoch 38 Batch 1120 @5 erroes Loss 1.1832\n",
      "  Epoch 38 Batch 1130 @5 erroes Loss 1.1119\n",
      "  Epoch 38 Batch 1140 @5 erroes Loss 0.9515\n",
      "rsonal injlry settlzmect te.st in a move ==>  \n",
      "rs lsy indlry settlimect te.t htn a move  Epoch 38 Batch 1150 @5 erroes Loss 1.1544\n",
      "  Epoch 38 Batch 1160 @5 erroes Loss 1.0651\n",
      "  Epoch 38 Batch 1170 @5 erroes Loss 0.8792\n",
      "  Epoch 38 Batch 1180 @5 erroes Loss 0.7466\n",
      "  Epoch 38 Batch 1190 @5 erroes Loss 0.6622\n",
      " apulied to automobiles jzganesw domesti ==>  \n",
      " arolied to automoneles aoganesw domes    Epoch 38 Batch 1200 @5 erroes Loss 1.0199\n",
      "  Epoch 38 Batch 1210 @5 erroes Loss 0.8529\n",
      "  Epoch 38 Batch 1220 @5 erroes Loss 0.9240\n",
      "  Epoch 38 Batch 1230 @5 erroes Loss 0.7465\n",
      "  Epoch 38 Batch 1240 @5 erroes Loss 0.9381\n",
      "branch puq into co mezcial lending or ma ==>  \n",
      "brerch put in o co me ci c ie   n  or ma  Epoch 38 Batch 1250 @5 erroes Loss 0.8718\n",
      "  Epoch 38 Batch 1260 @5 erroes Loss 0.8025\n",
      "  Epoch 38 Batch 1270 @5 erroes Loss 0.9118\n",
      "  Epoch 38 Batch 1280 @5 erroes Loss 0.8421\n",
      "  Epoch 38 Batch 1290 @5 erroes Loss 0.9036\n",
      "inbindustri s cith world wids ovepcapaci ==>  \n",
      "in bndustri s ohth world wids orepcapaci  Epoch 38 Batch 1300 @5 erroes Loss 0.7410\n",
      "  Epoch 38 Batch 1310 @5 erroes Loss 2.3766\n",
      "  Epoch 38 Batch 1320 @5 erroes Loss 2.1914\n",
      "  Epoch 38 Batch 1330 @5 erroes Loss 2.0113\n",
      "  Epoch 38 Batch 1340 @5 erroes Loss 1.9213\n",
      "co. wascawarded en n million iayi contra ==>  \n",
      "con wan an r    an n minlinl n le aint a  Epoch 38 Batch 1350 @5 erroes Loss 1.7497\n",
      "  Epoch 38 Batch 1360 @5 erroes Loss 1.3690\n",
      "  Epoch 38 Batch 1370 @5 erroes Loss 1.1003\n",
      "  Epoch 38 Batch 1380 @5 erroes Loss 1.0260\n",
      "  Epoch 38 Batch 1390 @5 erroes Loss 0.9658\n",
      "urial isein the midst of heducing its ju ==>  \n",
      "uranl iseis hhe midst of heducing its ju  Epoch 38 Batch 1400 @5 erroes Loss 0.7105\n",
      "  Epoch 38 Batch 1410 @5 erroes Loss 0.9134\n",
      "  Epoch 38 Batch 1420 @5 erroes Loss 0.8719\n",
      "  Epoch 38 Batch 1430 @5 erroes Loss 0.8810\n",
      "  Epoch 38 Batch 1440 @5 erroes Loss 0.9322\n",
      "n h controlling their like jhey used tos ==>  \n",
      "n nhcont onllld lhe nenooednhe  lt   thn  Epoch 38 Batch 1450 @5 erroes Loss 1.6154\n",
      "  Epoch 38 Batch 1460 @5 erroes Loss 1.3698\n",
      "  Epoch 38 Batch 1470 @5 erroes Loss 1.1080\n",
      "  Epoch 38 Batch 1480 @5 erroes Loss 0.8223\n",
      "  Epoch 38 Batch 1490 @5 erroes Loss 0.9358\n",
      "earose n n .o n billi n  rom n billion i ==>  \n",
      "eaoese n n so m sr ei nior niartolli nio  Epoch 38 Batch 1500 @5 erroes Loss 0.7854\n",
      "  Epoch 38 Batch 1510 @5 erroes Loss 0.8672\n",
      "  Epoch 38 Batch 1520 @5 erroes Loss 0.6736\n",
      "  Epoch 38 Batch 1530 @5 erroes Loss 0.9597\n",
      "  Epoch 38 Batch 1540 @5 erroes Loss 0.9143\n",
      "prepare. by chers from one ofdthe cith s ==>  \n",
      "pra are  by chers from one of the cith s  Epoch 38 Batch 1550 @5 erroes Loss 0.8396\n",
      "  Epoch 38 Batch 1560 @5 erroes Loss 0.8271\n",
      "  Epoch 38 Batch 1570 @5 erroes Loss 0.6794\n",
      "  Epoch 38 Batch 1580 @5 erroes Loss 0.6884\n",
      "  Epoch 38 Batch 1590 @5 erroes Loss 0.7015\n",
      "  imptct on ohe market inxustrius a brit ==>  \n",
      "  mnptct on the tarket industrids a brit  Epoch 38 Batch 1600 @5 erroes Loss 1.1216\n",
      "  Epoch 38 Batch 1610 @5 erroes Loss 0.8184\n",
      "  Epoch 38 Batch 1620 @5 erroes Loss 0.9092\n",
      "  Epoch 38 Batch 1630 @5 erroes Loss 0.7999\n",
      "  Epoch 38 Batch 1640 @5 erroes Loss 0.8141\n",
      "stockldivvdendsiii owns about nen of she ==>  \n",
      "storkldivedendsiin owns about nen s  s e  Epoch 38 Batch 1650 @5 erroes Loss 0.7633\n",
      "  Epoch 38 Batch 1660 @5 erroes Loss 0.6284\n",
      "  Epoch 38 Batch 1670 @5 erroes Loss 0.7657\n",
      "  Epoch 38 Batch 1680 @5 erroes Loss 0.7664\n",
      "  Epoch 38 Batch 1690 @5 erroes Loss 0.6915\n",
      "rjia pennsylvanqa connecticut ajz missou ==>  \n",
      "reatnna     aen nnnon   ohnonhtn  tan  n  Epoch 38 Batch 1700 @5 erroes Loss 2.6436\n",
      "  Epoch 38 Batch 1710 @5 erroes Loss 2.5351\n",
      "  Epoch 38 Batch 1720 @5 erroes Loss 2.3162\n",
      "  Epoch 38 Batch 1730 @5 erroes Loss 2.3036\n",
      "  Epoch 38 Batch 1740 @5 erroes Loss 2.2512\n",
      "nd that means nornew bssineks xor us he  ==>  \n",
      "nd ihen ie n  i nt   iin n    ior tr ie   Epoch 38 Batch 1750 @5 erroes Loss 2.1745\n",
      "  Epoch 38 Batch 1760 @5 erroes Loss 2.2060\n",
      "  Epoch 38 Batch 1770 @5 erroes Loss 2.1713\n",
      "  Epoch 38 Batch 1780 @5 erroes Loss 2.2787\n",
      "  Epoch 38 Batch 1790 @5 erroes Loss 2.2049\n",
      "j.c. penney co. whiqh fmom new gark tw s ==>  \n",
      "j. e sr     sor seeneetoe  t   s n  shft  Epoch 38 Batch 1800 @5 erroes Loss 2.1292\n",
      "  Epoch 38 Batch 1810 @5 erroes Loss 2.1598\n",
      "  Epoch 38 Batch 1820 @5 erroes Loss 2.0767\n",
      "  Epoch 38 Batch 1830 @5 erroes Loss 2.0266\n",
      "  Epoch 38 Batch 1840 @5 erroes Loss 2.0926\n",
      "ubig bobrd were sold yestewdaw for n and ==>  \n",
      "uben fonn  ter  t r  ted   t n for t and  Epoch 38 Batch 1850 @5 erroes Loss 2.0752\n",
      "  Epoch 38 Batch 1860 @5 erroes Loss 1.9850\n",
      "  Epoch 38 Batch 1870 @5 erroes Loss 1.9508\n",
      "  Epoch 38 Batch 1880 @5 erroes Loss 2.0757\n",
      "  Epoch 38 Batch 1890 @5 erroes Loss 1.9256\n",
      "shippers crt abmut n billionhfaom vheir  ==>  \n",
      "sh nla e aht anlnl a billinn aoen ahe n   Epoch 38 Batch 1900 @5 erroes Loss 1.9157\n",
      "  Epoch 38 Batch 1910 @5 erroes Loss 1.9484\n",
      "  Epoch 38 Batch 1920 @5 erroes Loss 1.9256\n",
      "  Epoch 38 Batch 1930 @5 erroes Loss 1.7600\n",
      "  Epoch 38 Batch 1940 @5 erroes Loss 1.8039\n",
      " ko pu.cxase n n of thc oompany s shares ==>  \n",
      " k  son e n  n n nf the non one s s e es  Epoch 38 Batch 1950 @5 erroes Loss 1.6894\n",
      "  Epoch 38 Batch 1960 @5 erroes Loss 1.8535\n",
      "  Epoch 38 Batch 1970 @5 erroes Loss 1.5842\n",
      "  Epoch 38 Batch 1980 @5 erroes Loss 1.7685\n",
      "  Epoch 38 Batch 1990 @5 erroes Loss 1.6041\n",
      "rteo the bilo becsuse they thoughz themt ==>  \n",
      "rt   whe tilo tecsut  the  thout   the t  Epoch 38 Batch 2000 @5 erroes Loss 1.5582\n",
      "  Epoch 38 Batch 2010 @5 erroes Loss 1.5516\n",
      "  Epoch 38 Batch 2020 @5 erroes Loss 1.4109\n",
      "  Epoch 38 Batch 2030 @5 erroes Loss 1.4957\n",
      "  Epoch 38 Batch 2040 @5 erroes Loss 1.2249\n",
      "cks xnd futules bft smunning volapility  ==>  \n",
      "cke tmg solhles bot t urg ng polatititi   Epoch 38 Batch 2050 @5 erroes Loss 1.4146\n",
      "  Epoch 38 Batch 2060 @5 erroes Loss 1.4304\n",
      "  Epoch 38 Batch 2070 @5 erroes Loss 1.4739\n",
      "  Epoch 38 Batch 2080 @5 erroes Loss 1.4773\n",
      "  Epoch 38 Batch 2090 @5 erroes Loss 1.4092\n",
      "s and ex hange commissioi to rtviewa.he  ==>  \n",
      "s s  eo  eenge commissiom to mosenseioe   Epoch 38 Batch 2100 @5 erroes Loss 1.3493\n",
      "  Epoch 38 Batch 2110 @5 erroes Loss 1.2620\n",
      "  Epoch 38 Batch 2120 @5 erroes Loss 1.1724\n",
      "  Epoch 38 Batch 2130 @5 erroes Loss 1.3020\n",
      "  Epoch 38 Batch 2140 @5 erroes Loss 1.3126\n",
      "wdrug ibufebruary on patients whn hcd re ==>  \n",
      "wdeen on pati ary on patients whe hediee  Epoch 38 Batch 2150 @5 erroes Loss 1.2687\n",
      "  Epoch 38 Batch 2160 @5 erroes Loss 1.2422\n",
      "  Epoch 38 Batch 2170 @5 erroes Loss 1.0785\n",
      "  Epoch 38 Batch 2180 @5 erroes Loss 1.1995\n",
      "  Epoch 38 Batch 2190 @5 erroes Loss 1.2366\n",
      "mpxed in new yxrk late yesterday lt  as  ==>  \n",
      "mpnel in new yerk late yesterd y nn nere  Epoch 38 Batch 2200 @5 erroes Loss 1.0837\n",
      "  Epoch 38 Batch 2210 @5 erroes Loss 1.1579\n",
      "  Epoch 38 Batch 2220 @5 erroes Loss 1.1826\n",
      "  Epoch 38 Batch 2230 @5 erroes Loss 1.1730\n",
      "  Epoch 38 Batch 2240 @5 erroes Loss 1.2774\n",
      "ty went hiammr. clean kaage was by an yc ==>  \n",
      "ty caag hia taa tlean sa l rwas by an ah  Epoch 38 Batch 2250 @5 erroes Loss 1.2846\n",
      "  Epoch 38 Batch 2260 @5 erroes Loss 1.1124\n",
      "  Epoch 38 Batch 2270 @5 erroes Loss 1.0542\n",
      "  Epoch 38 Batch 2280 @5 erroes Loss 1.1379\n",
      "  Epoch 38 Batch 2290 @5 erroes Loss 1.2236\n",
      "tncludes apprlximatzly n accouoqs with a ==>  \n",
      "tn eud s apprln matsld m a aoup s with a  Epoch 38 Batch 2300 @5 erroes Loss 1.1850\n",
      "  Epoch 38 Batch 2310 @5 erroes Loss 1.2411\n",
      "  Epoch 38 Batch 2320 @5 erroes Loss 1.0317\n",
      "  Epoch 38 Batch 2330 @5 erroes Loss 1.0973\n",
      "  Epoch 38 Batch 2340 @5 erroes Loss 1.0739\n",
      " company s corporate accouht.nt cystem i ==>  \n",
      " ch aant s corporate accouatsna s  ten i  Epoch 38 Batch 2350 @5 erroes Loss 0.9510\n",
      "  Epoch 38 Batch 2360 @5 erroes Loss 1.1238\n",
      "  Epoch 38 Batch 2370 @5 erroes Loss 0.8750\n",
      "  Epoch 38 Batch 2380 @5 erroes Loss 1.1044\n",
      "  Epoch 38 Batch 2390 @5 erroes Loss 1.1538\n",
      " pany sovijtpleaaer mikhvil gorbachev op ==>  \n",
      " p    sovidtheol  resoveeinacor achev op  Epoch 38 Batch 2400 @5 erroes Loss 1.1505\n",
      "  Epoch 38 Batch 2410 @5 erroes Loss 1.1327\n",
      "  Epoch 38 Batch 2420 @5 erroes Loss 1.7261\n",
      "  Epoch 38 Batch 2430 @5 erroes Loss 1.4707\n",
      "  Epoch 38 Batch 2440 @5 erroes Loss 1.4589\n",
      "d at infreaszd its stakt indwestern fore ==>  \n",
      "d so itduest d tt  itake sn western fore  Epoch 38 Batch 2450 @5 erroes Loss 1.3143\n",
      "Epoch 38 Loss 1.5878\n",
      "\n",
      "Time taken for 1 epoch 2390.1675379276276 sec\n",
      "\n",
      " education improvemtnt act thv bonus uep ==>  \n",
      " e  aot on improvemtn  t tothe ton s aev  Epoch 39 Batch 0 @5 erroes Loss 1.1882\n",
      "  Epoch 39 Batch 10 @5 erroes Loss 1.3259\n",
      "  Epoch 39 Batch 20 @5 erroes Loss 0.9795\n",
      "  Epoch 39 Batch 30 @5 erroes Loss 1.2003\n",
      "  Epoch 39 Batch 40 @5 erroes Loss 1.1030\n",
      "aib keiuk collins a depargmenj analyst f ==>  \n",
      "ain aeiliefollins anaepargmend analyst f  Epoch 39 Batch 50 @5 erroes Loss 1.1123\n",
      "  Epoch 39 Batch 60 @5 erroes Loss 1.1071\n",
      "  Epoch 39 Batch 70 @5 erroes Loss 0.9210\n",
      "  Epoch 39 Batch 80 @5 erroes Loss 0.9837\n",
      "  Epoch 39 Batch 90 @5 erroes Loss 1.1004\n",
      "nbyqcarry any finagce rates mr. says ame ==>  \n",
      "n aaaons  any finagce rates mr. says ame  Epoch 39 Batch 100 @5 erroes Loss 0.7988\n",
      "  Epoch 39 Batch 110 @5 erroes Loss 1.1550\n",
      "  Epoch 39 Batch 120 @5 erroes Loss 0.9909\n",
      "  Epoch 39 Batch 130 @5 erroes Loss 0.7292\n",
      "  Epoch 39 Batch 140 @5 erroes Loss 0.8371\n",
      "gain of n mhllionrin thh year ago quwrte ==>  \n",
      "gann if n mallion in nhe yuwr ago suwr e  Epoch 39 Batch 150 @5 erroes Loss 0.9808\n",
      "  Epoch 39 Batch 160 @5 erroes Loss 1.1486\n",
      "  Epoch 39 Batch 170 @5 erroes Loss 1.1119\n",
      "  Epoch 39 Batch 180 @5 erroes Loss 1.0923\n",
      "  Epoch 39 Batch 190 @5 erroes Loss 0.9587\n",
      " p.ant rill produce uontrol devices usec ==>  \n",
      " plldt rill produce uontrol devices usec  Epoch 39 Batch 200 @5 erroes Loss 0.9008\n",
      "  Epoch 39 Batch 210 @5 erroes Loss 1.0124\n",
      "  Epoch 39 Batch 220 @5 erroes Loss 1.0230\n",
      "  Epoch 39 Batch 230 @5 erroes Loss 1.0931\n",
      "  Epoch 39 Batch 240 @5 erroes Loss 1.0311\n",
      "fodm n and other reports thau paxpcyers  ==>  \n",
      "for an and other reports thau pappryers   Epoch 39 Batch 250 @5 erroes Loss 0.8236\n",
      "  Epoch 39 Batch 260 @5 erroes Loss 1.0343\n",
      "  Epoch 39 Batch 270 @5 erroes Loss 1.0437\n",
      "  Epoch 39 Batch 280 @5 erroes Loss 0.9564\n",
      "  Epoch 39 Batch 290 @5 erroes Loss 1.0689\n",
      "fto aaound d marks as u.s. economig dati ==>  \n",
      "ftono ound d marks as u.s. economig dati  Epoch 39 Batch 300 @5 erroes Loss 0.9381\n",
      "  Epoch 39 Batch 310 @5 erroes Loss 0.9893\n",
      "  Epoch 39 Batch 320 @5 erroes Loss 1.0794\n",
      "  Epoch 39 Batch 330 @5 erroes Loss 0.8988\n",
      "  Epoch 39 Batch 340 @5 erroes Loss 0.7792\n",
      "s.ock pricss and the dollauemean.hile bo ==>  \n",
      "s.h h pricss and the dollaueme nghile bo  Epoch 39 Batch 350 @5 erroes Loss 1.0115\n",
      "  Epoch 39 Batch 360 @5 erroes Loss 0.8399\n",
      "  Epoch 39 Batch 370 @5 erroes Loss 0.8033\n",
      "  Epoch 39 Batch 380 @5 erroes Loss 0.9450\n",
      "  Epoch 39 Batch 390 @5 erroes Loss 0.8989\n",
      "the .aby fvced a test usingdnew eynetic  ==>  \n",
      "the taby frcy  a test usingdneiet  y  nt  Epoch 39 Batch 400 @5 erroes Loss 0.9286\n",
      "  Epoch 39 Batch 410 @5 erroes Loss 0.8517\n",
      "  Epoch 39 Batch 420 @5 erroes Loss 0.9565\n",
      "  Epoch 39 Batch 430 @5 erroes Loss 0.9508\n",
      "  Epoch 39 Batch 440 @5 erroes Loss 0.8973\n",
      "ncu positioc is that mersonw may not tak ==>  \n",
      "nct masitioc is that mersont may notitok  Epoch 39 Batch 450 @5 erroes Loss 0.7643\n",
      "  Epoch 39 Batch 460 @5 erroes Loss 1.0598\n",
      "  Epoch 39 Batch 470 @5 erroes Loss 1.0276\n",
      "  Epoch 39 Batch 480 @5 erroes Loss 1.0324\n",
      "  Epoch 39 Batch 490 @5 erroes Loss 0.9398\n",
      "delay its shapthr n qroceedings the comp ==>  \n",
      "dedin its shapthr n qroceedings the comp  Epoch 39 Batch 500 @5 erroes Loss 0.7908\n",
      "  Epoch 39 Batch 510 @5 erroes Loss 0.8667\n",
      "  Epoch 39 Batch 520 @5 erroes Loss 0.9564\n",
      "  Epoch 39 Batch 530 @5 erroes Loss 0.7187\n",
      "  Epoch 39 Batch 540 @5 erroes Loss 0.9199\n",
      "itn iy lasteyear s reporw the tbeasury s ==>  \n",
      "itr rn lastey rt shreporw the teeasury s  Epoch 39 Batch 550 @5 erroes Loss 0.9221\n",
      "  Epoch 39 Batch 560 @5 erroes Loss 0.8547\n",
      "  Epoch 39 Batch 570 @5 erroes Loss 0.7965\n",
      "  Epoch 39 Batch 580 @5 erroes Loss 0.8728\n",
      "  Epoch 39 Batch 590 @5 erroes Loss 2.0613\n",
      "incurred inccofnectionwwith thearvcapita ==>  \n",
      "inccceec in car c titnctit aahaacirtiac   Epoch 39 Batch 600 @5 erroes Loss 1.9047\n",
      "  Epoch 39 Batch 610 @5 erroes Loss 1.4250\n",
      "  Epoch 39 Batch 620 @5 erroes Loss 1.2502\n",
      "  Epoch 39 Batch 630 @5 erroes Loss 1.1907\n",
      "  Epoch 39 Batch 640 @5 erroes Loss 1.2174\n",
      "d but while mnaly tsvsaf that junicipal  ==>  \n",
      "d taa while mnaly t  sat t es junicipal   Epoch 39 Batch 650 @5 erroes Loss 1.1787\n",
      "  Epoch 39 Batch 660 @5 erroes Loss 1.0267\n",
      "  Epoch 39 Batch 670 @5 erroes Loss 1.0218\n",
      "  Epoch 39 Batch 680 @5 erroes Loss 0.9850\n",
      "  Epoch 39 Batch 690 @5 erroes Loss 0.9390\n",
      "kof the current poryion of lonf term deb ==>  \n",
      "kor ihe current poryion of lon  tebm deb  Epoch 39 Batch 700 @5 erroes Loss 1.1049\n",
      "  Epoch 39 Batch 710 @5 erroes Loss 0.8469\n",
      "  Epoch 39 Batch 720 @5 erroes Loss 0.9585\n",
      "  Epoch 39 Batch 730 @5 erroes Loss 0.9034\n",
      "  Epoch 39 Batch 740 @5 erroes Loss 0.8762\n",
      "er tp prevest thcm from pzying their lig ==>  \n",
      "erith prevest themes im try ng the r liy  Epoch 39 Batch 750 @5 erroes Loss 0.9260\n",
      "  Epoch 39 Batch 760 @5 erroes Loss 0.8142\n",
      "  Epoch 39 Batch 770 @5 erroes Loss 0.9490\n",
      "  Epoch 39 Batch 780 @5 erroes Loss 0.9968\n",
      "  Epoch 39 Batch 790 @5 erroes Loss 0.9151\n",
      "of itf spkes volume kelloggniszso anxiou ==>  \n",
      "of ss  spkls tolume kelloggnis so an iou  Epoch 39 Batch 800 @5 erroes Loss 0.9853\n",
      "  Epoch 39 Batch 810 @5 erroes Loss 0.8008\n",
      "  Epoch 39 Batch 820 @5 erroes Loss 0.8863\n",
      "  Epoch 39 Batch 830 @5 erroes Loss 0.9487\n",
      "  Epoch 39 Batch 840 @5 erroes Loss 0.8849\n",
      " it theyrare n payroll tu meet azs ory o ==>  \n",
      " it ahe rare n pay orl th meet a s ofy o  Epoch 39 Batch 850 @5 erroes Loss 0.9213\n",
      "  Epoch 39 Batch 860 @5 erroes Loss 0.8673\n",
      "  Epoch 39 Batch 870 @5 erroes Loss 0.9434\n",
      "  Epoch 39 Batch 880 @5 erroes Loss 0.9247\n",
      "  Epoch 39 Batch 890 @5 erroes Loss 0.9414\n",
      "ncitg andcthat investors wollwneud a sol ==>  \n",
      "nchl nandithat investors wollun ad invos  Epoch 39 Batch 900 @5 erroes Loss 0.9314\n",
      "  Epoch 39 Batch 910 @5 erroes Loss 0.9115\n",
      "  Epoch 39 Batch 920 @5 erroes Loss 0.8386\n",
      "  Epoch 39 Batch 930 @5 erroes Loss 0.7659\n",
      "  Epoch 39 Batch 940 @5 erroes Loss 0.6206\n",
      "p.tn mgllion redymption amountyof zerojc ==>  \n",
      "p. o mellion red mption amouno om ton no  Epoch 39 Batch 950 @5 erroes Loss 0.9358\n",
      "  Epoch 39 Batch 960 @5 erroes Loss 0.9776\n",
      "  Epoch 39 Batch 970 @5 erroes Loss 1.0080\n",
      "  Epoch 39 Batch 980 @5 erroes Loss 1.0090\n",
      "  Epoch 39 Batch 990 @5 erroes Loss 2.1016\n",
      "e a ne.wyykdof or hedge walls with reol  ==>  \n",
      "e tnt   en  on on oe    til   nin ete t   Epoch 39 Batch 1000 @5 erroes Loss 2.0675\n",
      "  Epoch 39 Batch 1010 @5 erroes Loss 1.9002\n",
      "  Epoch 39 Batch 1020 @5 erroes Loss 1.7116\n",
      "  Epoch 39 Batch 1030 @5 erroes Loss 1.5012\n",
      "  Epoch 39 Batch 1040 @5 erroes Loss 1.4010\n",
      "ent toqavoid direct combat nzth chiaper  ==>  \n",
      "eneacombnoid direct comaat ditaecoia  a   Epoch 39 Batch 1050 @5 erroes Loss 1.3506\n",
      "  Epoch 39 Batch 1060 @5 erroes Loss 1.2136\n",
      "  Epoch 39 Batch 1070 @5 erroes Loss 1.0540\n",
      "  Epoch 39 Batch 1080 @5 erroes Loss 1.0875\n",
      "  Epoch 39 Batch 1090 @5 erroes Loss 1.0748\n",
      "d earlieo t is kear saysfjohn portfolio  ==>  \n",
      "d a n  o  thisio rr says aohn port olion  Epoch 39 Batch 1100 @5 erroes Loss 0.9743\n",
      "  Epoch 39 Batch 1110 @5 erroes Loss 1.1261\n",
      "  Epoch 39 Batch 1120 @5 erroes Loss 0.9120\n",
      "  Epoch 39 Batch 1130 @5 erroes Loss 0.9550\n",
      "  Epoch 39 Batch 1140 @5 erroes Loss 0.9864\n",
      " grixfs ways and means veteraned. mjqes  ==>  \n",
      " gyes   ways and means vateraned  saye    Epoch 39 Batch 1150 @5 erroes Loss 0.8977\n",
      "  Epoch 39 Batch 1160 @5 erroes Loss 0.8739\n",
      "  Epoch 39 Batch 1170 @5 erroes Loss 0.9626\n",
      "  Epoch 39 Batch 1180 @5 erroes Loss 0.7609\n",
      "  Epoch 39 Batch 1190 @5 erroes Loss 0.8783\n",
      "e phople iho suffer in the short run ahe ==>  \n",
      "e ner ie therthrfer in the short run ane  Epoch 39 Batch 1200 @5 erroes Loss 0.7254\n",
      "  Epoch 39 Batch 1210 @5 erroes Loss 0.8851\n",
      "  Epoch 39 Batch 1220 @5 erroes Loss 0.8939\n",
      "  Epoch 39 Batch 1230 @5 erroes Loss 0.8253\n",
      "  Epoch 39 Batch 1240 @5 erroes Loss 0.7785\n",
      " nbt want po earn so much tdat sorialcse ==>  \n",
      " n ehchnt wo murn so much toat soritlcse  Epoch 39 Batch 1250 @5 erroes Loss 0.8396\n",
      "  Epoch 39 Batch 1260 @5 erroes Loss 0.8755\n",
      "  Epoch 39 Batch 1270 @5 erroes Loss 0.8417\n",
      "  Epoch 39 Batch 1280 @5 erroes Loss 0.8486\n",
      "  Epoch 39 Batch 1290 @5 erroes Loss 0.7484\n",
      "eiblion of cash out of the sji vv s buya ==>  \n",
      "einyoon of cash out of the seisvhit oo    Epoch 39 Batch 1300 @5 erroes Loss 0.7911\n",
      "  Epoch 39 Batch 1310 @5 erroes Loss 0.8788\n",
      "  Epoch 39 Batch 1320 @5 erroes Loss 0.8798\n",
      "  Epoch 39 Batch 1330 @5 erroes Loss 0.8644\n",
      "  Epoch 39 Batch 1340 @5 erroes Loss 0.8662\n",
      "eathe firm sdlice.setth sell securities  ==>  \n",
      "eatie iirm selice se th sell securities   Epoch 39 Batch 1350 @5 erroes Loss 0.8480\n",
      "  Epoch 39 Batch 1360 @5 erroes Loss 0.9374\n",
      "  Epoch 39 Batch 1370 @5 erroes Loss 0.6764\n",
      "  Epoch 39 Batch 1380 @5 erroes Loss 0.8006\n",
      "  Epoch 39 Batch 1390 @5 erroes Loss 0.7638\n",
      "ormunicatidns fgreed no pzy viacom inc.  ==>  \n",
      "or pticatidns fgreed no pry viacom inc.   Epoch 39 Batch 1400 @5 erroes Loss 0.9019\n",
      "  Epoch 39 Batch 1410 @5 erroes Loss 0.8316\n",
      "  Epoch 39 Batch 1420 @5 erroes Loss 0.7997\n",
      "  Epoch 39 Batch 1430 @5 erroes Loss 0.7253\n",
      "  Epoch 39 Batch 1440 @5 erroes Loss 0.8112\n",
      "p upwtheocompfny s stoik price throulh f ==>  \n",
      "p ctat eocompany s stoik tr sl throulh f  Epoch 39 Batch 1450 @5 erroes Loss 0.8839\n",
      "  Epoch 39 Batch 1460 @5 erroes Loss 0.8781\n",
      "  Epoch 39 Batch 1470 @5 erroes Loss 0.7977\n",
      "  Epoch 39 Batch 1480 @5 erroes Loss 0.9850\n",
      "  Epoch 39 Batch 1490 @5 erroes Loss 0.9548\n",
      "loor andushe bas wearingii knew iq waa s ==>  \n",
      "lous andushe bas wearingiinandwsin waa w  Epoch 39 Batch 1500 @5 erroes Loss 1.0185\n",
      "  Epoch 39 Batch 1510 @5 erroes Loss 0.8576\n",
      "  Epoch 39 Batch 1520 @5 erroes Loss 0.7876\n",
      "  Epoch 39 Batch 1530 @5 erroes Loss 0.8492\n",
      "  Epoch 39 Batch 1540 @5 erroes Loss 0.8165\n",
      "ion that inueaest raths wouldrcontiyue t ==>  \n",
      "iondthat inueaest raths wouldrcontinue t  Epoch 39 Batch 1550 @5 erroes Loss 0.8251\n",
      "  Epoch 39 Batch 1560 @5 erroes Loss 0.9140\n",
      "  Epoch 39 Batch 1570 @5 erroes Loss 1.0028\n",
      "  Epoch 39 Batch 1580 @5 erroes Loss 0.8179\n",
      "  Epoch 39 Batch 1590 @5 erroes Loss 0.8018\n",
      "we ker.tbey added that mvrtet makers wer ==>  \n",
      "wet erkt ey added that marter makers wer  Epoch 39 Batch 1600 @5 erroes Loss 0.8016\n",
      "  Epoch 39 Batch 1610 @5 erroes Loss 0.8079\n",
      "  Epoch 39 Batch 1620 @5 erroes Loss 0.8110\n",
      "  Epoch 39 Batch 1630 @5 erroes Loss 0.8556\n",
      "  Epoch 39 Batch 1640 @5 erroes Loss 0.7909\n",
      "wd othrrs said thlt the yrop continaed t ==>  \n",
      "wd tthrrs said thet the yrop continaed t  Epoch 39 Batch 1650 @5 erroes Loss 0.8134\n",
      "  Epoch 39 Batch 1660 @5 erroes Loss 0.7899\n",
      "  Epoch 39 Batch 1670 @5 erroes Loss 0.8256\n",
      "  Epoch 39 Batch 1680 @5 erroes Loss 0.7944\n",
      "  Epoch 39 Batch 1690 @5 erroes Loss 0.6927\n",
      "orp morgin guaranty trusj bank and natko ==>  \n",
      "orsinorgin guaranty trus  bank and nat o  Epoch 39 Batch 1700 @5 erroes Loss 0.7183\n",
      "  Epoch 39 Batch 1710 @5 erroes Loss 0.7750\n",
      "  Epoch 39 Batch 1720 @5 erroes Loss 0.7402\n",
      "  Epoch 39 Batch 1730 @5 erroes Loss 1.8459\n",
      "  Epoch 39 Batch 1740 @5 erroes Loss 1.5770\n",
      "e.rz say the red tame they post lovezto  ==>  \n",
      "est  o t poe fed tame they post oove th   Epoch 39 Batch 1750 @5 erroes Loss 1.2923\n",
      "  Epoch 39 Batch 1760 @5 erroes Loss 1.1746\n",
      "  Epoch 39 Batch 1770 @5 erroes Loss 1.0323\n",
      "  Epoch 39 Batch 1780 @5 erroes Loss 1.0813\n",
      "  Epoch 39 Batch 1790 @5 erroes Loss 0.9301\n",
      " itbwas fvom my own brain t ey lospnfor  ==>  \n",
      " isonos foop my own brain t e  lospnfor   Epoch 39 Batch 1800 @5 erroes Loss 0.9266\n",
      "  Epoch 39 Batch 1810 @5 erroes Loss 0.8379\n",
      "  Epoch 39 Batch 1820 @5 erroes Loss 0.9463\n",
      "  Epoch 39 Batch 1830 @5 erroes Loss 0.9516\n",
      "  Epoch 39 Batch 1840 @5 erroes Loss 0.8555\n",
      " fnatpree wall fouus on family health an ==>  \n",
      " fa t ree toee toues on family health an  Epoch 39 Batch 1850 @5 erroes Loss 0.8298\n",
      "  Epoch 39 Batch 1860 @5 erroes Loss 0.9310\n",
      "  Epoch 39 Batch 1870 @5 erroes Loss 0.8897\n",
      "  Epoch 39 Batch 1880 @5 erroes Loss 0.8975\n",
      "  Epoch 39 Batch 1890 @5 erroes Loss 0.6620\n",
      "n earniogs as nmericaz .ome prodqcts cor ==>  \n",
      "n mriniogs as nmerical some prod cts cor  Epoch 39 Batch 1900 @5 erroes Loss 0.9040\n",
      "  Epoch 39 Batch 1910 @5 erroes Loss 0.6725\n",
      "  Epoch 39 Batch 1920 @5 erroes Loss 0.7282\n",
      "  Epoch 39 Batch 1930 @5 erroes Loss 0.8067\n",
      "  Epoch 39 Batch 1940 @5 erroes Loss 0.6853\n",
      "so . and other cotperitors that tried to ==>  \n",
      "sot hand other cotperitors that tried to  Epoch 39 Batch 1950 @5 erroes Loss 0.5336\n",
      "  Epoch 39 Batch 1960 @5 erroes Loss 0.6839\n",
      "  Epoch 39 Batch 1970 @5 erroes Loss 2.1551\n",
      "  Epoch 39 Batch 1980 @5 erroes Loss 2.2339\n",
      "  Epoch 39 Batch 1990 @5 erroes Loss 2.1397\n",
      "st tiae il hascfailed tv rite since n mr ==>  \n",
      "st thne cniten trnn   ihnten  c n h c ta  Epoch 39 Batch 2000 @5 erroes Loss 2.0773\n",
      "  Epoch 39 Batch 2010 @5 erroes Loss 2.1148\n",
      "  Epoch 39 Batch 2020 @5 erroes Loss 2.1440\n",
      "  Epoch 39 Batch 2030 @5 erroes Loss 1.9494\n",
      "  Epoch 39 Batch 2040 @5 erroes Loss 1.9181\n",
      " for thoze left hoxeless che agriaulture ==>  \n",
      " f rether  aer  tere l   ohe tnreltreire  Epoch 39 Batch 2050 @5 erroes Loss 1.8353\n",
      "  Epoch 39 Batch 2060 @5 erroes Loss 1.7026\n",
      "  Epoch 39 Batch 2070 @5 erroes Loss 1.6744\n",
      "  Epoch 39 Batch 2080 @5 erroes Loss 1.6123\n",
      "  Epoch 39 Batch 2090 @5 erroes Loss 1.4419\n",
      " sales the real estoto inhestment trukp  ==>  \n",
      " st is the real estot  tnaest ent thek h  Epoch 39 Batch 2100 @5 erroes Loss 1.3216\n",
      "  Epoch 39 Batch 2110 @5 erroes Loss 1.2982\n",
      "  Epoch 39 Batch 2120 @5 erroes Loss 1.2202\n",
      "  Epoch 39 Batch 2130 @5 erroes Loss 1.2329\n",
      "  Epoch 39 Batch 2140 @5 erroes Loss 1.1831\n",
      "tirill take a tbtal of n million in .fte ==>  \n",
      "tinill ohte a tital of n mill on in n  e  Epoch 39 Batch 2150 @5 erroes Loss 0.8116\n",
      "  Epoch 39 Batch 2160 @5 erroes Loss 1.0942\n",
      "  Epoch 39 Batch 2170 @5 erroes Loss 1.1193\n",
      "  Epoch 39 Batch 2180 @5 erroes Loss 1.0762\n",
      "  Epoch 39 Batch 2190 @5 erroes Loss 0.9952\n",
      "vq fernands who has led cathtliq churchx ==>  \n",
      "ve lernands aho har led cath lichcherch   Epoch 39 Batch 2200 @5 erroes Loss 0.9061\n",
      "  Epoch 39 Batch 2210 @5 erroes Loss 0.9276\n",
      "  Epoch 39 Batch 2220 @5 erroes Loss 0.9847\n",
      "  Epoch 39 Batch 2230 @5 erroes Loss 0.9753\n",
      "  Epoch 39 Batch 2240 @5 erroes Loss 0.9286\n",
      " of sourh africa ltd. shed the thsrd qua ==>  \n",
      " on sourh a rica li . shsr soerchen  wua  Epoch 39 Batch 2250 @5 erroes Loss 0.7980\n",
      "  Epoch 39 Batch 2260 @5 erroes Loss 0.8162\n",
      "  Epoch 39 Batch 2270 @5 erroes Loss 0.7899\n",
      "  Epoch 39 Batch 2280 @5 erroes Loss 0.8183\n",
      "  Epoch 39 Batch 2290 @5 erroes Loss 0.7404\n",
      "d for commentdkhe merc sagd thap as part ==>  \n",
      "d t r commentdthe merc sagd thap as part  Epoch 39 Batch 2300 @5 erroes Loss 0.7557\n",
      "  Epoch 39 Batch 2310 @5 erroes Loss 1.2249\n",
      "  Epoch 39 Batch 2320 @5 erroes Loss 1.1175\n",
      "  Epoch 39 Batch 2330 @5 erroes Loss 1.0817\n",
      "  Epoch 39 Batch 2340 @5 erroes Loss 0.8976\n",
      " national xecuvjty councix cjuld n t be  ==>  \n",
      " nddional necuvety councin ceuld n t be   Epoch 39 Batch 2350 @5 erroes Loss 1.0245\n",
      "  Epoch 39 Batch 2360 @5 erroes Loss 0.8011\n",
      "  Epoch 39 Batch 2370 @5 erroes Loss 0.8623\n",
      "  Epoch 39 Batch 2380 @5 erroes Loss 1.0347\n",
      "  Epoch 39 Batch 2390 @5 erroes Loss 0.9066\n",
      ".s sales also had strhng growth in tha q ==>  \n",
      ".s stres alst had streng growth in tha q  Epoch 39 Batch 2400 @5 erroes Loss 0.6606\n",
      "  Epoch 39 Batch 2410 @5 erroes Loss 0.6707\n",
      "  Epoch 39 Batch 2420 @5 erroes Loss 0.7966\n",
      "  Epoch 39 Batch 2430 @5 erroes Loss 0.6707\n",
      "  Epoch 39 Batch 2440 @5 erroes Loss 0.7945\n",
      "en presideht sqncn n mr. n joined bearin ==>  \n",
      "en nresideht sencn n mr. n joined bearin  Epoch 39 Batch 2450 @5 erroes Loss 0.6233\n",
      "Epoch 39 Loss 1.0230\n",
      "\n",
      "Time taken for 1 epoch 2441.3543570041656 sec\n",
      "\n",
      "ye. dougoasfmadison a corporfte trad r w ==>  \n",
      "ye gdougoastmadison a corporste trad r t  Epoch 40 Batch 0 @5 erroes Loss 0.7359\n",
      "  Epoch 40 Batch 10 @5 erroes Loss 0.8665\n",
      "  Epoch 40 Batch 20 @5 erroes Loss 0.5586\n",
      "  Epoch 40 Batch 30 @5 erroes Loss 0.8406\n",
      "  Epoch 40 Batch 40 @5 erroes Loss 0.9146\n",
      "out tomoztow the average estimate efjn e ==>  \n",
      "out to e  e  moe tte et  t   tet  tf m t  Epoch 40 Batch 50 @5 erroes Loss 2.2464\n",
      "  Epoch 40 Batch 60 @5 erroes Loss 2.1596\n",
      "  Epoch 40 Batch 70 @5 erroes Loss 2.0635\n",
      "  Epoch 40 Batch 80 @5 erroes Loss 2.0216\n",
      "  Epoch 40 Batch 90 @5 erroes Loss 1.9563\n",
      "s from the computer stclor .ur prfmary m ==>  \n",
      "s trer mhertor er r ture r trr mren r  m  Epoch 40 Batch 100 @5 erroes Loss 1.8840\n",
      "  Epoch 40 Batch 110 @5 erroes Loss 1.8578\n",
      "  Epoch 40 Batch 120 @5 erroes Loss 1.7608\n",
      "  Epoch 40 Batch 130 @5 erroes Loss 1.6981\n",
      "  Epoch 40 Batch 140 @5 erroes Loss 1.6263\n",
      "the fiamys net capizal maiktainedvinaccu ==>  \n",
      "tha pinaeaapi  caiacanaaaicg ing  catlea  Epoch 40 Batch 150 @5 erroes Loss 1.4437\n",
      "  Epoch 40 Batch 160 @5 erroes Loss 1.2767\n",
      "  Epoch 40 Batch 170 @5 erroes Loss 1.1221\n",
      "  Epoch 40 Batch 180 @5 erroes Loss 1.2191\n",
      "  Epoch 40 Batch 190 @5 erroes Loss 1.1592\n",
      "anr other buildings di. sayd theeu.s. is ==>  \n",
      "an  tnher buildings die suyd theeus   is  Epoch 40 Batch 200 @5 erroes Loss 1.0096\n",
      "  Epoch 40 Batch 210 @5 erroes Loss 1.1514\n",
      "  Epoch 40 Batch 220 @5 erroes Loss 1.0793\n",
      "  Epoch 40 Batch 230 @5 erroes Loss 1.2238\n",
      "  Epoch 40 Batch 240 @5 erroes Loss 1.0041\n",
      "tentronpbecause tzem are so ullike any o ==>  \n",
      "testron br aus  toem are so ullioe any o  Epoch 40 Batch 250 @5 erroes Loss 1.0980\n",
      "  Epoch 40 Batch 260 @5 erroes Loss 1.0433\n",
      "  Epoch 40 Batch 270 @5 erroes Loss 0.9733\n",
      "  Epoch 40 Batch 280 @5 erroes Loss 0.9759\n",
      "  Epoch 40 Batch 290 @5 erroes Loss 0.9842\n",
      "hsses heroperiou up through tbexseventh  ==>  \n",
      "hs    heroperiou tbetheough the se enth   Epoch 40 Batch 300 @5 erroes Loss 0.9532\n",
      "  Epoch 40 Batch 310 @5 erroes Loss 1.0327\n",
      "  Epoch 40 Batch 320 @5 erroes Loss 0.7831\n",
      "  Epoch 40 Batch 330 @5 erroes Loss 0.8802\n",
      "  Epoch 40 Batch 340 @5 erroes Loss 0.8320\n",
      "cedchwm nack to the the nextryeag there  ==>  \n",
      "cen hem nack to the nhe nextrye n there   Epoch 40 Batch 350 @5 erroes Loss 0.8218\n",
      "  Epoch 40 Batch 360 @5 erroes Loss 0.8658\n",
      "  Epoch 40 Batch 370 @5 erroes Loss 0.7939\n",
      "  Epoch 40 Batch 380 @5 erroes Loss 0.8589\n",
      "  Epoch 40 Batch 390 @5 erroes Loss 0.8680\n",
      "pufchase the warrants eapipe nov. nenxth ==>  \n",
      "pupihase the warrants eapipe nov. nen sh  Epoch 40 Batch 400 @5 erroes Loss 0.7342\n",
      "  Epoch 40 Batch 410 @5 erroes Loss 0.7514\n",
      "  Epoch 40 Batch 420 @5 erroes Loss 0.9701\n",
      "  Epoch 40 Batch 430 @5 erroes Loss 0.9799\n",
      "  Epoch 40 Batch 440 @5 erroes Loss 0.9633\n",
      "ilials said chicagnfhas theqlargest popu ==>  \n",
      "ilicls said chicagnfaas the largest popu  Epoch 40 Batch 450 @5 erroes Loss 0.9620\n",
      "  Epoch 40 Batch 460 @5 erroes Loss 0.7638\n",
      "  Epoch 40 Batch 470 @5 erroes Loss 1.6459\n",
      "  Epoch 40 Batch 480 @5 erroes Loss 1.2670\n",
      "  Epoch 40 Batch 490 @5 erroes Loss 1.0530\n",
      "rain stimu ators bqtrare soddafor nerves ==>  \n",
      "rane neimu asu a betramu atid air nerves  Epoch 40 Batch 500 @5 erroes Loss 1.2063\n",
      "  Epoch 40 Batch 510 @5 erroes Loss 1.0421\n",
      "  Epoch 40 Batch 520 @5 erroes Loss 0.9397\n",
      "  Epoch 40 Batch 530 @5 erroes Loss 1.0510\n",
      "  Epoch 40 Batch 540 @5 erroes Loss 0.8164\n",
      " saidgshmreholdehs approvsd its acquisit ==>  \n",
      " s rdgshmre olders apprisid sne amauisst  Epoch 40 Batch 550 @5 erroes Loss 0.8298\n",
      "  Epoch 40 Batch 560 @5 erroes Loss 0.9072\n",
      "  Epoch 40 Batch 570 @5 erroes Loss 0.5546\n",
      "  Epoch 40 Batch 580 @5 erroes Loss 0.8549\n",
      "  Epoch 40 Batch 590 @5 erroes Loss 0.8099\n",
      "his groub will contifxe fo oush for cont ==>  \n",
      "hio groub will contifse fo oush for cont  Epoch 40 Batch 600 @5 erroes Loss 0.8551\n",
      "  Epoch 40 Batch 610 @5 erroes Loss 1.3682\n",
      "  Epoch 40 Batch 620 @5 erroes Loss 1.5799\n",
      "  Epoch 40 Batch 630 @5 erroes Loss 1.2587\n",
      "  Epoch 40 Batch 640 @5 erroes Loss 1.1102\n",
      "nerations preakyngmthis cycle vill bu ne ==>  \n",
      "necktions puenkyngmthis cycle vill bu ne  Epoch 40 Batch 650 @5 erroes Loss 0.9662\n",
      "  Epoch 40 Batch 660 @5 erroes Loss 0.8871\n",
      "  Epoch 40 Batch 670 @5 erroes Loss 1.4824\n",
      "  Epoch 40 Batch 680 @5 erroes Loss 1.2239\n",
      "  Epoch 40 Batch 690 @5 erroes Loss 1.1000\n",
      "e anm senate jujiciary c.kmittees she re ==>  \n",
      "e sci senate jujiciary c.kmittees she re  Epoch 40 Batch 700 @5 erroes Loss 1.1433\n",
      "  Epoch 40 Batch 710 @5 erroes Loss 0.9452\n",
      "  Epoch 40 Batch 720 @5 erroes Loss 1.0727\n",
      "  Epoch 40 Batch 730 @5 erroes Loss 0.8169\n",
      "  Epoch 40 Batch 740 @5 erroes Loss 0.9415\n",
      "jic car sales harepplunled n n since the ==>  \n",
      "jinecar sales haresplunled n n since the  Epoch 40 Batch 750 @5 erroes Loss 0.9572\n",
      "  Epoch 40 Batch 760 @5 erroes Loss 0.8957\n",
      "  Epoch 40 Batch 770 @5 erroes Loss 0.9121\n",
      "  Epoch 40 Batch 780 @5 erroes Loss 0.8920\n",
      "  Epoch 40 Batch 790 @5 erroes Loss 0.9523\n",
      "drug scfi p personally a vetexanxof this ==>  \n",
      "dral scficpepersonally a vetexan of this  Epoch 40 Batch 800 @5 erroes Loss 0.9090\n",
      "  Epoch 40 Batch 810 @5 erroes Loss 0.8161\n",
      "  Epoch 40 Batch 820 @5 erroes Loss 0.7195\n",
      "  Epoch 40 Batch 830 @5 erroes Loss 0.7546\n",
      "  Epoch 40 Batch 840 @5 erroes Loss 0.9245\n",
      "board member bronx bovozgh presiddnt pes ==>  \n",
      "bomnd mesber brond bem nr  presiddnd mem  Epoch 40 Batch 850 @5 erroes Loss 0.8773\n",
      "  Epoch 40 Batch 860 @5 erroes Loss 0.7683\n",
      "  Epoch 40 Batch 870 @5 erroes Loss 0.6661\n",
      "  Epoch 40 Batch 880 @5 erroes Loss 0.6703\n",
      "  Epoch 40 Batch 890 @5 erroes Loss 0.7581\n",
      "as ableftoqstxnd before my finayce st.de ==>  \n",
      "as tblefto s hnd before my finayce stede  Epoch 40 Batch 900 @5 erroes Loss 0.8429\n",
      "  Epoch 40 Batch 910 @5 erroes Loss 0.6505\n",
      "  Epoch 40 Batch 920 @5 erroes Loss 0.8476\n",
      "  Epoch 40 Batch 930 @5 erroes Loss 0.8677\n",
      "  Epoch 40 Batch 940 @5 erroes Loss 1.0177\n",
      "action one reaso rhan s likely to reacs  ==>  \n",
      "aco ln hl lselr   ilnti tlne l lilnerrlh  Epoch 40 Batch 950 @5 erroes Loss 1.9106\n",
      "  Epoch 40 Batch 960 @5 erroes Loss 1.5691\n",
      "  Epoch 40 Batch 970 @5 erroes Loss 1.3868\n",
      "  Epoch 40 Batch 980 @5 erroes Loss 1.2530\n",
      "  Epoch 40 Batch 990 @5 erroes Loss 1.2554\n",
      " a leacof req ink the heroes cansfino th ==>  \n",
      " anoencof req in  the herons cansfin  th  Epoch 40 Batch 1000 @5 erroes Loss 1.0532\n",
      "  Epoch 40 Batch 1010 @5 erroes Loss 1.0433\n",
      "  Epoch 40 Batch 1020 @5 erroes Loss 0.9975\n",
      "  Epoch 40 Batch 1030 @5 erroes Loss 0.9729\n",
      "  Epoch 40 Batch 1040 @5 erroes Loss 2.4969\n",
      "ialkpublkc offeripg of combon stock of f ==>  \n",
      "ianitosiinotm   en  tm tonien t  moetm t  Epoch 40 Batch 1050 @5 erroes Loss 2.3244\n",
      "  Epoch 40 Batch 1060 @5 erroes Loss 2.1183\n",
      "  Epoch 40 Batch 1070 @5 erroes Loss 2.0270\n",
      "  Epoch 40 Batch 1080 @5 erroes Loss 2.1064\n",
      "  Epoch 40 Batch 1090 @5 erroes Loss 1.7677\n",
      "ows the of xaving listingb on o.ly oee e ==>  \n",
      "ow  nee of oasing tisting  on of y of  t  Epoch 40 Batch 1100 @5 erroes Loss 1.5590\n",
      "  Epoch 40 Batch 1110 @5 erroes Loss 1.2591\n",
      "  Epoch 40 Batch 1120 @5 erroes Loss 1.3091\n",
      "  Epoch 40 Batch 1130 @5 erroes Loss 1.0150\n",
      "  Epoch 40 Batch 1140 @5 erroes Loss 1.1207\n",
      " unprecedented in the coqytrs s hsstorn  ==>  \n",
      " us rechdented in the comytrs s hsstorn   Epoch 40 Batch 1150 @5 erroes Loss 0.9831\n",
      "  Epoch 40 Batch 1160 @5 erroes Loss 0.9361\n",
      "  Epoch 40 Batch 1170 @5 erroes Loss 0.8931\n",
      "  Epoch 40 Batch 1180 @5 erroes Loss 1.0087\n",
      "  Epoch 40 Batch 1190 @5 erroes Loss 0.7912\n",
      "n stake but sinfe tioe s .erger with wac ==>  \n",
      "n teote but sinfe tioe sucerger with wac  Epoch 40 Batch 1200 @5 erroes Loss 0.8368\n",
      "  Epoch 40 Batch 1210 @5 erroes Loss 0.8645\n",
      "  Epoch 40 Batch 1220 @5 erroes Loss 0.9552\n",
      "  Epoch 40 Batch 1230 @5 erroes Loss 0.8403\n",
      "  Epoch 40 Batch 1240 @5 erroes Loss 0.8777\n",
      "proposale.nadequate and said thf  estrpc ==>  \n",
      "pranosaleste   uade and said the oea  ea  Epoch 40 Batch 1250 @5 erroes Loss 0.8329\n",
      "  Epoch 40 Batch 1260 @5 erroes Loss 0.8385\n",
      "  Epoch 40 Batch 1270 @5 erroes Loss 0.7335\n",
      "  Epoch 40 Batch 1280 @5 erroes Loss 0.7760\n",
      "  Epoch 40 Batch 1290 @5 erroes Loss 0.7732\n",
      "employees a minorityustajarthe companyfr ==>  \n",
      "emarotees a minorityusta arthe company r  Epoch 40 Batch 1300 @5 erroes Loss 0.8101\n",
      "  Epoch 40 Batch 1310 @5 erroes Loss 0.8289\n",
      "  Epoch 40 Batch 1320 @5 erroes Loss 0.7537\n",
      "  Epoch 40 Batch 1330 @5 erroes Loss 0.7770\n",
      "  Epoch 40 Batch 1340 @5 erroes Loss 0.7093\n",
      "herevt day here tomo row two fompanies p ==>  \n",
      "hese tomo  bay  tomo eov thy hor anies p  Epoch 40 Batch 1350 @5 erroes Loss 1.1439\n",
      "  Epoch 40 Batch 1360 @5 erroes Loss 0.8536\n",
      "  Epoch 40 Batch 1370 @5 erroes Loss 0.7466\n",
      "  Epoch 40 Batch 1380 @5 erroes Loss 0.9696\n",
      "  Epoch 40 Batch 1390 @5 erroes Loss 0.9492\n",
      "ide texacc withkmostlyhgas reseqves the  ==>  \n",
      "ide texacc withemost yhgas rese ves the   Epoch 40 Batch 1400 @5 erroes Loss 0.8562\n",
      "  Epoch 40 Batch 1410 @5 erroes Loss 0.6816\n",
      "  Epoch 40 Batch 1420 @5 erroes Loss 0.7460\n",
      "  Epoch 40 Batch 1430 @5 erroes Loss 0.9114\n",
      "  Epoch 40 Batch 1440 @5 erroes Loss 0.8843\n",
      "t n biltcon togethed the sij or escentia ==>  \n",
      "t t tol  on tone hed ihe sineor escentia  Epoch 40 Batch 1450 @5 erroes Loss 0.9302\n",
      "  Epoch 40 Batch 1460 @5 erroes Loss 0.7511\n",
      "  Epoch 40 Batch 1470 @5 erroes Loss 1.1298\n",
      "  Epoch 40 Batch 1480 @5 erroes Loss 0.9261\n",
      "  Epoch 40 Batch 1490 @5 erroes Loss 0.8244\n",
      "ily ih fctobzr mr. stein saix is margija ==>  \n",
      "il  is mfroblr mr. stein saix is margina  Epoch 40 Batch 1500 @5 erroes Loss 0.8245\n",
      "  Epoch 40 Batch 1510 @5 erroes Loss 0.8658\n",
      "  Epoch 40 Batch 1520 @5 erroes Loss 0.8868\n",
      "  Epoch 40 Batch 1530 @5 erroes Loss 1.9161\n",
      "  Epoch 40 Batch 1540 @5 erroes Loss 1.4522\n",
      "si parl a aceougt of a life already fami ==>  \n",
      "si arleaa aceo l  af a limi already limi  Epoch 40 Batch 1550 @5 erroes Loss 1.1985\n",
      "  Epoch 40 Batch 1560 @5 erroes Loss 1.1954\n",
      "  Epoch 40 Batch 1570 @5 erroes Loss 1.1401\n",
      "  Epoch 40 Batch 1580 @5 erroes Loss 0.7920\n",
      "  Epoch 40 Batch 1590 @5 erroes Loss 0.9714\n",
      "ral on on. bpads are the dominant ib ano ==>  \n",
      "ran on onc buads are the dominant ib ano  Epoch 40 Batch 1600 @5 erroes Loss 0.9464\n",
      "  Epoch 40 Batch 1610 @5 erroes Loss 0.8882\n",
      "  Epoch 40 Batch 1620 @5 erroes Loss 0.9688\n",
      "  Epoch 40 Batch 1630 @5 erroes Loss 0.8659\n",
      "  Epoch 40 Batch 1640 @5 erroes Loss 0.8704\n",
      " n billion in unautfergzed creditshto tc ==>  \n",
      " n tillion in uialt ergzed creditshto th  Epoch 40 Batch 1650 @5 erroes Loss 0.8410\n",
      "  Epoch 40 Batch 1660 @5 erroes Loss 0.8140\n",
      "  Epoch 40 Batch 1670 @5 erroes Loss 0.8598\n",
      "  Epoch 40 Batch 1680 @5 erroes Loss 0.7842\n",
      "  Epoch 40 Batch 1690 @5 erroes Loss 0.8832\n",
      "e womzetent manaaemenm available butcmr. ==>  \n",
      "e mimaetent manaaemenm available butcmr.  Epoch 40 Batch 1700 @5 erroes Loss 0.7761\n",
      "  Epoch 40 Batch 1710 @5 erroes Loss 0.6340\n",
      "  Epoch 40 Batch 1720 @5 erroes Loss 0.7911\n",
      "  Epoch 40 Batch 1730 @5 erroes Loss 0.8180\n",
      "  Epoch 40 Batch 1740 @5 erroes Loss 0.6809\n",
      "ided vrouble if they hqd the isslysfirst ==>  \n",
      "id   vrouble in they hed the is tysfirst  Epoch 40 Batch 1750 @5 erroes Loss 0.7699\n",
      "  Epoch 40 Batch 1760 @5 erroes Loss 0.8209\n",
      "  Epoch 40 Batch 1770 @5 erroes Loss 0.5143\n",
      "  Epoch 40 Batch 1780 @5 erroes Loss 0.8453\n",
      "  Epoch 40 Batch 1790 @5 erroes Loss 0.8101\n",
      "on owner keith gezied thm allegations cr ==>  \n",
      "onttwner teith ge ied the allegations cr  Epoch 40 Batch 1800 @5 erroes Loss 0.7795\n",
      "  Epoch 40 Batch 1810 @5 erroes Loss 0.8160\n",
      "  Epoch 40 Batch 1820 @5 erroes Loss 0.7731\n",
      "  Epoch 40 Batch 1830 @5 erroes Loss 0.7250\n",
      "  Epoch 40 Batch 1840 @5 erroes Loss 1.4383\n",
      "ommthing newly ar sved at asaa result of ==>  \n",
      "om   ao  hewly ar sne  at asaa be  lt rf  Epoch 40 Batch 1850 @5 erroes Loss 1.0042\n",
      "  Epoch 40 Batch 1860 @5 erroes Loss 0.9632\n",
      "  Epoch 40 Batch 1870 @5 erroes Loss 0.7788\n",
      "  Epoch 40 Batch 1880 @5 erroes Loss 0.9367\n",
      "  Epoch 40 Batch 1890 @5 erroes Loss 0.8318\n",
      " ik one plant xec fs spendingvn million  ==>  \n",
      " ittone plant dec fs spending n eillion   Epoch 40 Batch 1900 @5 erroes Loss 0.7017\n",
      "  Epoch 40 Batch 1910 @5 erroes Loss 0.6975\n",
      "  Epoch 40 Batch 1920 @5 erroes Loss 0.9679\n",
      "  Epoch 40 Batch 1930 @5 erroes Loss 0.8689\n",
      "  Epoch 40 Batch 1940 @5 erroes Loss 1.5529\n",
      " cwraw unexpectedly failed to obtain bnn ==>  \n",
      " cluee ane pedted y aoiled to obtain byn  Epoch 40 Batch 1950 @5 erroes Loss 1.3270\n",
      "  Epoch 40 Batch 1960 @5 erroes Loss 1.2035\n",
      "  Epoch 40 Batch 1970 @5 erroes Loss 0.8350\n",
      "  Epoch 40 Batch 1980 @5 erroes Loss 1.0248\n",
      "  Epoch 40 Batch 1990 @5 erroes Loss 1.0573\n",
      "bg.investigators and tre shdia as the de ==>  \n",
      "bg andestigators and tre sadia as the de  Epoch 40 Batch 2000 @5 erroes Loss 0.8470\n",
      "  Epoch 40 Batch 2010 @5 erroes Loss 0.8323\n",
      "  Epoch 40 Batch 2020 @5 erroes Loss 0.8607\n",
      "  Epoch 40 Batch 2030 @5 erroes Loss 0.7394\n",
      "  Epoch 40 Batch 2040 @5 erroes Loss 0.7744\n",
      "com arison ht saie people rre n t necess ==>  \n",
      "co  anison ht st e prople rre n t necess  Epoch 40 Batch 2050 @5 erroes Loss 0.6338\n",
      "  Epoch 40 Batch 2060 @5 erroes Loss 0.8189\n",
      "  Epoch 40 Batch 2070 @5 erroes Loss 0.5338\n",
      "  Epoch 40 Batch 2080 @5 erroes Loss 0.7192\n",
      "  Epoch 40 Batch 2090 @5 erroes Loss 0.8462\n",
      "privatizep de asumr. orahzeclaims in the ==>  \n",
      "pr tatine  de asumr. orahsetiirpa an the  Epoch 40 Batch 2100 @5 erroes Loss 0.9507\n",
      "  Epoch 40 Batch 2110 @5 erroes Loss 0.9221\n",
      "  Epoch 40 Batch 2120 @5 erroes Loss 0.9643\n",
      "  Epoch 40 Batch 2130 @5 erroes Loss 1.0731\n",
      "  Epoch 40 Batch 2140 @5 erroes Loss 0.9896\n",
      "rices in tse cbmpanyxs two majoruareas b ==>  \n",
      "ritos in the campany s two majoruareas b  Epoch 40 Batch 2150 @5 erroes Loss 0.7586\n",
      "  Epoch 40 Batch 2160 @5 erroes Loss 0.7458\n",
      "  Epoch 40 Batch 2170 @5 erroes Loss 1.3411\n",
      "  Epoch 40 Batch 2180 @5 erroes Loss 1.0248\n",
      "  Epoch 40 Batch 2190 @5 erroes Loss 0.9952\n",
      " wejnesdaymearlier this aontymima said i ==>  \n",
      " w   esdaymearlier this aontymiaaesaid i  Epoch 40 Batch 2200 @5 erroes Loss 0.8986\n",
      "  Epoch 40 Batch 2210 @5 erroes Loss 0.7819\n",
      "  Epoch 40 Batch 2220 @5 erroes Loss 0.6831\n",
      "  Epoch 40 Batch 2230 @5 erroes Loss 0.8192\n",
      "  Epoch 40 Batch 2240 @5 erroes Loss 0.8121\n",
      "feel tmey re ssifping between thj crlcks ==>  \n",
      "fen  toey re ssifping betseen the crlcks  Epoch 40 Batch 2250 @5 erroes Loss 0.8076\n",
      "  Epoch 40 Batch 2260 @5 erroes Loss 0.8113\n",
      "  Epoch 40 Batch 2270 @5 erroes Loss 0.8243\n",
      "  Epoch 40 Batch 2280 @5 erroes Loss 0.8027\n",
      "  Epoch 40 Batch 2290 @5 erroes Loss 0.6804\n",
      " regions ark the faetest growing crras f ==>  \n",
      " re ions ark ihe faetest growing cresi f  Epoch 40 Batch 2300 @5 erroes Loss 0.7416\n",
      "  Epoch 40 Batch 2310 @5 erroes Loss 0.7562\n",
      "  Epoch 40 Batch 2320 @5 erroes Loss 0.6831\n",
      "  Epoch 40 Batch 2330 @5 erroes Loss 2.3077\n",
      "  Epoch 40 Batch 2340 @5 erroes Loss 2.1459\n",
      "gs the.onlpynew issuedpriced yesterday i ==>  \n",
      "gs she on   s   sn  n ooede  w   h     s  Epoch 40 Batch 2350 @5 erroes Loss 2.2023\n",
      "  Epoch 40 Batch 2360 @5 erroes Loss 2.0954\n",
      "  Epoch 40 Batch 2370 @5 erroes Loss 1.8308\n",
      "  Epoch 40 Batch 2380 @5 erroes Loss 1.5926\n",
      "  Epoch 40 Batch 2390 @5 erroes Loss 1.3735\n",
      "at shbov america was noh q bcttle for ju ==>  \n",
      "at o e a a erica was no  h was ho for ju  Epoch 40 Batch 2400 @5 erroes Loss 1.1471\n",
      "  Epoch 40 Batch 2410 @5 erroes Loss 1.1181\n",
      "  Epoch 40 Batch 2420 @5 erroes Loss 1.0211\n",
      "  Epoch 40 Batch 2430 @5 erroes Loss 1.1137\n",
      "  Epoch 40 Batch 2440 @5 erroes Loss 0.8044\n",
      "unds wre a tomparedawith what japsn and  ==>  \n",
      "un   wrd a tomparedaweth wret japsn and   Epoch 40 Batch 2450 @5 erroes Loss 0.7461\n",
      "Epoch 40 Loss 1.0354\n",
      "\n",
      "Time taken for 1 epoch 2771.905330181122 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 40\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (inp, targ)) in enumerate(dataset):\n",
    "        loss = 0\n",
    "        \n",
    "        # modify input data to represent error\n",
    "        nb_error = min(max(epoch - 0, 0), 5) # stoping change in first 5 epoch\n",
    "        random_index = np.random.randint(0, max_length, (BATCH_SIZE, nb_error))\n",
    "        random_value = np.random.randint(1, len(unique), (BATCH_SIZE, nb_error))\n",
    "        \n",
    "        for b in range(BATCH_SIZE):\n",
    "            for d in range(nb_error):\n",
    "                inp = inp.numpy()\n",
    "                inp[b, random_index[d, d]] = random_value[b, d]\n",
    "                inp = tf.contrib.eager.Variable(inp)\n",
    "                # tf.scatter_update(inp, [b, random_index[d, d]], random_value[b, d])\n",
    "        \n",
    "        if batch % 50 ==0:\n",
    "            # print one for test\n",
    "            input_string = [idx2char[idx.numpy()] for idx in inp[0]]\n",
    "            for i in input_string:\n",
    "                print(i, end='')\n",
    "            print(' ==>  ')\n",
    "        \n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_output, enc_hidden = encoder(inp, hidden)\n",
    "            \n",
    "            dec_hidden = enc_hidden\n",
    "            \n",
    "            dec_input = tf.expand_dims([char2idx['<start>']] * BATCH_SIZE, 1)       \n",
    "            \n",
    "            # Teacher forcing - feeding the target as the next input\n",
    "            for t in range(0, targ.shape[1]):\n",
    "                # passing enc_output to the decoder\n",
    "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "                \n",
    "                if batch % 50==0:\n",
    "                    output = predictions[0]\n",
    "                    print(idx2char[output.numpy().argmax(-1)], end='')\n",
    "                    \n",
    "                \n",
    "                loss += loss_function(targ[:, t], predictions)\n",
    "                \n",
    "                # using teacher forcing\n",
    "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "        \n",
    "        batch_loss = (loss / int(targ.shape[1]))\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        variables = encoder.variables + decoder.variables\n",
    "        \n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        if batch % 10 == 0:\n",
    "            print('  Epoch {} Batch {} @{} erroes Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         nb_error,\n",
    "                                                         batch_loss.numpy()))\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    \n",
    "    print('Epoch {} Loss {:.4f}\\n'.format(epoch + 1,\n",
    "                                        total_loss / batch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "brZsYa3P_IKd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Spelling_correction.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
