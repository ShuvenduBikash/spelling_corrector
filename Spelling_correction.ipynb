{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9533,
     "status": "ok",
     "timestamp": 1535255969007,
     "user": {
      "displayName": "Shuvendu BIkash",
      "photoUrl": "//lh4.googleusercontent.com/-cb2XgdoysqI/AAAAAAAAAAI/AAAAAAAAAFg/3GJlDMCUuNI/s50-c-k-no/photo.jpg",
      "userId": "114258866435529703404"
     },
     "user_tz": -360
    },
    "id": "Oi7fhhdI5Ima",
    "outputId": "0a3c153a-9c7c-4f0f-9725-b42928573c07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in c:\\users\\bikas\\anaconda3\\lib\\site-packages (1.0.22)\n",
      "Requirement already satisfied: observations in c:\\users\\bikas\\anaconda3\\lib\\site-packages (0.1.4)\n",
      "Requirement already satisfied: numpy>=1.7 in c:\\users\\bikas\\anaconda3\\lib\\site-packages (from observations) (1.14.3)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\bikas\\anaconda3\\lib\\site-packages (from observations) (1.11.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode\n",
    "!pip install observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8258,
     "status": "ok",
     "timestamp": 1535255977367,
     "user": {
      "displayName": "Shuvendu BIkash",
      "photoUrl": "//lh4.googleusercontent.com/-cb2XgdoysqI/AAAAAAAAAAI/AAAAAAAAAFg/3GJlDMCUuNI/s50-c-k-no/photo.jpg",
      "userId": "114258866435529703404"
     },
     "user_tz": -360
    },
    "id": "CRs1lhjh4ttT",
    "outputId": "6180eb10-0649-4415-9c9b-cecfd8aa8d7d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bikas\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "# Import TensorFlow >= 1.10 and enable eager execution\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import unidecode\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EnG1_mD4EOGB"
   },
   "source": [
    "## Load PTB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13829,
     "status": "ok",
     "timestamp": 1535256006703,
     "user": {
      "displayName": "Shuvendu BIkash",
      "photoUrl": "//lh4.googleusercontent.com/-cb2XgdoysqI/AAAAAAAAAAI/AAAAAAAAAFg/3GJlDMCUuNI/s50-c-k-no/photo.jpg",
      "userId": "114258866435529703404"
     },
     "user_tz": -360
    },
    "id": "fpvsxgZOEKkS",
    "outputId": "e4703fe3-8da4-4ca7-e17f-cd44b5074771"
   },
   "outputs": [],
   "source": [
    "import observations\n",
    "text, testfile, valfile = getattr(observations, 'ptb')('data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1131,
     "status": "ok",
     "timestamp": 1535256007949,
     "user": {
      "displayName": "Shuvendu BIkash",
      "photoUrl": "//lh4.googleusercontent.com/-cb2XgdoysqI/AAAAAAAAAAI/AAAAAAAAAFg/3GJlDMCUuNI/s50-c-k-no/photo.jpg",
      "userId": "114258866435529703404"
     },
     "user_tz": -360
    },
    "id": "IOGvsf87EWfJ",
    "outputId": "f47efcad-206c-47e5-f9b1-3bb9f7030d63"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5269890"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1513,
     "status": "ok",
     "timestamp": 1535256009723,
     "user": {
      "displayName": "Shuvendu BIkash",
      "photoUrl": "//lh4.googleusercontent.com/-cb2XgdoysqI/AAAAAAAAAAI/AAAAAAAAAFg/3GJlDMCUuNI/s50-c-k-no/photo.jpg",
      "userId": "114258866435529703404"
     },
     "user_tz": -360
    },
    "id": "RM1Av3BjylpW",
    "outputId": "ecaa3474-f0cd-4e79-9a3c-55c477691311"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4715474\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(w):\n",
    "    w = w.lower()\n",
    "    w = re.sub(\"<eos>\", \" \", w)\n",
    "    w = re.sub(\"<unk>\", \" \", w)\n",
    "    \n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-z?.!,]+\", \" \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    \n",
    "    return w\n",
    "\n",
    "text = preprocess_sentence(text)\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1069,
     "status": "ok",
     "timestamp": 1535256011004,
     "user": {
      "displayName": "Shuvendu BIkash",
      "photoUrl": "//lh4.googleusercontent.com/-cb2XgdoysqI/AAAAAAAAAAI/AAAAAAAAAFg/3GJlDMCUuNI/s50-c-k-no/photo.jpg",
      "userId": "114258866435529703404"
     },
     "user_tz": -360
    },
    "id": "khE448cQ6Xfw",
    "outputId": "f06c5c9d-e9b2-4b24-b2f2-cbd7c2be39c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique characters  29\n"
     ]
    }
   ],
   "source": [
    "# unique contains all the unique characters in the file\n",
    "unique = ['<start>']+ sorted(set(text))\n",
    "\n",
    "# creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(unique)}\n",
    "idx2char = {i:u for i, u in enumerate(unique)}\n",
    "\n",
    "print(\"number of unique characters \", len(unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c4lvvfUS6dSz"
   },
   "outputs": [],
   "source": [
    "# setting the maximum length sentence we want for a single input in characters\n",
    "max_length = 35\n",
    "\n",
    "# length of the vocabulary in chars\n",
    "vocab_size = len(unique)\n",
    "\n",
    "# the embedding dimension \n",
    "embedding_dim = 256\n",
    "\n",
    "# number of RNN (here GRU) units\n",
    "units = 1024\n",
    "\n",
    "# batch size \n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# buffer size to shuffle our dataset\n",
    "BUFFER_SIZE = 12000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3405,
     "status": "ok",
     "timestamp": 1535256016095,
     "user": {
      "displayName": "Shuvendu BIkash",
      "photoUrl": "//lh4.googleusercontent.com/-cb2XgdoysqI/AAAAAAAAAAI/AAAAAAAAAFg/3GJlDMCUuNI/s50-c-k-no/photo.jpg",
      "userId": "114258866435529703404"
     },
     "user_tz": -360
    },
    "id": "tIuKhrHsl0LI",
    "outputId": "df1ea0e1-af1f-49ab-cf79-fc6d109ed327"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1571813, 35)\n",
      "(1571813, 35)\n"
     ]
    }
   ],
   "source": [
    "input_text = []\n",
    "target_text = []\n",
    "\n",
    "for f in range(0, len(text)-max_length, 3):\n",
    "    inps = text[f:f+max_length]\n",
    "    targ = text[f:f+max_length]\n",
    "\n",
    "    input_text.append([char2idx[i] for i in inps])\n",
    "    target_text.append([char2idx[t] for t in targ])\n",
    "    \n",
    "print (np.array(input_text).shape)\n",
    "print (np.array(target_text).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13345,
     "status": "ok",
     "timestamp": 1535256029536,
     "user": {
      "displayName": "Shuvendu BIkash",
      "photoUrl": "//lh4.googleusercontent.com/-cb2XgdoysqI/AAAAAAAAAAI/AAAAAAAAAFg/3GJlDMCUuNI/s50-c-k-no/photo.jpg",
      "userId": "114258866435529703404"
     },
     "user_tz": -360
    },
    "id": "FrM3H4JMl-D5",
    "outputId": "034adb16-b1fc-467c-8cc5-224d62ec237f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-3bbc2611a7c8>:2: batch_and_drop_remainder (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.batch(..., drop_remainder=True)`.\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((input_text, target_text)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.apply(tf.contrib.data.batch_and_drop_remainder(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RdRPkpZmmtSe"
   },
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J4JrCs-QmCQl"
   },
   "outputs": [],
   "source": [
    "def gru(units):\n",
    "  # If you have a GPU, we recommend using CuDNNGRU(provides a 3x speedup than GRU)\n",
    "  # the code automatically does that.\n",
    "  if tf.test.is_gpu_available():\n",
    "    return tf.keras.layers.CuDNNGRU(units, \n",
    "                                    return_sequences=True, \n",
    "                                    return_state=True,\n",
    "                                    recurrent_initializer='glorot_uniform')\n",
    "  else:\n",
    "    return tf.keras.layers.GRU(units, \n",
    "                               return_sequences=True, \n",
    "                               return_state=True,\n",
    "                               recurrent_initializer='glorot_uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8T4dsBt_nWTh"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.enc_units)\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)        \n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X9fldFu5nYLu"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.dec_units)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        # used for attention\n",
    "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        \n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        \n",
    "        # score shape == (batch_size, max_length, hidden_size)\n",
    "        score = tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis))\n",
    "        \n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "        \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        # output shape == (batch_size * max_length, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        # output shape == (batch_size * max_length, vocab)\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, state, attention_weights\n",
    "        \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.dec_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ViKc0u3WqVY4"
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(len(unique), embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(len(unique), embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q8Sbf22Yq4sx"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(2.5e-5)\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = 1 - np.equal(real, 0)\n",
    "  loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K9VDoDV3rqCB"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_dir = os.path.join(checkpoint_dir, \"lr.000025_error4\")\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.checkpointable.util.CheckpointLoadStatus at 0x1e1d9181f98>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "UtkqNvVVrtH9",
    "outputId": "0470a286-b70c-493d-fe2f-c5daeee9d49e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnational business machines corp. a   (original)\n",
      "rnation l business mychines corp. a ==>  \n",
      "rnation l business mychines corp. a  Epoch 1 Batch 0 @2 erroes Loss 0.2373\n",
      "  Epoch 1 Batch 50 @3 erroes Loss 0.3878\n",
      "  Epoch 1 Batch 100 @2 erroes Loss 0.2420\n",
      "  Epoch 1 Batch 150 @4 erroes Loss 0.4566\n",
      "  Epoch 1 Batch 200 @0 erroes Loss 0.0420\n",
      "  Epoch 1 Batch 250 @4 erroes Loss 0.4631\n",
      "  Epoch 1 Batch 300 @1 erroes Loss 0.1431\n",
      "  Epoch 1 Batch 350 @2 erroes Loss 0.2312\n",
      "  Epoch 1 Batch 400 @0 erroes Loss 0.0355\n",
      "  Epoch 1 Batch 450 @3 erroes Loss 0.3549\n",
      "increasingly trade debate foreigner   (original)\n",
      "increasinglc trade deb te foreimder ==>  \n",
      "increasingly trade deb te foreimder  Epoch 1 Batch 500 @4 erroes Loss 0.4243\n",
      "  Epoch 1 Batch 550 @2 erroes Loss 0.2292\n",
      "  Epoch 1 Batch 600 @2 erroes Loss 0.2388\n",
      "  Epoch 1 Batch 650 @3 erroes Loss 0.3318\n",
      "  Epoch 1 Batch 700 @2 erroes Loss 0.2480\n",
      "  Epoch 1 Batch 750 @0 erroes Loss 0.0399\n",
      "  Epoch 1 Batch 800 @1 erroes Loss 0.1387\n",
      "  Epoch 1 Batch 850 @4 erroes Loss 0.4900\n",
      "  Epoch 1 Batch 900 @4 erroes Loss 0.4374\n",
      "  Epoch 1 Batch 950 @3 erroes Loss 0.3564\n",
      "wth next year common wisdom suggest   (original)\n",
      "wth next dear common wisdom suggest ==>  \n",
      "wth next dear common wisdom suggest  Epoch 1 Batch 1000 @1 erroes Loss 0.1351\n",
      "  Epoch 1 Batch 1050 @2 erroes Loss 0.1438\n",
      "  Epoch 1 Batch 1100 @1 erroes Loss 0.1390\n",
      "  Epoch 1 Batch 1150 @2 erroes Loss 0.2274\n",
      "  Epoch 1 Batch 1200 @2 erroes Loss 0.2541\n",
      "  Epoch 1 Batch 1250 @1 erroes Loss 0.1416\n",
      "  Epoch 1 Batch 1300 @3 erroes Loss 0.3511\n",
      "  Epoch 1 Batch 1350 @1 erroes Loss 0.1340\n",
      "  Epoch 1 Batch 1400 @2 erroes Loss 0.2258\n",
      "  Epoch 1 Batch 1450 @1 erroes Loss 0.1455\n",
      "ntial the problem involves the moti   (original)\n",
      "nlial the problemcinvoltes the moti ==>  \n",
      "nlial the problemcinvoltes the moti  Epoch 1 Batch 1500 @3 erroes Loss 0.3367\n",
      "  Epoch 1 Batch 1550 @0 erroes Loss 0.0336\n",
      "  Epoch 1 Batch 1600 @2 erroes Loss 0.2554\n",
      "  Epoch 1 Batch 1650 @3 erroes Loss 0.3428\n",
      "  Epoch 1 Batch 1700 @0 erroes Loss 0.0348\n",
      "  Epoch 1 Batch 1750 @3 erroes Loss 0.3626\n",
      "  Epoch 1 Batch 1800 @4 erroes Loss 0.4222\n",
      "  Epoch 1 Batch 1850 @2 erroes Loss 0.2356\n",
      "  Epoch 1 Batch 1900 @4 erroes Loss 0.4404\n",
      "  Epoch 1 Batch 1950 @1 erroes Loss 0.1299\n",
      "a remarkable turnaround in terms of   (original)\n",
      "a rtmarkable nurnaround in tecms of ==>  \n",
      "a rtmarkable nurnaround in tecms of  Epoch 1 Batch 2000 @3 erroes Loss 0.3058\n",
      "  Epoch 1 Batch 2050 @3 erroes Loss 0.3295\n",
      "  Epoch 1 Batch 2100 @3 erroes Loss 0.3117\n",
      "  Epoch 1 Batch 2150 @0 erroes Loss 0.0324\n",
      "  Epoch 1 Batch 2200 @3 erroes Loss 0.3239\n",
      "  Epoch 1 Batch 2250 @2 erroes Loss 0.2199\n",
      "  Epoch 1 Batch 2300 @2 erroes Loss 0.2397\n",
      "  Epoch 1 Batch 2350 @2 erroes Loss 0.2445\n",
      "  Epoch 1 Batch 2400 @3 erroes Loss 0.3296\n",
      "  Epoch 1 Batch 2450 @0 erroes Loss 0.0410\n",
      "its insistence that any plane has o   (original)\n",
      "itslinsistence thahdany plane has o ==>  \n",
      "its insistence thatdany plane has o  Epoch 1 Batch 2500 @4 erroes Loss 0.3543\n",
      "  Epoch 1 Batch 2550 @0 erroes Loss 0.0349\n",
      "  Epoch 1 Batch 2600 @0 erroes Loss 0.0344\n",
      "  Epoch 1 Batch 2650 @4 erroes Loss 0.4341\n",
      "  Epoch 1 Batch 2700 @0 erroes Loss 0.0309\n",
      "  Epoch 1 Batch 2750 @4 erroes Loss 0.4209\n",
      "  Epoch 1 Batch 2800 @0 erroes Loss 0.0403\n",
      "  Epoch 1 Batch 2850 @4 erroes Loss 0.3285\n",
      "  Epoch 1 Batch 2900 @1 erroes Loss 0.1335\n",
      "  Epoch 1 Batch 2950 @3 erroes Loss 0.3253\n",
      "irline amendment said rep. james d.   (original)\n",
      "irline amendment said rep.gjames d. ==>  \n",
      "irline amendment said rep. james d.  Epoch 1 Batch 3000 @1 erroes Loss 0.1997\n",
      "  Epoch 1 Batch 3050 @3 erroes Loss 0.3233\n",
      "  Epoch 1 Batch 3100 @0 erroes Loss 0.0413\n",
      "  Epoch 1 Batch 3150 @0 erroes Loss 0.0696\n",
      "  Epoch 1 Batch 3200 @2 erroes Loss 0.2671\n",
      "  Epoch 1 Batch 3250 @3 erroes Loss 0.3310\n",
      "  Epoch 1 Batch 3300 @2 erroes Loss 0.2127\n",
      "  Epoch 1 Batch 3350 @3 erroes Loss 0.3162\n",
      "  Epoch 1 Batch 3400 @1 erroes Loss 0.1488\n",
      "  Epoch 1 Batch 3450 @4 erroes Loss 0.4273\n",
      "cludes the ad agencies or the polit   (original)\n",
      "cludes thi ad agencieszor the po it ==>  \n",
      "cludes thi ad agencies or the po it  Epoch 1 Batch 3500 @3 erroes Loss 0.2985\n",
      "  Epoch 1 Batch 3550 @0 erroes Loss 0.0373\n",
      "  Epoch 1 Batch 3600 @1 erroes Loss 0.1348\n",
      "  Epoch 1 Batch 3650 @4 erroes Loss 0.4489\n",
      "  Epoch 1 Batch 3700 @3 erroes Loss 0.3249\n",
      "  Epoch 1 Batch 3750 @0 erroes Loss 0.0376\n",
      "  Epoch 1 Batch 3800 @1 erroes Loss 0.1313\n",
      "  Epoch 1 Batch 3850 @4 erroes Loss 0.3451\n",
      "  Epoch 1 Batch 3900 @1 erroes Loss 0.1280\n",
      "  Epoch 1 Batch 3950 @3 erroes Loss 0.4335\n",
      "rp price swings has been under atta   (original)\n",
      "rp prpce swings has been under atta ==>  \n",
      "rp prpce swings has been under atta  Epoch 1 Batch 4000 @1 erroes Loss 0.1160\n",
      "  Epoch 1 Batch 4050 @4 erroes Loss 0.3420\n",
      "  Epoch 1 Batch 4100 @2 erroes Loss 0.2211\n",
      "  Epoch 1 Batch 4150 @4 erroes Loss 0.4234\n",
      "  Epoch 1 Batch 4200 @2 erroes Loss 0.2478\n",
      "  Epoch 1 Batch 4250 @0 erroes Loss 0.0344\n",
      "  Epoch 1 Batch 4300 @4 erroes Loss 0.4154\n",
      "  Epoch 1 Batch 4350 @0 erroes Loss 0.0339\n",
      "  Epoch 1 Batch 4400 @1 erroes Loss 0.1366\n",
      "  Epoch 1 Batch 4450 @2 erroes Loss 0.2323\n",
      "equipment are now being ordered to    (original)\n",
      "equipment are now being ordered to  ==>  \n",
      "equipment are now being ordered to   Epoch 1 Batch 4500 @0 erroes Loss 0.0351\n",
      "  Epoch 1 Batch 4550 @4 erroes Loss 0.3789\n",
      "  Epoch 1 Batch 4600 @2 erroes Loss 0.1746\n",
      "  Epoch 1 Batch 4650 @2 erroes Loss 0.2251\n",
      "  Epoch 1 Batch 4700 @2 erroes Loss 0.2310\n",
      "  Epoch 1 Batch 4750 @4 erroes Loss 0.4539\n",
      "  Epoch 1 Batch 4800 @4 erroes Loss 0.4433\n",
      "  Epoch 1 Batch 4850 @3 erroes Loss 0.3193\n",
      "  Epoch 1 Batch 4900 @3 erroes Loss 0.3455\n",
      "  Epoch 1 Batch 4950 @0 erroes Loss 0.0387\n",
      "painted tacked down in a behind the   (original)\n",
      "pannled tacked down in a behind thp ==>  \n",
      "pannled tacked down in a behind the  Epoch 1 Batch 5000 @4 erroes Loss 0.3412\n",
      "  Epoch 1 Batch 5050 @2 erroes Loss 0.2538\n",
      "  Epoch 1 Batch 5100 @1 erroes Loss 0.1240\n",
      "  Epoch 1 Batch 5150 @3 erroes Loss 0.3294\n",
      "  Epoch 1 Batch 5200 @0 erroes Loss 0.0351\n",
      "  Epoch 1 Batch 5250 @2 erroes Loss 0.2386\n",
      "  Epoch 1 Batch 5300 @0 erroes Loss 0.0381\n",
      "  Epoch 1 Batch 5350 @1 erroes Loss 0.1326\n",
      "  Epoch 1 Batch 5400 @1 erroes Loss 0.1297\n",
      "  Epoch 1 Batch 5450 @3 erroes Loss 0.3224\n",
      "ain reject mr. green s proposal in    (original)\n",
      "ain reject m . gresn s proposal in  ==>  \n",
      "ain reject m . gresn s proso al in   Epoch 1 Batch 5500 @2 erroes Loss 0.2406\n",
      "  Epoch 1 Batch 5550 @4 erroes Loss 0.3926\n",
      "  Epoch 1 Batch 5600 @3 erroes Loss 0.3186\n",
      "  Epoch 1 Batch 5650 @4 erroes Loss 0.4452\n",
      "  Epoch 1 Batch 5700 @0 erroes Loss 0.0371\n",
      "  Epoch 1 Batch 5750 @0 erroes Loss 0.0387\n",
      "  Epoch 1 Batch 5800 @3 erroes Loss 0.3729\n",
      "  Epoch 1 Batch 5850 @4 erroes Loss 0.4418\n",
      "  Epoch 1 Batch 5900 @1 erroes Loss 0.1526\n",
      "  Epoch 1 Batch 5950 @0 erroes Loss 0.0390\n",
      "we should also concede that in the    (original)\n",
      "we should aleoxconmeqe that in the  ==>  \n",
      "we should aleorconmere that in the   Epoch 1 Batch 6000 @4 erroes Loss 0.4105\n",
      "  Epoch 1 Batch 6050 @1 erroes Loss 0.1358\n",
      "  Epoch 1 Batch 6100 @2 erroes Loss 0.2555\n",
      "  Epoch 1 Batch 6150 @4 erroes Loss 0.3949\n",
      "  Epoch 1 Batch 6200 @4 erroes Loss 0.4665\n",
      "  Epoch 1 Batch 6250 @2 erroes Loss 0.2053\n",
      "  Epoch 1 Batch 6300 @2 erroes Loss 0.2282\n",
      "  Epoch 1 Batch 6350 @1 erroes Loss 0.1355\n",
      "  Epoch 1 Batch 6400 @0 erroes Loss 0.0353\n",
      "  Epoch 1 Batch 6450 @0 erroes Loss 0.0322\n",
      "ll aliens who in the dark to it is    (original)\n",
      "ll aliens who in the dark to it is  ==>  \n",
      "ll aliens who in the dark to it is   Epoch 1 Batch 6500 @0 erroes Loss 0.0325\n",
      "  Epoch 1 Batch 6550 @4 erroes Loss 0.3497\n",
      "  Epoch 1 Batch 6600 @1 erroes Loss 0.1578\n",
      "  Epoch 1 Batch 6650 @2 erroes Loss 0.2616\n",
      "  Epoch 1 Batch 6700 @2 erroes Loss 0.2322\n",
      "  Epoch 1 Batch 6750 @1 erroes Loss 0.1373\n",
      "  Epoch 1 Batch 6800 @0 erroes Loss 0.0400\n",
      "  Epoch 1 Batch 6850 @0 erroes Loss 0.0340\n",
      "  Epoch 1 Batch 6900 @4 erroes Loss 0.4646\n",
      "  Epoch 1 Batch 6950 @0 erroes Loss 0.0571\n",
      " new york federal reserve bank call   (original)\n",
      " new york federal reserve bank call ==>  \n",
      " new york federal reserve bank call  Epoch 1 Batch 7000 @0 erroes Loss 0.0335\n",
      "  Epoch 1 Batch 7050 @3 erroes Loss 0.3526\n",
      "  Epoch 1 Batch 7100 @3 erroes Loss 0.3097\n",
      "  Epoch 1 Batch 7150 @2 erroes Loss 0.2867\n",
      "  Epoch 1 Batch 7200 @2 erroes Loss 0.2126\n",
      "  Epoch 1 Batch 7250 @0 erroes Loss 0.0356\n",
      "  Epoch 1 Batch 7300 @0 erroes Loss 0.0291\n",
      "  Epoch 1 Batch 7350 @0 erroes Loss 0.0326\n",
      "  Epoch 1 Batch 7400 @0 erroes Loss 0.0420\n",
      "  Epoch 1 Batch 7450 @2 erroes Loss 0.2294\n",
      "t number of of traders and fines of   (original)\n",
      "t numker ol of triders vnd fines of ==>  \n",
      "t numeer ol of triders vnd fines of  Epoch 1 Batch 7500 @4 erroes Loss 0.4082\n",
      "  Epoch 1 Batch 7550 @4 erroes Loss 0.4363\n",
      "  Epoch 1 Batch 7600 @1 erroes Loss 0.1351\n",
      "  Epoch 1 Batch 7650 @0 erroes Loss 0.0334\n",
      "  Epoch 1 Batch 7700 @2 erroes Loss 0.2078\n",
      "  Epoch 1 Batch 7750 @4 erroes Loss 0.4525\n",
      "  Epoch 1 Batch 7800 @1 erroes Loss 0.1201\n",
      "  Epoch 1 Batch 7850 @2 erroes Loss 0.2216\n",
      "  Epoch 1 Batch 7900 @0 erroes Loss 0.0324\n",
      "  Epoch 1 Batch 7950 @4 erroes Loss 0.2893\n",
      "or possibly a of with the soviet un   (original)\n",
      "ol p ssibly a of with the soviet un ==>  \n",
      "ol p ssibly a of with the soviet un  Epoch 1 Batch 8000 @2 erroes Loss 0.2532\n",
      "  Epoch 1 Batch 8050 @2 erroes Loss 0.2389\n",
      "  Epoch 1 Batch 8100 @3 erroes Loss 0.3224\n",
      "  Epoch 1 Batch 8150 @4 erroes Loss 0.4186\n",
      "  Epoch 1 Batch 8200 @2 erroes Loss 0.2072\n",
      "  Epoch 1 Batch 8250 @3 erroes Loss 0.2232\n",
      "  Epoch 1 Batch 8300 @2 erroes Loss 0.2724\n",
      "  Epoch 1 Batch 8350 @3 erroes Loss 0.3814\n",
      "  Epoch 1 Batch 8400 @1 erroes Loss 0.1364\n",
      "  Epoch 1 Batch 8450 @0 erroes Loss 0.0337\n",
      "a month after his administration pl   (original)\n",
      "a month after his adbinistration pl ==>  \n",
      "a month after his adbinistration pl  Epoch 1 Batch 8500 @1 erroes Loss 0.1324\n",
      "  Epoch 1 Batch 8550 @0 erroes Loss 0.0683\n",
      "  Epoch 1 Batch 8600 @2 erroes Loss 0.2225\n",
      "  Epoch 1 Batch 8650 @2 erroes Loss 0.3408\n",
      "  Epoch 1 Batch 8700 @0 erroes Loss 0.0364\n",
      "  Epoch 1 Batch 8750 @2 erroes Loss 0.2307\n",
      "  Epoch 1 Batch 8800 @0 erroes Loss 0.0353\n",
      "  Epoch 1 Batch 8850 @2 erroes Loss 0.2263\n",
      "  Epoch 1 Batch 8900 @0 erroes Loss 0.0294\n",
      "  Epoch 1 Batch 8950 @3 erroes Loss 0.3679\n",
      "nitely not be an option he said non   (original)\n",
      "nitely not be an optiom he said non ==>  \n",
      "nitely not be an option he said non  Epoch 1 Batch 9000 @1 erroes Loss 0.1295\n",
      "  Epoch 1 Batch 9050 @0 erroes Loss 0.0476\n",
      "  Epoch 1 Batch 9100 @0 erroes Loss 0.0290\n",
      "  Epoch 1 Batch 9150 @1 erroes Loss 0.1327\n",
      "  Epoch 1 Batch 9200 @3 erroes Loss 0.3328\n",
      "  Epoch 1 Batch 9250 @4 erroes Loss 0.4123\n",
      "  Epoch 1 Batch 9300 @4 erroes Loss 0.4364\n",
      "  Epoch 1 Batch 9350 @4 erroes Loss 0.4330\n",
      "  Epoch 1 Batch 9400 @4 erroes Loss 0.4271\n",
      "  Epoch 1 Batch 9450 @4 erroes Loss 0.4575\n",
      "because of the with which he handle   (original)\n",
      "because of thc with which h. handle ==>  \n",
      "because of the with which h. handle  Epoch 1 Batch 9500 @2 erroes Loss 0.2007\n",
      "  Epoch 1 Batch 9550 @4 erroes Loss 0.4149\n",
      "  Epoch 1 Batch 9600 @1 erroes Loss 0.1353\n",
      "  Epoch 1 Batch 9650 @1 erroes Loss 0.1252\n",
      "  Epoch 1 Batch 9700 @2 erroes Loss 0.2162\n",
      "  Epoch 1 Batch 9750 @3 erroes Loss 0.3166\n",
      "  Epoch 1 Batch 9800 @4 erroes Loss 0.3989\n",
      "  Epoch 1 Batch 9850 @4 erroes Loss 0.4200\n",
      "  Epoch 1 Batch 9900 @1 erroes Loss 0.1270\n",
      "  Epoch 1 Batch 9950 @0 erroes Loss 0.0318\n",
      "open line to his assistant s office   (original)\n",
      "ope  line do yis assistant s office ==>  \n",
      "ope  line do yis assistant s office  Epoch 1 Batch 10000 @3 erroes Loss 0.3508\n",
      "  Epoch 1 Batch 10050 @2 erroes Loss 0.2108\n",
      "  Epoch 1 Batch 10100 @4 erroes Loss 0.3966\n",
      "  Epoch 1 Batch 10150 @0 erroes Loss 0.0393\n",
      "  Epoch 1 Batch 10200 @4 erroes Loss 0.4894\n",
      "  Epoch 1 Batch 10250 @2 erroes Loss 0.2342\n",
      "  Epoch 1 Batch 10300 @3 erroes Loss 0.3366\n",
      "  Epoch 1 Batch 10350 @3 erroes Loss 0.3234\n",
      "  Epoch 1 Batch 10400 @2 erroes Loss 0.2210\n",
      "  Epoch 1 Batch 10450 @4 erroes Loss 0.4703\n",
      "funds in the latest week despite fu   (original)\n",
      "fundc in t.e lotest qeek despite fu ==>  \n",
      "fund  in the lotest qeek despite fu  Epoch 1 Batch 10500 @4 erroes Loss 0.4097\n",
      "  Epoch 1 Batch 10550 @4 erroes Loss 0.3986\n",
      "  Epoch 1 Batch 10600 @0 erroes Loss 0.0360\n",
      "  Epoch 1 Batch 10650 @0 erroes Loss 0.1227\n",
      "  Epoch 1 Batch 10700 @3 erroes Loss 0.3671\n",
      "  Epoch 1 Batch 10750 @4 erroes Loss 0.4158\n",
      "  Epoch 1 Batch 10800 @3 erroes Loss 0.3517\n",
      "  Epoch 1 Batch 10850 @0 erroes Loss 0.0357\n",
      "  Epoch 1 Batch 10900 @0 erroes Loss 0.0387\n",
      "  Epoch 1 Batch 10950 @4 erroes Loss 0.4435\n",
      "luxury auto maker it was further he   (original)\n",
      "luxury auto maker it was further he ==>  \n",
      "lutury auto maker it was further he  Epoch 1 Batch 11000 @0 erroes Loss 0.0400\n",
      "  Epoch 1 Batch 11050 @2 erroes Loss 0.2091\n",
      "  Epoch 1 Batch 11100 @4 erroes Loss 0.4268\n",
      "  Epoch 1 Batch 11150 @4 erroes Loss 0.4350\n",
      "  Epoch 1 Batch 11200 @0 erroes Loss 0.0383\n",
      "  Epoch 1 Batch 11250 @2 erroes Loss 0.2232\n",
      "  Epoch 1 Batch 11300 @0 erroes Loss 0.0314\n",
      "  Epoch 1 Batch 11350 @4 erroes Loss 0.4279\n",
      "  Epoch 1 Batch 11400 @0 erroes Loss 0.0442\n",
      "  Epoch 1 Batch 11450 @3 erroes Loss 0.3271\n",
      "es to suit the bureaucrats in bruss   (original)\n",
      "es to suit the bureaucrats in bruss ==>  \n",
      "es to suit the bureaucrats in bruss  Epoch 1 Batch 11500 @0 erroes Loss 0.0360\n",
      "  Epoch 1 Batch 11550 @2 erroes Loss 0.2343\n",
      "  Epoch 1 Batch 11600 @1 erroes Loss 0.1404\n",
      "  Epoch 1 Batch 11650 @4 erroes Loss 0.3787\n",
      "  Epoch 1 Batch 11700 @4 erroes Loss 0.4532\n",
      "  Epoch 1 Batch 11750 @1 erroes Loss 0.1392\n",
      "  Epoch 1 Batch 11800 @3 erroes Loss 0.3245\n",
      "  Epoch 1 Batch 11850 @4 erroes Loss 0.3199\n",
      "  Epoch 1 Batch 11900 @3 erroes Loss 0.3581\n",
      "  Epoch 1 Batch 11950 @4 erroes Loss 0.4005\n",
      "s what he really wanted to know was   (original)\n",
      "s what he reallyywanted to enow was ==>  \n",
      "s what he reallyywanted to enow was  Epoch 1 Batch 12000 @3 erroes Loss 0.2169\n",
      "  Epoch 1 Batch 12050 @4 erroes Loss 0.2498\n",
      "  Epoch 1 Batch 12100 @2 erroes Loss 0.2302\n",
      "  Epoch 1 Batch 12150 @3 erroes Loss 0.3099\n",
      "  Epoch 1 Batch 12200 @2 erroes Loss 0.2210\n",
      "  Epoch 1 Batch 12250 @0 erroes Loss 0.0346\n",
      "  Epoch 1 Batch 12300 @1 erroes Loss 0.1255\n",
      "  Epoch 1 Batch 12350 @2 erroes Loss 0.2250\n",
      "  Epoch 1 Batch 12400 @4 erroes Loss 0.4446\n",
      "  Epoch 1 Batch 12450 @3 erroes Loss 0.3580\n",
      "g practices including the developme   (original)\n",
      "g nracticesyincluding thl developme ==>  \n",
      "g nractices including the developme  Epoch 1 Batch 12500 @3 erroes Loss 0.3207\n",
      "  Epoch 1 Batch 12550 @0 erroes Loss 0.0323\n",
      "  Epoch 1 Batch 12600 @0 erroes Loss 0.0287\n",
      "  Epoch 1 Batch 12650 @2 erroes Loss 0.2081\n",
      "  Epoch 1 Batch 12700 @3 erroes Loss 0.3821\n",
      "  Epoch 1 Batch 12750 @4 erroes Loss 0.4324\n",
      "  Epoch 1 Batch 12800 @3 erroes Loss 0.3110\n",
      "  Epoch 1 Batch 12850 @1 erroes Loss 0.1135\n",
      "  Epoch 1 Batch 12900 @0 erroes Loss 0.0338\n",
      "  Epoch 1 Batch 12950 @0 erroes Loss 0.0393\n",
      "have shown that blacks receive more   (original)\n",
      "have shown that blacks receive tore ==>  \n",
      "have shown that blacks receive tore  Epoch 1 Batch 13000 @1 erroes Loss 0.1283\n",
      "  Epoch 1 Batch 13050 @4 erroes Loss 0.3874\n",
      "  Epoch 1 Batch 13100 @1 erroes Loss 0.1244\n",
      "  Epoch 1 Batch 13150 @3 erroes Loss 0.3134\n",
      "  Epoch 1 Batch 13200 @3 erroes Loss 0.3700\n",
      "  Epoch 1 Batch 13250 @3 erroes Loss 0.3845\n",
      "  Epoch 1 Batch 13300 @1 erroes Loss 0.1174\n",
      "  Epoch 1 Batch 13350 @4 erroes Loss 0.3627\n",
      "  Epoch 1 Batch 13400 @4 erroes Loss 0.4019\n",
      "  Epoch 1 Batch 13450 @3 erroes Loss 0.3171\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-c627400d7a98>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[0mvariables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariables\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[0mgradients\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients)\u001b[0m\n\u001b[0;32m    856\u001b[0m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0;32m    857\u001b[0m         \u001b[0m_default_vspace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflat_sources\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 858\u001b[1;33m         output_gradients=output_gradients)\n\u001b[0m\u001b[0;32m    859\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    860\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(vspace, tape, target, sources, output_gradients)\u001b[0m\n\u001b[0;32m     61\u001b[0m   \"\"\"\n\u001b[0;32m     62\u001b[0m   return pywrap_tensorflow.TFE_Py_TapeGradient(\n\u001b[1;32m---> 63\u001b[1;33m       tape._tape, vspace, target, sources, output_gradients)  # pylint: disable=protected-access\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py\u001b[0m in \u001b[0;36m_MeanGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_MeanGrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m   \u001b[1;34m\"\"\"Gradient for Mean.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m   \u001b[0msum_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_SumGrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m   \u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shape_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m   \u001b[0moutput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shape_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py\u001b[0m in \u001b[0;36m_SumGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m           \u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m   \u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mtile\u001b[1;34m(input, multiples, name)\u001b[0m\n\u001b[0;32m  10497\u001b[0m       _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n\u001b[0;32m  10498\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Tile\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 10499\u001b[1;33m         _ctx._post_execution_callbacks, input, multiples)\n\u001b[0m\u001b[0;32m  10500\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10501\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (inp, targ)) in enumerate(dataset):\n",
    "        loss = 0\n",
    "        \n",
    "        if batch % 500 ==0:\n",
    "            # print one for test\n",
    "            input_string = [idx2char[idx.numpy()] for idx in inp[0]]\n",
    "            for i in input_string:\n",
    "                print(i, end='')\n",
    "            print('   (original)')\n",
    "        \n",
    "        # modify input data to represent error\n",
    "        nb_error = np.random.randint(0,5) # min(max(epoch - 0, 0), 5) # stoping change in first 5 epoch\n",
    "        random_index = np.random.randint(0, max_length, (BATCH_SIZE, nb_error))\n",
    "        random_value = np.random.randint(1, len(unique), (BATCH_SIZE, nb_error))\n",
    "        \n",
    "        for b in range(BATCH_SIZE):\n",
    "            for d in range(nb_error):\n",
    "                inp = inp.numpy()\n",
    "                inp[b, random_index[d, d]] = random_value[b, d]\n",
    "                inp = tf.contrib.eager.Variable(inp)\n",
    "                # tf.scatter_update(inp, [b, random_index[d, d]], random_value[b, d])\n",
    "        \n",
    "        if batch % 500 ==0:\n",
    "            # print one for test\n",
    "            input_string = [idx2char[idx.numpy()] for idx in inp[0]]\n",
    "            for i in input_string:\n",
    "                print(i, end='')\n",
    "            print(' ==>  ')\n",
    "        \n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_output, enc_hidden = encoder(inp, hidden)\n",
    "            \n",
    "            dec_hidden = enc_hidden\n",
    "            \n",
    "            dec_input = tf.expand_dims([char2idx['<start>']] * BATCH_SIZE, 1)       \n",
    "            \n",
    "            # Teacher forcing - feeding the target as the next input\n",
    "            for t in range(0, targ.shape[1]):\n",
    "                # passing enc_output to the decoder\n",
    "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "                \n",
    "                if batch % 500==0:\n",
    "                    output = predictions[0]\n",
    "                    print(idx2char[output.numpy().argmax(-1)], end='')\n",
    "                    \n",
    "                \n",
    "                loss += loss_function(targ[:, t], predictions)\n",
    "                \n",
    "                # using teacher forcing\n",
    "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "        \n",
    "        batch_loss = (loss / int(targ.shape[1]))\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        variables = encoder.variables + decoder.variables\n",
    "        \n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        if batch % 50 == 0:\n",
    "            print('  Epoch {} Batch {} @{} erroes Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         nb_error,\n",
    "                                                         batch_loss.numpy()))\n",
    "        if batch % 3000 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "            \n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    \n",
    "    print('Epoch {} Loss {:.4f}\\n'.format(epoch + 1,\n",
    "                                        total_loss / batch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "brZsYa3P_IKd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Spelling_correction.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
